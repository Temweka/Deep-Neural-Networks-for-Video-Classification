{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:14:04.575187Z",
     "start_time": "2019-01-26T20:14:04.571919Z"
    }
   },
   "outputs": [],
   "source": [
    "WORKER_COUNT = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:14:04.582963Z",
     "start_time": "2019-01-26T20:14:04.580410Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sync models\n",
    "# gsutil -m rsync -r /mnt/seals/models/ gs://thesis-penguins/models/ \n",
    "# aws s3 sync /mnt/seals/models s3://thesisvids/penguins/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:14:05.137094Z",
     "start_time": "2019-01-26T20:14:05.133699Z"
    }
   },
   "outputs": [],
   "source": [
    "# whether to log each feature and sequence status\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:14:05.143776Z",
     "start_time": "2019-01-26T20:14:05.139534Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 5000\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:14:05.149356Z",
     "start_time": "2019-01-26T20:14:05.146147Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup paths\n",
    "pwd = os.getcwd().replace(\"notebooks\",\"\")\n",
    "path_cache = pwd + 'cache/'\n",
    "path_data = pwd + 'data/'\n",
    "path_models = pwd + 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:14:05.797243Z",
     "start_time": "2019-01-26T20:14:05.792151Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup logging\n",
    "# any explicit log messages or uncaught errors to stdout and file /logs.log\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"{0}/{1}.log\".format(pwd, \"logs\")),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "# init logger\n",
    "logger = logging.getLogger()\n",
    "# make logger aware of any uncaught exceptions\n",
    "def handle_exception(exc_type, exc_value, exc_traceback):\n",
    "    if issubclass(exc_type, KeyboardInterrupt):\n",
    "        sys.__excepthook__(exc_type, exc_value, exc_traceback)\n",
    "        return\n",
    "\n",
    "    logger.error(\"Uncaught exception\", exc_info=(exc_type, exc_value, exc_traceback))\n",
    "sys.excepthook = handle_exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:14:05.803035Z",
     "start_time": "2019-01-26T20:14:05.799961Z"
    }
   },
   "outputs": [],
   "source": [
    "from deepvideoclassification.pretrained_CNNs import pretrained_model_names, pretrained_model_names_bucketed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:14:07.325862Z",
     "start_time": "2019-01-26T20:14:07.320722Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_results():\n",
    "    \"\"\"\n",
    "    Load the results of all completed models into a dataframe and return sorted on val_acc\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for folder, subs, files in os.walk(path_models):\n",
    "        for filename in files:\n",
    "            if 'results.json' in filename:\n",
    "                with open(os.path.abspath(os.path.join(folder, filename))) as f:\n",
    "                    data = json.load(f)\n",
    "                results.append(data)\n",
    "\n",
    "    results = pd.DataFrame(results)        \n",
    "    results.sort_values(\"fit_val_acc\", inplace=True, ascending=False)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:14:07.332851Z",
     "start_time": "2019-01-26T20:14:07.328074Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_completed_experiments(experiments):\n",
    "    \"\"\"\n",
    "    Helper function to remove the completed experiments from experiments dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # get updated result set\n",
    "    results = get_results()\n",
    "    \n",
    "    # merge results onto experiments\n",
    "    experiments = pd.merge(experiments, results[['model_id','fit_val_acc']], left_on='model_id', right_on='model_id', how='left')\n",
    "    experiments['done'] = (experiments['fit_val_acc']>0).astype(int)\n",
    "    print(\"{}/{} experiments done\".format(experiments[experiments['done'] == 1].shape[0], len(experiments)))\n",
    "    \n",
    "    # only keep experiments not done\n",
    "    experiments = experiments[experiments['done'] == 0]\n",
    "\n",
    "    # clean up\n",
    "    del experiments['done']\n",
    "    del experiments['fit_val_acc']\n",
    "    \n",
    "    # return experiments already done\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:14:07.340131Z",
     "start_time": "2019-01-26T20:14:07.334999Z"
    }
   },
   "outputs": [],
   "source": [
    "def reassign_workers(experiments):\n",
    "    \"\"\"\n",
    "    Helper function to reassign outstanding experiments to workers\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # get updated result set\n",
    "    results = get_results()\n",
    "    \n",
    "    # merge results onto experiments\n",
    "    experiments = pd.merge(experiments, results[['model_id','fit_val_acc']], left_on='model_id', right_on='model_id', how='left')\n",
    "    experiments['done'] = (experiments['fit_val_acc']>0).astype(int)\n",
    "    \n",
    "    # reassign workers\n",
    "    global todo_idx\n",
    "    todo_idx = 0\n",
    "    \n",
    "    def assign_not_done_to_worker(row):\n",
    "        \"\"\" helper lambda\"\"\"\n",
    "        global todo_idx\n",
    "        if row['done'] == 0:\n",
    "            todo_idx+=1\n",
    "            return todo_idx % WORKER_COUNT\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    experiments['WORKER'] = experiments.apply(assign_not_done_to_worker, axis=1)\n",
    "    \n",
    "    # clean up\n",
    "    del experiments['done']\n",
    "    del experiments['fit_val_acc']\n",
    "    \n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:14:09.034808Z",
     "start_time": "2019-01-26T20:14:09.031757Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment_batch_name = 'experiment_batch_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:14:09.652832Z",
     "start_time": "2019-01-26T20:14:09.626950Z"
    }
   },
   "outputs": [],
   "source": [
    "# reload experiments\n",
    "experiments = pd.read_csv(pwd + \"experiments/\" + experiment_batch_name + \".csv\")\n",
    "experiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:14:18.083078Z",
     "start_time": "2019-01-26T20:14:10.042824Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove experiments already completed\n",
    "experiments = remove_completed_experiments(experiments)\n",
    "experiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:14:18.094436Z",
     "start_time": "2019-01-26T20:14:18.085608Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments['WORKER'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T07:20:13.253713Z",
     "start_time": "2019-01-26T07:20:12.510462Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments = reassign_workers(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T07:20:15.746193Z",
     "start_time": "2019-01-26T07:20:15.736154Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments['WORKER'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:14:18.122538Z",
     "start_time": "2019-01-26T20:14:18.096914Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:18:33.761608Z",
     "start_time": "2019-01-25T22:18:33.746118Z"
    }
   },
   "outputs": [],
   "source": [
    "# init model id - need to make sure we pick up where we leave off don't overwrite it between batches\n",
    "model_id_start = pd.read_csv(pwd + \"experiments/experiment_batch_3.csv\")['model_id'].max() + 1\n",
    "# read list of paths and find max there too to be double safe we don't overwrite model_ids\n",
    "paths = os.listdir(path_models)\n",
    "paths = [int(p) for p in paths]\n",
    "model_id_start = max(model_id_start,max(paths) + 1)\n",
    "print(model_id_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:18:43.137878Z",
     "start_time": "2019-01-25T22:18:43.121368Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:19:42.260674Z",
     "start_time": "2019-01-25T22:19:42.236920Z"
    }
   },
   "outputs": [],
   "source": [
    "# add frame size column\n",
    "model_id_start+=1\n",
    "experiments['frame_size'] = None\n",
    "\n",
    "new_experiment = {'WORKER': 5,\n",
    "                     'architecture': 'c3dsmall',\n",
    "                     'dropout': 0,\n",
    "                     'layer_1_size': 0,\n",
    "                     'layer_2_size': 0,\n",
    "                     'layer_3_size': 0,\n",
    "                     'model_id': model_id_start ,\n",
    "                     'pooling': None,\n",
    "                     'pretrained_model_name': None,\n",
    "                     'sequence_length': 16,\n",
    "                     'sequence_model': None,\n",
    "                     'sequence_model_layers': None,\n",
    "                     'frame_size': (112,112)}\n",
    "experiments = experiments.append(new_experiment, ignore_index=True)\n",
    "model_id_start+=1\n",
    "new_experiment = {'WORKER': 4,\n",
    "                     'architecture': 'c3d',\n",
    "                     'dropout': 0,\n",
    "                     'layer_1_size': 0,\n",
    "                     'layer_2_size': 0,\n",
    "                     'layer_3_size': 0,\n",
    "                     'model_id': model_id_start,\n",
    "                     'pooling': None,\n",
    "                     'pretrained_model_name': None,\n",
    "                     'sequence_length': 16,\n",
    "                     'sequence_model': None,\n",
    "                     'sequence_model_layers': None,\n",
    "                     'frame_size':(112,112)}\n",
    "experiments = experiments.append(new_experiment, ignore_index=True)\n",
    "model_id +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:19:43.022858Z",
     "start_time": "2019-01-25T22:19:43.004280Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:19:52.784415Z",
     "start_time": "2019-01-25T22:19:52.776471Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments.to_csv(pwd + \"experiments/\" + experiment_batch_name + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create list of experiments to be run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* batch 1 = run frozen image MLP, LRCNNs and concat models on 1 of each pretrained_model_name in buckets (bucketed on feature sizes and limited to max sequence_length of 10)\n",
    "\n",
    "* batch 2 = for best configurations from batch 1, run other pretrained models in buckets and run longer sequence lengths, maybe try different convolution kernel sizes\n",
    "\n",
    "* batch 3 = run even longer sequence lengths for 3 best models\n",
    "\n",
    "* batch 4 = run trainable MLP and LRCNN on best performing frozen variants\n",
    "\n",
    "* batch 5 = run trainable but initializing with best CNN weights\n",
    "\n",
    "* batch 6 = run C3D models\n",
    "\n",
    "* batch 7 = analyze effect of dropout and pooling with best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T22:46:05.928701Z",
     "start_time": "2019-01-24T22:46:05.925028Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiment_batch_name = 'experiment_batch_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T22:46:06.075681Z",
     "start_time": "2019-01-24T22:46:06.072051Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# init model id - need to make sure we pick up where we leave off don't overwrite it between batches\n",
    "model_id_start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T22:46:06.272719Z",
     "start_time": "2019-01-24T22:46:06.269010Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# init list of experiments\n",
    "experiments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T22:46:06.929727Z",
     "start_time": "2019-01-24T22:46:06.925603Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pooling = 'max'\n",
    "layer_sizes = [512, 256, 128, 0]\n",
    "dropouts = [0.2]\n",
    "sequence_lengths = [3,5,10]\n",
    "sequence_models = [\"LSTM\", \"SimpleRNN\", \"GRU\", \"Convolution1D\"]\n",
    "sequence_model_layer_counts = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T22:46:06.938183Z",
     "start_time": "2019-01-24T22:46:06.932733Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "### image_MLP_frozen \n",
    "####################\n",
    "\n",
    "for pretrained_model_name in pretrained_model_names_bucketed:\n",
    "    for layer_1_size in layer_sizes:\n",
    "        for layer_2_size in layer_sizes:\n",
    "            for layer_3_size in layer_sizes:\n",
    "                for dropout in dropouts:\n",
    "\n",
    "                    # build experiment parameters\n",
    "                    experiment = {}\n",
    "                    \n",
    "                    experiment['architecture'] = 'image_MLP_frozen'\n",
    "                    experiment['sequence_length'] = 1\n",
    "                    experiment['pretrained_model_name'] = pretrained_model_name\n",
    "                    experiment['layer_1_size'] = layer_1_size\n",
    "                    experiment['layer_2_size'] = layer_2_size\n",
    "                    experiment['layer_3_size'] = layer_3_size\n",
    "                    experiment['dropout'] = dropout\n",
    "                    experiment['pooling'] = 'max' # outperforms avg across all parameters\n",
    "                    \n",
    "                    # add to list of experiments\n",
    "                    experiments.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T22:46:06.946582Z",
     "start_time": "2019-01-24T22:46:06.940637Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "### video_MLP_concat\n",
    "####################\n",
    "\n",
    "for sequence_length in sequence_lengths:\n",
    "    for pretrained_model_name in pretrained_model_names_bucketed:\n",
    "        for layer_1_size in layer_sizes:\n",
    "            for layer_2_size in layer_sizes:\n",
    "                for layer_3_size in layer_sizes:\n",
    "                    for dropout in dropouts:\n",
    "\n",
    "                        # build experiment parameters\n",
    "                        experiment = {}\n",
    "\n",
    "                        experiment['architecture'] = 'video_MLP_concat'\n",
    "                        experiment['pretrained_model_name'] = pretrained_model_name\n",
    "                        experiment['layer_1_size'] = layer_1_size\n",
    "                        experiment['layer_2_size'] = layer_2_size\n",
    "                        experiment['layer_3_size'] = layer_3_size\n",
    "                        experiment['dropout'] = dropout\n",
    "                        experiment['pooling'] = 'max' # outperforms avg across all parameters\n",
    "                        experiment['sequence_length'] = sequence_length\n",
    "\n",
    "                        # add to list of experiments\n",
    "                        experiments.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T22:46:06.960154Z",
     "start_time": "2019-01-24T22:46:06.949374Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "### video_LRCNN_frozen\n",
    "######################\n",
    "\n",
    "for sequence_length in sequence_lengths:\n",
    "    for pretrained_model_name in pretrained_model_names_bucketed:\n",
    "        for layer_1_size in layer_sizes:\n",
    "            for layer_2_size in layer_sizes:\n",
    "                for layer_3_size in layer_sizes:\n",
    "                    for dropout in dropouts:\n",
    "                        for sequence_model in sequence_models:\n",
    "                            for sequence_model_layers in sequence_model_layer_counts:\n",
    "\n",
    "                                # build experiment parameters\n",
    "                                experiment = {}\n",
    "\n",
    "                                experiment['architecture'] = 'video_LRCNN_frozen'\n",
    "                                experiment['pretrained_model_name'] = pretrained_model_name\n",
    "                                experiment['layer_1_size'] = layer_1_size\n",
    "                                experiment['layer_2_size'] = layer_2_size\n",
    "                                experiment['layer_3_size'] = layer_3_size\n",
    "                                experiment['dropout'] = dropout\n",
    "                                experiment['pooling'] = 'max' # outperforms avg across all parameters\n",
    "                                experiment['sequence_model'] = sequence_model\n",
    "                                experiment['sequence_model_layers'] = sequence_model_layers\n",
    "                                experiment['sequence_length'] = sequence_length\n",
    "\n",
    "                                # add to list of experiments\n",
    "                                experiments.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T22:46:07.504696Z",
     "start_time": "2019-01-24T22:46:07.476730Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "### convert to dataframe\n",
    "########################\n",
    "\n",
    "experiments = pd.DataFrame(experiments)\n",
    "\n",
    "### create model id column for this experiment batch\n",
    "model_id_list = list(range(0,len(experiments)))\n",
    "experiments['model_id'] = model_id_list\n",
    "\n",
    "# assign to workers\n",
    "experiments['WORKER'] = experiments['model_id'].apply(lambda x: x % WORKER_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T22:46:07.512131Z",
     "start_time": "2019-01-24T22:46:07.507518Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T22:46:08.161868Z",
     "start_time": "2019-01-24T22:46:08.147174Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "### remove invalid experiment configurations\n",
    "############################################\n",
    "\n",
    "# Just won't run experiments for those model_ids - not an error that model ids not congituous count from 0!\n",
    "\n",
    "# delete video experiments with 0 neurons in a layer with nonzero neurons in later layers\n",
    "experiments = experiments[~((experiments['layer_1_size'] == 0) & (experiments['layer_2_size'] > 0))]\n",
    "experiments = experiments[~((experiments['layer_1_size'] == 0) & (experiments['layer_3_size'] > 0))]\n",
    "experiments = experiments[~((experiments['layer_2_size'] == 0) & (experiments['layer_3_size'] > 0))]\n",
    "\n",
    "# delete video experiments where convolution_kernel_size > sequence_length (convolution_kernel_size defaults to 3 and not set in this batch)\n",
    "experiments = experiments[~((experiments['sequence_model'] == 'Convolution1D') & (experiments['sequence_length']<=3))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T22:46:08.170012Z",
     "start_time": "2019-01-24T22:46:08.164530Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# delete LRCNN_frozen experiments with layer_1_size == 0\n",
    "experiments = experiments[~((experiments['architecture'] == 'video_LRCNN_frozen') & (experiments['layer_1_size']==0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T22:46:08.954566Z",
     "start_time": "2019-01-24T22:46:08.915377Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "### output experiment batch to CSV\n",
    "##################################\n",
    "print(experiment_batch_name)\n",
    "experiments.to_csv(pwd + \"experiments/\" + experiment_batch_name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T22:46:09.695430Z",
     "start_time": "2019-01-24T22:46:09.677262Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(experiments.shape)\n",
    "experiments.tail().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T22:46:10.016400Z",
     "start_time": "2019-01-24T22:46:10.012902Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # upload to s3\n",
    "# response = os.system(\"aws s3 cp \" + pwd + \"experiments/\" + experiment_batch_name + '.csv s3://thesisvids/penguins/' + experiment_batch_name + '.csv')\n",
    "# if response == 0:\n",
    "#     print(\"upload success\")\n",
    "# else:\n",
    "#     print(\"upload error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T22:46:12.606710Z",
     "start_time": "2019-01-24T22:46:12.603048Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # # upload to GCP\n",
    "# response = os.system(\"gsutil cp \" + pwd + \"experiments/\" + experiment_batch_name + '.csv gs://thesis-penguins/' + experiment_batch_name + '.csv')\n",
    "# if response == 0:\n",
    "#     print(\"upload success\")\n",
    "# else:\n",
    "#     print(\"upload error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T10:40:35.920863Z",
     "start_time": "2019-01-25T10:40:35.890009Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# reload experiments\n",
    "experiments = pd.read_csv(pwd + \"experiments/experiment_batch_1.csv\")\n",
    "experiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T10:40:36.170478Z",
     "start_time": "2019-01-25T10:40:36.166225Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# define experiment batch name\n",
    "experiment_batch_name = 'experiment_batch_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T10:40:36.409091Z",
     "start_time": "2019-01-25T10:40:36.383983Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# init model id - need to make sure we pick up where we leave off don't overwrite it between batches\n",
    "model_id_start = pd.read_csv(pwd + \"experiments/experiment_batch_1.csv\")['model_id'].max() + 1\n",
    "# read list of paths and find max there too to be double safe we don't overwrite model_ids\n",
    "paths = os.listdir(path_models)\n",
    "paths = [int(p) for p in paths]\n",
    "model_id_start = max(model_id_start,max(paths) + 1)\n",
    "print(model_id_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T10:40:37.514447Z",
     "start_time": "2019-01-25T10:40:36.678902Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove experiments already completed\n",
    "experiments = remove_completed_experiments(experiments)\n",
    "experiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T10:40:38.374705Z",
     "start_time": "2019-01-25T10:40:37.517846Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get top 20 results from previous batch\n",
    "results = get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T10:40:40.666228Z",
     "start_time": "2019-01-25T10:40:40.659744Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# we'll create experiments with same parameters as best 20 so far but now with longer sequence lengths\n",
    "results_top20 = results[['architecture','layer_1_size','layer_2_size', 'layer_3_size','dropout','pretrained_model_name','pooling','sequence_length','sequence_model','sequence_model_layers']].head(20)\n",
    "sequence_lengths = [15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T10:40:42.546833Z",
     "start_time": "2019-01-25T10:40:42.538821Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# init list for extra experiments to append to outstanding experiments\n",
    "experiments_next = []\n",
    "\n",
    "# init model_id\n",
    "model_id = model_id_start\n",
    "\n",
    "for sequence_length in sequence_lengths:\n",
    "    for result in results_top20.values:\n",
    "        \n",
    "        # convert result params to dict to create experiment from\n",
    "        result_dict = dict(zip(results_top20.columns, result))\n",
    "\n",
    "        experiment = {}                    \n",
    "        experiment['architecture'] = 'video_lrcnn_frozen'\n",
    "        experiment['sequence_model'] = result_dict['sequence_model']\n",
    "        experiment['sequence_model_layers'] = result_dict['sequence_model_layers']\n",
    "        experiment['sequence_length'] = sequence_length\n",
    "        experiment['pretrained_model_name'] = result_dict['pretrained_model_name']\n",
    "        experiment['layer_1_size'] = result_dict['layer_1_size']\n",
    "        experiment['layer_2_size'] = result_dict['layer_2_size']\n",
    "        experiment['layer_3_size'] = result_dict['layer_3_size']\n",
    "        experiment['dropout'] = result_dict['dropout']\n",
    "        experiment['pooling'] = 'max' # always outperforms avg\n",
    "        \n",
    "        experiment['model_id'] = model_id\n",
    "        \n",
    "        # assign to worker\n",
    "        experiment['WORKER'] = model_id % WORKER_COUNT\n",
    "\n",
    "        # increment unique model id\n",
    "        model_id +=1\n",
    "        \n",
    "        # add to list of additional experiments\n",
    "        experiments_next.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T10:40:43.372837Z",
     "start_time": "2019-01-25T10:40:43.363637Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert to dataframe\n",
    "experiments_next = pd.DataFrame(experiments_next)\n",
    "experiments_next.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T10:40:43.811363Z",
     "start_time": "2019-01-25T10:40:43.800803Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# merge new experiments onto outstanding experiments\n",
    "experiments = pd.concat([experiments, experiments_next], axis = 0)\n",
    "experiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T10:40:47.977293Z",
     "start_time": "2019-01-25T10:40:47.929590Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T10:40:48.498555Z",
     "start_time": "2019-01-25T10:40:48.489477Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments['WORKER'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T10:40:52.089258Z",
     "start_time": "2019-01-25T10:40:51.226080Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments = reassign_workers(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T10:40:52.102557Z",
     "start_time": "2019-01-25T10:40:52.092589Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments['WORKER'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T07:06:55.195248Z",
     "start_time": "2019-01-25T07:06:55.185454Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "### output experiment batch to CSV\n",
    "##################################\n",
    "print(experiment_batch_name)\n",
    "experiments.to_csv(pwd + \"experiments/\" + experiment_batch_name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T07:06:55.337784Z",
     "start_time": "2019-01-25T07:06:55.334316Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # upload to s3\n",
    "# response = os.system(\"aws s3 cp \" + pwd + \"experiments/\" + experiment_batch_name + '.csv s3://thesisvids/penguins/' + experiment_batch_name + '.csv')\n",
    "# if response == 0:\n",
    "#     print(\"upload success\")\n",
    "# else:\n",
    "#     print(\"upload error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T07:06:57.540404Z",
     "start_time": "2019-01-25T07:06:55.787311Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # upload to GCP\n",
    "response = os.system(\"gsutil cp \" + pwd + \"experiments/\" + experiment_batch_name + '.csv gs://thesis-penguins/' + experiment_batch_name + '.csv')\n",
    "if response == 0:\n",
    "    print(\"upload success\")\n",
    "else:\n",
    "    print(\"upload error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:16:53.124833Z",
     "start_time": "2019-01-25T22:16:53.112121Z"
    }
   },
   "outputs": [],
   "source": [
    "# reload experiments\n",
    "experiments = pd.read_csv(pwd + \"experiments/experiment_batch_2.csv\")\n",
    "experiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:16:53.254962Z",
     "start_time": "2019-01-25T22:16:53.251859Z"
    }
   },
   "outputs": [],
   "source": [
    "# define experiment batch name\n",
    "experiment_batch_name = 'experiment_batch_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:16:53.658846Z",
     "start_time": "2019-01-25T22:16:53.644991Z"
    }
   },
   "outputs": [],
   "source": [
    "# init model id - need to make sure we pick up where we leave off don't overwrite it between batches\n",
    "model_id_start = pd.read_csv(pwd + \"experiments/experiment_batch_2.csv\")['model_id'].max() + 1\n",
    "# read list of paths and find max there too to be double safe we don't overwrite model_ids\n",
    "paths = os.listdir(path_models)\n",
    "paths = [int(p) for p in paths]\n",
    "model_id_start = max(model_id_start,max(paths) + 1)\n",
    "print(model_id_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:16:55.223599Z",
     "start_time": "2019-01-25T22:16:54.575313Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove experiments already completed\n",
    "experiments = remove_completed_experiments(experiments)\n",
    "experiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:16:58.344364Z",
     "start_time": "2019-01-25T22:16:57.695416Z"
    }
   },
   "outputs": [],
   "source": [
    "# get top 20 results from previous batch\n",
    "results = get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:17:00.209556Z",
     "start_time": "2019-01-25T22:17:00.199581Z"
    }
   },
   "outputs": [],
   "source": [
    "# we'll create experiments with same parameters as best 20 so far but now with longer sequence lengths\n",
    "results_top = results[['architecture','layer_1_size','layer_2_size', 'layer_3_size','dropout','pretrained_model_name','pooling','sequence_length','sequence_model','sequence_model_layers']].head(20)\n",
    "sequence_lengths = [25, 30, 35, 40, 45, 50, 75, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:17:00.218919Z",
     "start_time": "2019-01-25T22:17:00.213978Z"
    }
   },
   "outputs": [],
   "source": [
    "results_top = results_top[results_top['sequence_length'] == 20].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:17:00.974007Z",
     "start_time": "2019-01-25T22:17:00.959266Z"
    }
   },
   "outputs": [],
   "source": [
    "results_top.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:17:00.985900Z",
     "start_time": "2019-01-25T22:17:00.976873Z"
    }
   },
   "outputs": [],
   "source": [
    "# init list for extra experiments to append to outstanding experiments\n",
    "experiments_next = []\n",
    "\n",
    "# init model_id\n",
    "model_id = model_id_start\n",
    "\n",
    "for sequence_length in sequence_lengths:\n",
    "    for result in results_top.values:\n",
    "        \n",
    "        # convert result params to dict to create experiment from\n",
    "        result_dict = dict(zip(results_top.columns, result))\n",
    "\n",
    "        experiment = {}                    \n",
    "        experiment['architecture'] = 'video_lrcnn_frozen'\n",
    "        experiment['sequence_model'] = result_dict['sequence_model']\n",
    "        experiment['sequence_model_layers'] = result_dict['sequence_model_layers']\n",
    "        experiment['sequence_length'] = sequence_length\n",
    "        experiment['pretrained_model_name'] = result_dict['pretrained_model_name']\n",
    "        experiment['layer_1_size'] = result_dict['layer_1_size']\n",
    "        experiment['layer_2_size'] = result_dict['layer_2_size']\n",
    "        experiment['layer_3_size'] = result_dict['layer_3_size']\n",
    "        experiment['dropout'] = result_dict['dropout']\n",
    "        experiment['pooling'] = 'max' # always outperforms avg\n",
    "        \n",
    "        experiment['model_id'] = model_id\n",
    "        \n",
    "        # assign to worker\n",
    "        experiment['WORKER'] = model_id % WORKER_COUNT\n",
    "\n",
    "        # increment unique model id\n",
    "        model_id +=1\n",
    "        \n",
    "        # add to list of additional experiments\n",
    "        experiments_next.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:17:02.666480Z",
     "start_time": "2019-01-25T22:17:02.658817Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert to dataframe\n",
    "experiments_next = pd.DataFrame(experiments_next)\n",
    "experiments_next.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:17:03.254937Z",
     "start_time": "2019-01-25T22:17:03.244743Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge new experiments onto outstanding experiments\n",
    "experiments = pd.concat([experiments, experiments_next], axis = 0)\n",
    "experiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:17:03.909508Z",
     "start_time": "2019-01-25T22:17:03.882658Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:17:06.810989Z",
     "start_time": "2019-01-25T22:17:06.801255Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments['WORKER'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:17:17.377717Z",
     "start_time": "2019-01-25T22:17:16.709174Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments = remove_completed_experiments(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:17:24.831311Z",
     "start_time": "2019-01-25T22:17:24.158868Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments = reassign_workers(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T22:17:32.458413Z",
     "start_time": "2019-01-25T22:17:32.432825Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T14:44:29.617806Z",
     "start_time": "2019-01-25T14:44:29.610840Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments['WORKER'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T14:44:30.761298Z",
     "start_time": "2019-01-25T14:44:30.750517Z"
    }
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "### output experiment batch to CSV\n",
    "##################################\n",
    "print(experiment_batch_name)\n",
    "experiments.to_csv(pwd + \"experiments/\" + experiment_batch_name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T14:44:33.434356Z",
     "start_time": "2019-01-25T14:44:33.430020Z"
    }
   },
   "outputs": [],
   "source": [
    "# # upload to s3\n",
    "# response = os.system(\"aws s3 cp \" + pwd + \"experiments/\" + experiment_batch_name + '.csv s3://thesisvids/penguins/' + experiment_batch_name + '.csv')\n",
    "# if response == 0:\n",
    "#     print(\"upload success\")\n",
    "# else:\n",
    "#     print(\"upload error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T14:44:37.177090Z",
     "start_time": "2019-01-25T14:44:35.415765Z"
    }
   },
   "outputs": [],
   "source": [
    "# # upload to GCP\n",
    "response = os.system(\"gsutil cp \" + pwd + \"experiments/\" + experiment_batch_name + '.csv gs://thesis-penguins/' + experiment_batch_name + '.csv')\n",
    "if response == 0:\n",
    "    print(\"upload success\")\n",
    "else:\n",
    "    print(\"upload error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load results.json for all models into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:14:28.280769Z",
     "start_time": "2019-01-26T20:14:27.616127Z"
    }
   },
   "outputs": [],
   "source": [
    "results = get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:14:42.931190Z",
     "start_time": "2019-01-26T20:14:42.736576Z"
    }
   },
   "outputs": [],
   "source": [
    "results.groupby(\"architecture\").agg('max')['fit_val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:14:28.373146Z",
     "start_time": "2019-01-26T20:14:28.283833Z"
    }
   },
   "outputs": [],
   "source": [
    "results.head(20).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> top 20 models all `video_lrcnn_frozen` with `vgg16` and `LSTM`, `SimpleRNN`, `GRU` sequence models and sequence_length = 10 ... we'll re-run these with sequence length = 15 and 20 for batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T14:30:47.290954Z",
     "start_time": "2019-01-25T14:30:47.287507Z"
    }
   },
   "outputs": [],
   "source": [
    "# results[results['model_id'].isin([362, 550, 162, 133, 3115, 3125])].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## results per architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T14:30:47.777968Z",
     "start_time": "2019-01-25T14:30:47.752169Z"
    }
   },
   "outputs": [],
   "source": [
    "results[results['architecture'] == 'video_mlp_concat'].head(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T14:30:48.171219Z",
     "start_time": "2019-01-25T14:30:48.146741Z"
    }
   },
   "outputs": [],
   "source": [
    "results[results['architecture'] == 'image_mlp_frozen'].head(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-25T14:30:48.390530Z",
     "start_time": "2019-01-25T14:30:48.365018Z"
    }
   },
   "outputs": [],
   "source": [
    "results[results['architecture'] == 'video_lrcnn_frozen'].head(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyze best combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:15:29.484003Z",
     "start_time": "2019-01-26T20:15:29.301597Z"
    }
   },
   "outputs": [],
   "source": [
    "results.groupby(\"sequence_model\").agg('max')['fit_val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:15:30.305371Z",
     "start_time": "2019-01-26T20:15:30.052848Z"
    }
   },
   "outputs": [],
   "source": [
    "results.groupby(\"sequence_length\").agg('max')['fit_train_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:15:30.544392Z",
     "start_time": "2019-01-26T20:15:30.308167Z"
    }
   },
   "outputs": [],
   "source": [
    "results.groupby(\"sequence_length\").agg('max')['fit_val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:16:31.484517Z",
     "start_time": "2019-01-26T20:16:31.067495Z"
    }
   },
   "outputs": [],
   "source": [
    "results[results['architecture']=='video_lrcnn_frozen'].groupby(\"sequence_length\").agg('max')['fit_val_acc'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:16:48.381562Z",
     "start_time": "2019-01-26T20:16:48.060620Z"
    }
   },
   "outputs": [],
   "source": [
    "grp = results.groupby([\"sequence_model\",\"sequence_length\",\"sequence_model_layers\"]).agg(\"max\")['fit_val_acc'].unstack().sort_values(\"sequence_length\")\n",
    "grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:16:52.773178Z",
     "start_time": "2019-01-26T20:16:52.769621Z"
    }
   },
   "outputs": [],
   "source": [
    "grp.reset_index(level=1, drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:16:52.779079Z",
     "start_time": "2019-01-26T20:16:52.775665Z"
    }
   },
   "outputs": [],
   "source": [
    "grp.columns=['one','two']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:16:54.645698Z",
     "start_time": "2019-01-26T20:16:54.637947Z"
    }
   },
   "outputs": [],
   "source": [
    "grp['gap'] = (grp['two'] - grp['one'] )/ grp['two']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:16:54.989547Z",
     "start_time": "2019-01-26T20:16:54.976465Z"
    }
   },
   "outputs": [],
   "source": [
    "grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> two sequence levels better than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:17:08.623143Z",
     "start_time": "2019-01-26T20:17:08.360131Z"
    }
   },
   "outputs": [],
   "source": [
    "grp = results.groupby([\"sequence_model\",\"sequence_length\"]).agg(\"max\")['fit_val_acc'].unstack()\n",
    "grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:17:09.208675Z",
     "start_time": "2019-01-26T20:17:08.696288Z"
    }
   },
   "outputs": [],
   "source": [
    "# sort cols\n",
    "grpcols = list(grp.columns)\n",
    "grpcols.sort()\n",
    "grp = grp[grpcols]\n",
    "# plot\n",
    "plt = grp.plot(kind='bar', figsize=(10,5))\n",
    "x1,x2,y1,y2 = plt.axis()\n",
    "plt.axis((x1,x2,0.85,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:17:10.072929Z",
     "start_time": "2019-01-26T20:17:09.840178Z"
    }
   },
   "outputs": [],
   "source": [
    "results.sort_values(\"fit_val_acc\",ascending=False).head(30).plot(x='model_param_count',y='fit_val_acc', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:17:10.427825Z",
     "start_time": "2019-01-26T20:17:10.400311Z"
    }
   },
   "outputs": [],
   "source": [
    "results.sort_values('fit_dt_test_end',ascending=False).head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# re-allocate experiments to workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove experiments already done\n",
    "experiments = remove_completed_experiments(experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re-assign workers on the experiments that are not complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign outstanding experiments to workers\n",
    "experiments = reassign_workers(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T09:59:57.378916Z",
     "start_time": "2019-01-23T09:59:57.341702Z"
    }
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "### output experiment batch to CSV\n",
    "##################################\n",
    "print(experiment_batch_name)\n",
    "experiments.to_csv(pwd + \"experiments/\" + experiment_batch_name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug experiment worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:17:39.781803Z",
     "start_time": "2019-01-26T20:17:38.062762Z"
    }
   },
   "outputs": [],
   "source": [
    "from deepvideoclassification.architectures import Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:17:39.886300Z",
     "start_time": "2019-01-26T20:17:39.883029Z"
    }
   },
   "outputs": [],
   "source": [
    "WORKER_ID = 0\n",
    "GPU_ID = 0,1,2,3\n",
    "experiment_batch_name = 'experiment_batch_3'\n",
    "\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(GPU_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:17:40.450117Z",
     "start_time": "2019-01-26T20:17:40.443152Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup logging\n",
    "# separate log file for each worker\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s, [%(levelname)-8s] [%(filename)s:%(lineno)d] %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"{0}/{1}.log\".format(pwd, \"logs_\" + str(WORKER_ID))),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "# init logger - will pass this to our architecture\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logger.info(\"Start worker {} (GPU={}) processing {}\".format(WORKER_ID, GPU_ID, experiment_batch_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:17:41.854413Z",
     "start_time": "2019-01-26T20:17:41.843440Z"
    }
   },
   "outputs": [],
   "source": [
    "# load list of experiments\n",
    "experiments = pd.read_csv(pwd + \"experiments/\" + experiment_batch_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:17:41.873123Z",
     "start_time": "2019-01-26T20:17:41.857126Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:17:54.397550Z",
     "start_time": "2019-01-26T20:17:54.391621Z"
    }
   },
   "outputs": [],
   "source": [
    "# for row in experiments.values:\n",
    "debug_model_id = 5323\n",
    "\n",
    "row = list(experiments[experiments['model_id'] == debug_model_id].values[0])\n",
    "\n",
    "# get experiment params from dataframe row\n",
    "experiment = dict(zip(experiments.columns, row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T20:18:00.156408Z",
     "start_time": "2019-01-26T20:17:58.203526Z"
    }
   },
   "outputs": [],
   "source": [
    "print(str(experiment[\"model_id\"]) + \"   \" + \"X\"*60)\n",
    "#logging.info(\"Begin experiment for model_id={} on GPU:{} \".format(experiment['model_id'], os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n",
    "print(experiment)\n",
    "\n",
    "architecture = Architecture(model_id = experiment['model_id'], \n",
    "                            architecture = experiment['architecture'], \n",
    "                            sequence_length = experiment['sequence_length'], \n",
    "                            pretrained_model_name = experiment['pretrained_model_name'],\n",
    "                            pooling = experiment['pooling'],\n",
    "                            sequence_model = experiment['sequence_model'],\n",
    "                            sequence_model_layers = experiment['sequence_model_layers'],\n",
    "                            layer_1_size = experiment['layer_1_size'],\n",
    "                            layer_2_size = experiment['layer_2_size'],\n",
    "                            layer_3_size = experiment['layer_3_size'],\n",
    "                            dropout = experiment['dropout'],\n",
    "                            verbose=True,\n",
    "                            batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T23:43:08.594740Z",
     "start_time": "2019-01-26T20:18:02.583682Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "architecture.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
