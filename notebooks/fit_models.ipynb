{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD LAYER COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T23:04:43.457585Z",
     "start_time": "2018-12-17T23:04:41.557901Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from shutil import copy\n",
    "from sklearn.utils import shuffle\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T23:04:45.024654Z",
     "start_time": "2018-12-17T23:04:43.459999Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Dropout, ZeroPadding3D\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.convolutional import (Conv2D, MaxPooling3D, Conv3D, MaxPooling2D)\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T23:04:45.274000Z",
     "start_time": "2018-12-17T23:04:45.027521Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T23:04:45.279621Z",
     "start_time": "2018-12-17T23:04:45.276857Z"
    }
   },
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T23:04:45.648288Z",
     "start_time": "2018-12-17T23:04:45.282037Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup matplotlib to display plots in the notebook\n",
    "# %matplotlib inline\n",
    "\n",
    "# third party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# setup display options\n",
    "pd.options.display.max_rows = 200\n",
    "pd.options.display.max_colwidth = 400\n",
    "pd.options.display.float_format = '{:,.5g}'.format\n",
    "np.set_printoptions(precision=5, suppress=False)\n",
    "\n",
    "# setup seaborn to use matplotlib defaults & styles\n",
    "sns.set()\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T23:04:45.655653Z",
     "start_time": "2018-12-17T23:04:45.650915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/seals/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd = os.path.dirname(os.getcwd()) + '/'\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T23:04:45.661521Z",
     "start_time": "2018-12-17T23:04:45.658059Z"
    }
   },
   "outputs": [],
   "source": [
    "path_cache = pwd + 'cache/'\n",
    "path_models = pwd + 'models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T23:04:45.678397Z",
     "start_time": "2018-12-17T23:04:45.674706Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"{0}/{1}.log\".format(pwd, \"logs\")),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T23:04:45.683655Z",
     "start_time": "2018-12-17T23:04:45.680518Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_pic(imgpath):\n",
    "    plt.imshow(plt.imread(imgpath))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T23:04:46.256534Z",
     "start_time": "2018-12-17T23:04:46.226175Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_model(model_id, architecture, layer_1_sizefactor, layer_2_sizefactor, layer_3_sizefactor, dropout, sequence_length, pretrained_model_name, pooling):\n",
    "\n",
    "    ###########################\n",
    "    ### create folder for model \n",
    "    ###########################\n",
    "\n",
    "    path_model = path_models + str(model_id) + '/'\n",
    "    if not os.path.exists(path_model):\n",
    "        os.makedirs(path_model)\n",
    "\n",
    "    ###########################\n",
    "    ### create train/test split\n",
    "    ###########################\n",
    "\n",
    "    x_train, y_train = get_sequence_data_for_vids(vids_train, sequence_length, pretrained_model_name, pooling)\n",
    "    x_valid, y_valid = get_sequence_data_for_vids(vids_valid, sequence_length, pretrained_model_name, pooling)\n",
    "    x_test, y_test = get_sequence_data_for_vids(vids_test, sequence_length, pretrained_model_name, pooling)\n",
    "\n",
    "    # shuffle test and train batches\n",
    "    x_train, y_train = shuffle(x_train, y_train)\n",
    "    x_valid, y_valid = shuffle(x_valid, y_valid)\n",
    "\n",
    "    # CREATE CLASS BALANCE\n",
    "#     y_train = pd.DataFrame(y_train)\n",
    "#     keeps = list(y_train[y_train[0]==1].head(int(len(y_train[y_train[0]==1].index)/2)).index)\n",
    "#     keeps2 = list(y_train[y_train[0] == 0].index)\n",
    "#     keeps.extend(keeps2)\n",
    "#     #\n",
    "#     y_train = y_train.iloc[keeps]\n",
    "#     y_train = y_train.values\n",
    "#     x_train = x_train[keeps,:,:]\n",
    "\n",
    "    NUM_CLASSES = y_train.shape[1]\n",
    "    NUM_FEATURES = x_train.shape[2]\n",
    "    SEQ_LENGTH = x_train.shape[1]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ##############################\n",
    "    ### keep track of model params\n",
    "    ##############################\n",
    "\n",
    "    # create dict with model parameters\n",
    "    results = {}\n",
    "\n",
    "    results['id'] = str(model_id)\n",
    "\n",
    "    NUM_EPOCHS = 50\n",
    "    BATCH_SIZE = 32\n",
    "    results['fit_batch_size'] = BATCH_SIZE\n",
    "    \n",
    "    results['fit_num_classes'] = NUM_CLASSES\n",
    "    results['model_num_features'] = NUM_FEATURES\n",
    "    results['model_sequence_length'] = SEQ_LENGTH\n",
    "    \n",
    "    results['pretrained_model_name'] = pretrained_model_name\n",
    "    results['pretrained_model_pooling'] = pooling\n",
    "\n",
    "    results['shape_y_train'] = str(y_train.shape)\n",
    "    results['shape_x_train'] = str(x_train.shape)\n",
    "    results['shape_x_valid'] = str(x_valid.shape)\n",
    "    results['shape_x_test'] = str(x_test.shape)\n",
    "\n",
    "    results['model_architecture'] = architecture\n",
    "    results['model_layer_1_sizefactor'] = layer_1_sizefactor\n",
    "    results['model_layer_2_sizefactor'] = layer_2_sizefactor\n",
    "    results['model_layer_3_sizefactor'] = layer_3_sizefactor\n",
    "    results['model_dropout'] = dropout\n",
    "\n",
    "    ################\n",
    "    ### define model\n",
    "    ################\n",
    "\n",
    "    if architecture == \"LSTM\":\n",
    "        # https://github.com/sagarvegad/Video-Classification-CNN-and-LSTM-/blob/master/train_CNN_RNN.py\n",
    "        model = Sequential()\n",
    "\n",
    "        # layer 1 (LSTM layer)\n",
    "        model.add(LSTM(NUM_FEATURES//layer_1_sizefactor, return_sequences=False, dropout=dropout, input_shape=(SEQ_LENGTH, NUM_FEATURES)))\n",
    "\n",
    "        # layer 2 (dense)\n",
    "        if layer_2_sizefactor > 0:\n",
    "            model.add(Dropout(dropout))\n",
    "            model.add(Dense(NUM_FEATURES//layer_2_sizefactor, activation='relu'))\n",
    "\n",
    "        # layer 3 (dense)\n",
    "        if layer_2_sizefactor > 0 and layer_3_sizefactor > 0:\n",
    "            model.add(Dropout(dropout))\n",
    "            model.add(Dense(NUM_FEATURES//layer_3_sizefactor, activation='relu'))\n",
    "\n",
    "        # final layer\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        # define optimizer and compile model\n",
    "        opt = Adam()\n",
    "        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    if architecture == 'MLP':\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=(SEQ_LENGTH, NUM_FEATURES)))\n",
    "        model.add(Dense(NUM_FEATURES//2, activation='relu'))\n",
    "\n",
    "        if layer_2_sizefactor > 0:\n",
    "            model.add(Dense(NUM_FEATURES//layer_2_sizefactor, activation='relu'))\n",
    "            model.add(Dropout(dropout))\n",
    "\n",
    "        if layer_2_sizefactor > 0 and layer_3_sizefactor > 0:\n",
    "            model.add(Dense(NUM_FEATURES//layer_3_sizefactor, activation='relu'))\n",
    "            model.add(Dropout(dropout))\n",
    "\n",
    "        # final layer\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        # define optimizer and compile model\n",
    "        opt = Adam()\n",
    "        model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # save model summary to file\n",
    "    with open(path_model + 'model_summary.txt', 'w') as f:\n",
    "        with redirect_stdout(f):\n",
    "            model.summary()\n",
    "\n",
    "    # track number of params in model\n",
    "    results['param_count'] = model.count_params()\n",
    "\n",
    "    #############\n",
    "    ### fit model\n",
    "    #############\n",
    "\n",
    "    # setup training callbacks\n",
    "    stopper_patience = 10\n",
    "    results['fit_stopper_patience'] = stopper_patience\n",
    "    callback_stopper = EarlyStopping(monitor='val_acc', patience=stopper_patience, verbose=0)\n",
    "    callback_csvlogger = CSVLogger(path_model + 'training.log')\n",
    "    callback_checkpointer = ModelCheckpoint(path_model +  'model.h5', monitor='val_acc', \n",
    "                                 save_best_only=True, verbose=0)\n",
    "    \n",
    "    # start time training\n",
    "    start = dt.datetime.now()\n",
    "    results['fit_train_dt_start'] = start.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    history = model.fit(x_train, y_train, \n",
    "              validation_data=(x_valid,y_valid),\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=NUM_EPOCHS,\n",
    "              callbacks=[callback_stopper, callback_checkpointer, callback_csvlogger],\n",
    "              shuffle=True,\n",
    "              verbose=0)\n",
    "    \n",
    "    # end time training\n",
    "    end = dt.datetime.now()    \n",
    "    results['fit_train_dt_end']  = end.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    results['fit_train_dt_duration']  = str((end - start).total_seconds()).split(\".\")[0]\n",
    "\n",
    "    # get number of epochs actually trained (might have early stopped)\n",
    "    epochs_trained = 0\n",
    "    epochs_trained = callback_stopper.stopped_epoch - stopper_patience\n",
    "\n",
    "    results['fit_stopped_early'] = True\n",
    "    if epochs_trained == 0 and len(history.history) > stopper_patience:\n",
    "        results['fit_stopped_early'] = False\n",
    "        epochs_trained = NUM_EPOCHS - 1 \n",
    "\n",
    "    results['fit_num_epochs'] = epochs_trained\n",
    "    results['fit_val_acc'] = history.history['val_acc'][epochs_trained]\n",
    "    results['fit_train_acc'] = history.history['acc'][epochs_trained]\n",
    "    results['fit_val_loss'] = history.history['val_loss'][epochs_trained]\n",
    "    results['fit_train_loss'] = history.history['loss'][epochs_trained]\n",
    "    \n",
    "\n",
    "    #######################\n",
    "    ### predict on test set\n",
    "    #######################\n",
    "    \n",
    "    \n",
    "    # start time inference\n",
    "    start = dt.datetime.now()\n",
    "    results['fit_test_dt_start'] = start.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # calculate predictions on test set\n",
    "    predictions = model.predict(x_test)\n",
    "    \n",
    "    # end time training\n",
    "    end = dt.datetime.now()    \n",
    "    results['fit_test_dt_end']  = end.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    results['fit_test_dt_duration']  = str((end - start).total_seconds()).split(\".\")[0]\n",
    "\n",
    "    # calculate test error \n",
    "    pdf = pd.DataFrame(predictions)\n",
    "    pdf.columns = ['noseal','seal']\n",
    "\n",
    "    # get filenames for predictions\n",
    "    filenames = []\n",
    "    for vid_name in vids_test:\n",
    "        filenames_vid = list(dfp[dfp['vid'] == vid_name]['path'])\n",
    "        filenames.extend(filenames_vid[sequence_length-1:])\n",
    "    pdf['filename'] = filenames\n",
    "    truth = pd.DataFrame(y_test)\n",
    "    truth.columns = ['truth_noseal','truth_seal']\n",
    "    truth = truth[['truth_seal']]\n",
    "    pdf['prediction'] = pdf['seal'].apply(lambda x: round(x))\n",
    "    pdf = pd.concat([pdf, truth], axis=1)\n",
    "    pdf['error'] = (pdf['prediction'] != pdf['truth_seal']).astype(int)\n",
    "\n",
    "    test_acc = 1 - pdf['error'].mean()\n",
    "\n",
    "    pdf.to_csv(path_model + 'test_predictions.csv')\n",
    "\n",
    "    results['fit_test_acc'] = 1 - pdf['error'].mean()\n",
    "    logger.info(\"model {} test acc: {}\".format(model_id, test_acc))\n",
    "\n",
    "    ###################################\n",
    "    ### save experiment results to file\n",
    "    ###################################\n",
    "    # log results\n",
    "    logger.info(json.dumps(results))\n",
    "    \n",
    "    with open(path_model + 'params.json', 'w') as f:\n",
    "        json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T23:04:46.262507Z",
     "start_time": "2018-12-17T23:04:46.259144Z"
    }
   },
   "outputs": [],
   "source": [
    "# pretrained_model_names = [\"inception_resnet_v2\", \"inception_v3\", \"mobilenetv2_1.00_224\", \"resnet50\", \"vgg16\", \"xception\"]\n",
    "# poolings = ['avg','max']\n",
    "# sequence_lengths = [1, 3, 5, 10, 15, 20, 40]\n",
    "\n",
    "# architectures = ['LSTM', \"MLP\"]\n",
    "# layer_1_sizefactors = [1,2,4,8]\n",
    "# layer_2_sizefactors = [0,1,2,4,8]\n",
    "# layer_3_sizefactors = [0,1,2,4,8]\n",
    "# dropouts = [0, 0.1, 0.2,0.3,0.4,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T23:04:46.269073Z",
     "start_time": "2018-12-17T23:04:46.264605Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model_names = [\"inception_resnet_v2\"]\n",
    "poolings = ['avg']\n",
    "sequence_lengths = [1, 3]\n",
    "\n",
    "architectures = [\"MLP\", \"SimpleRNN\", \"GRU\", \"LSTM\", \"Convolution1D\", \"AtrousConvolution1D\"]\n",
    "\n",
    "layer_1_types = ['dense','sequence']\n",
    "layer_1_sizefactors = [1,2,4,8]\n",
    "\n",
    "layer_2_types = ['dense','sequence']\n",
    "layer_2_sizefactors = [0,2,4,8]\n",
    "\n",
    "layer_3_types = ['dense','sequence']\n",
    "layer_3_sizefactors = [0,2,4,8]\n",
    "dropouts = [0.2, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T23:04:46.277417Z",
     "start_time": "2018-12-17T23:04:46.271109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_count_total = 0\n",
    "for pretrained_model_name in pretrained_model_names:\n",
    "    for pooling in poolings:\n",
    "        for sequence_length in sequence_lengths:\n",
    "            for architecture in architectures:\n",
    "                for layer_1_sizefactor in layer_1_sizefactors:\n",
    "                    for layer_2_sizefactor in layer_2_sizefactors:\n",
    "                        for layer_3_sizefactor in layer_3_sizefactors:\n",
    "                            for dropout in dropouts:\n",
    "                                if sequence_length == 1 and architecture == \"LSTM\":\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    experiment_count_total+=1\n",
    "experiment_count_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T23:04:46.658818Z",
     "start_time": "2018-12-17T23:04:46.280627Z"
    }
   },
   "outputs": [],
   "source": [
    "model_id = 1\n",
    "experiment_count = 1\n",
    "\n",
    "for pretrained_model_name in pretrained_model_names:\n",
    "    for pooling in poolings:\n",
    "        for sequence_length in sequence_lengths:\n",
    "            for architecture in architectures:\n",
    "                for layer_1_sizefactor in layer_1_sizefactors:\n",
    "                    for layer_2_sizefactor in layer_2_sizefactors:\n",
    "                        for layer_3_sizefactor in layer_3_sizefactors:\n",
    "                            for dropout in dropouts:\n",
    "                                \n",
    "                                # skip LSTM experiement if not a sequence\n",
    "                                if sequence_length == 1 and architecture == \"LSTM\":\n",
    "                                    continue\n",
    "                                \n",
    "                                # log experiment\n",
    "                                param_names = [\"model_id\", \"architecture\", \"layer_1_sizefactor\", \"layer_2_sizefactor\", \"layer_3_sizefactor\", \"dropout\", \"sequence_length\", \"pretrained_model_name\", \"pooling\"]\n",
    "                                param_values = [str(x) for x in [model_id, architecture, layer_1_sizefactor, layer_2_sizefactor, layer_3_sizefactor, dropout, sequence_length, pretrained_model_name, pooling]]\n",
    "                                experiment_description = \"\"\n",
    "                                for c, p in enumerate(param_names):\n",
    "                                    experiment_description += p + ': ' + param_values[c] + ', '\n",
    "\n",
    "                                # only run experiment if results not already computed\n",
    "                                if not os.path.exists(path_models + str(model_id) + '/params.json'):\n",
    "                                    # run experiment\n",
    "                                    logging.info(\"begin experiment {}/{} - {}\".format(experiment_count, experiment_count_total, experiment_description))\n",
    "                                    fit_model(model_id, architecture, layer_1_sizefactor, layer_2_sizefactor, layer_3_sizefactor, dropout, sequence_length, pretrained_model_name, pooling)\n",
    "                                \n",
    "                                experiment_count += 1\n",
    "                                model_id+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"vgg16\"\n",
    "pooling = 'avg'\n",
    "sequence_length = 20\n",
    "\n",
    "layer_1_sizefactor = 2\n",
    "layer_2_sizefactor = 2\n",
    "layer_3_sizefactor = 2\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = 'SimpleRNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### create folder for model \n",
    "###########################\n",
    "\n",
    "path_model = path_models + str(model_id) + '/'\n",
    "if os.path.exists(path_model):\n",
    "    rmtree(path_model)\n",
    "if not os.path.exists(path_model):\n",
    "    os.makedirs(path_model)\n",
    "\n",
    "###########################\n",
    "### create train/test split\n",
    "###########################\n",
    "\n",
    "x_train, y_train = get_sequence_data_for_vids(vids_train, sequence_length, pretrained_model_name, pooling)\n",
    "x_valid, y_valid = get_sequence_data_for_vids(vids_valid, sequence_length, pretrained_model_name, pooling)\n",
    "x_test, y_test = get_sequence_data_for_vids(vids_test, sequence_length, pretrained_model_name, pooling)\n",
    "\n",
    "# shuffle test and train batches\n",
    "x_train, y_train = shuffle(x_train, y_train)\n",
    "x_valid, y_valid = shuffle(x_valid, y_valid)\n",
    "\n",
    "# CREATE CLASS BALANCE\n",
    "#     y_train = pd.DataFrame(y_train)\n",
    "#     keeps = list(y_train[y_train[0]==1].head(int(len(y_train[y_train[0]==1].index)/2)).index)\n",
    "#     keeps2 = list(y_train[y_train[0] == 0].index)\n",
    "#     keeps.extend(keeps2)\n",
    "#     #\n",
    "#     y_train = y_train.iloc[keeps]\n",
    "#     y_train = y_train.values\n",
    "#     x_train = x_train[keeps,:,:]\n",
    "\n",
    "NUM_CLASSES = y_train.shape[1]\n",
    "NUM_FEATURES = x_train.shape[2]\n",
    "SEQ_LENGTH = x_train.shape[1]\n",
    "\n",
    "\n",
    "##############################\n",
    "### keep track of model params\n",
    "##############################\n",
    "\n",
    "# create dict with model parameters\n",
    "results = {}\n",
    "\n",
    "results['id'] = str(model_id)\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "results['fit_batch_size'] = BATCH_SIZE\n",
    "\n",
    "results['fit_num_classes'] = NUM_CLASSES\n",
    "results['model_num_features'] = NUM_FEATURES\n",
    "results['model_sequence_length'] = SEQ_LENGTH\n",
    "\n",
    "results['pretrained_model_name'] = pretrained_model_name\n",
    "results['pretrained_model_pooling'] = pooling\n",
    "\n",
    "results['shape_y_train'] = str(y_train.shape)\n",
    "results['shape_x_train'] = str(x_train.shape)\n",
    "results['shape_x_valid'] = str(x_valid.shape)\n",
    "results['shape_x_test'] = str(x_test.shape)\n",
    "\n",
    "results['model_architecture'] = architecture\n",
    "results['model_layer_1_sizefactor'] = layer_1_sizefactor\n",
    "results['model_layer_2_sizefactor'] = layer_2_sizefactor\n",
    "results['model_layer_3_sizefactor'] = layer_3_sizefactor\n",
    "results['model_dropout'] = dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/sagarvegad/Video-Classification-CNN-and-LSTM-/blob/master/train_CNN_RNN.py\n",
    "model = Sequential()\n",
    "\n",
    "# layer 1 (LSTM layer)\n",
    "model.add(SimpleRNN(NUM_FEATURES//layer_1_sizefactor, return_sequences=True, dropout=dropout, input_shape=(SEQ_LENGTH, NUM_FEATURES)))\n",
    "\n",
    "if layer_2_sizefactor > 0:\n",
    "    model.add(SimpleRNN(NUM_FEATURES//layer_2_sizefactor, return_sequences=True, dropout=dropout))\n",
    "\n",
    "if layer_3_sizefactor > 0:\n",
    "    model.add(SimpleRNN(NUM_FEATURES//layer_3_sizefactor, dropout=dropout))\n",
    "    \n",
    "# final layer\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "# define optimizer and compile model\n",
    "opt = Adam()\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lstm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/sagarvegad/Video-Classification-CNN-and-LSTM-/blob/master/train_CNN_RNN.py\n",
    "model = Sequential()\n",
    "\n",
    "# layer 1 (LSTM layer)\n",
    "model.add(LSTM(NUM_FEATURES//layer_1_sizefactor, return_sequences=False, dropout=dropout, input_shape=(SEQ_LENGTH, NUM_FEATURES)))\n",
    "model.add(Dense(NUM_FEATURES//layer_2_sizefactor, activation='relu'))\n",
    "    \n",
    "# final layer\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "# define optimizer and compile model\n",
    "opt = Adam()\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9086 samples, validate on 2233 samples\n",
      "Epoch 1/50\n",
      "9086/9086 [==============================] - 18s 2ms/step - loss: 0.4642 - acc: 0.7732 - val_loss: 0.9810 - val_acc: 0.3166\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.31661, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 2/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.3833 - acc: 0.8240 - val_loss: 0.6104 - val_acc: 0.6305\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.31661 to 0.63054, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 3/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.3298 - acc: 0.8578 - val_loss: 0.4486 - val_acc: 0.7913\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.63054 to 0.79131, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 4/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.3059 - acc: 0.8692 - val_loss: 0.3389 - val_acc: 0.8786\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.79131 to 0.87864, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 5/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2845 - acc: 0.8840 - val_loss: 0.2855 - val_acc: 0.9180\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.87864 to 0.91805, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 6/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2808 - acc: 0.8844 - val_loss: 0.2309 - val_acc: 0.9257\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.91805 to 0.92566, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 7/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2704 - acc: 0.8920 - val_loss: 0.2290 - val_acc: 0.9163\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.92566\n",
      "Epoch 8/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2646 - acc: 0.8934 - val_loss: 0.2387 - val_acc: 0.9283\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.92566 to 0.92835, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 9/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2564 - acc: 0.8996 - val_loss: 0.2044 - val_acc: 0.9342\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.92835 to 0.93417, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 10/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2495 - acc: 0.8996 - val_loss: 0.2059 - val_acc: 0.9288\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.93417\n",
      "Epoch 11/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2529 - acc: 0.8987 - val_loss: 0.1906 - val_acc: 0.9369\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.93417 to 0.93686, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 12/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2466 - acc: 0.9014 - val_loss: 0.1912 - val_acc: 0.9328\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.93686\n",
      "Epoch 13/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2426 - acc: 0.9024 - val_loss: 0.5729 - val_acc: 0.7559\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.93686\n",
      "Epoch 14/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2331 - acc: 0.9095 - val_loss: 0.2139 - val_acc: 0.9351\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.93686\n",
      "Epoch 15/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2313 - acc: 0.9071 - val_loss: 0.2018 - val_acc: 0.9292\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.93686\n",
      "Epoch 16/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2271 - acc: 0.9081 - val_loss: 0.1878 - val_acc: 0.9351\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.93686\n",
      "Epoch 17/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2122 - acc: 0.9184 - val_loss: 0.1906 - val_acc: 0.9369\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.93686 to 0.93686, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 18/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2158 - acc: 0.9144 - val_loss: 0.4391 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.93686\n",
      "Epoch 19/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2129 - acc: 0.9173 - val_loss: 0.2900 - val_acc: 0.8880\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.93686\n",
      "Epoch 20/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2135 - acc: 0.9168 - val_loss: 0.2495 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.93686\n",
      "Epoch 21/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2175 - acc: 0.9164 - val_loss: 0.3665 - val_acc: 0.8509\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.93686\n",
      "Epoch 22/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2108 - acc: 0.9148 - val_loss: 0.3212 - val_acc: 0.8648\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.93686\n",
      "Epoch 23/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2106 - acc: 0.9189 - val_loss: 0.1870 - val_acc: 0.9378\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.93686 to 0.93775, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 24/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2094 - acc: 0.9173 - val_loss: 0.6010 - val_acc: 0.7313\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.93775\n",
      "Epoch 25/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.1967 - acc: 0.9221 - val_loss: 0.2000 - val_acc: 0.9328\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.93775\n",
      "Epoch 26/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2059 - acc: 0.9194 - val_loss: 0.2617 - val_acc: 0.8997\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.93775\n",
      "Epoch 27/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.1912 - acc: 0.9238 - val_loss: 0.2098 - val_acc: 0.9257\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.93775\n",
      "Epoch 28/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.1959 - acc: 0.9222 - val_loss: 0.1830 - val_acc: 0.9373\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.93775\n",
      "Epoch 29/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.1950 - acc: 0.9237 - val_loss: 0.2197 - val_acc: 0.9163\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.93775\n",
      "Epoch 30/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.1933 - acc: 0.9266 - val_loss: 0.3129 - val_acc: 0.8854\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.93775\n",
      "Epoch 31/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.1876 - acc: 0.9280 - val_loss: 0.2800 - val_acc: 0.8903\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.93775\n",
      "Epoch 32/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.1826 - acc: 0.9299 - val_loss: 0.1978 - val_acc: 0.9337\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.93775\n",
      "Epoch 33/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.1868 - acc: 0.9267 - val_loss: 0.2241 - val_acc: 0.9145\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.93775\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "### fit model\n",
    "#############\n",
    "\n",
    "# setup training callbacks\n",
    "stopper_patience = 10\n",
    "results['fit_stopper_patience'] = stopper_patience\n",
    "callback_stopper = EarlyStopping(monitor='val_acc', patience=stopper_patience, verbose=0)\n",
    "callback_csvlogger = CSVLogger(path_model + 'training.log')\n",
    "callback_checkpointer = ModelCheckpoint(path_model +  'model.h5', monitor='val_acc', \n",
    "                             save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "          validation_data=(x_valid,y_valid),\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          callbacks=[callback_stopper, callback_checkpointer, callback_csvlogger],\n",
    "          shuffle=True,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lstm 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/sagarvegad/Video-Classification-CNN-and-LSTM-/blob/master/train_CNN_RNN.py\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(NUM_FEATURES//layer_1_sizefactor, return_sequences=True, dropout=dropout, input_shape=(SEQ_LENGTH, NUM_FEATURES)))\n",
    "model.add(LSTM(NUM_FEATURES//layer_2_sizefactor, return_sequences=False, dropout=dropout))\n",
    "    \n",
    "# final layer\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "# define optimizer and compile model\n",
    "opt = Adam()\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9086 samples, validate on 2233 samples\n",
      "Epoch 1/50\n",
      "9086/9086 [==============================] - 33s 4ms/step - loss: 0.4656 - acc: 0.7769 - val_loss: 0.2662 - val_acc: 0.9104\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91043, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 2/50\n",
      "9086/9086 [==============================] - 31s 3ms/step - loss: 0.3662 - acc: 0.8405 - val_loss: 0.4527 - val_acc: 0.7958\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.91043\n",
      "Epoch 3/50\n",
      "9086/9086 [==============================] - 31s 3ms/step - loss: 0.3282 - acc: 0.8613 - val_loss: 0.2630 - val_acc: 0.9207\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.91043 to 0.92073, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 4/50\n",
      "9086/9086 [==============================] - 31s 3ms/step - loss: 0.2842 - acc: 0.8805 - val_loss: 0.3580 - val_acc: 0.8683\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.92073\n",
      "Epoch 5/50\n",
      "9086/9086 [==============================] - 31s 3ms/step - loss: 0.2822 - acc: 0.8852 - val_loss: 0.2608 - val_acc: 0.9207\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.92073\n",
      "Epoch 6/50\n",
      "9086/9086 [==============================] - 31s 3ms/step - loss: 0.2777 - acc: 0.8871 - val_loss: 0.2300 - val_acc: 0.9185\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.92073\n",
      "Epoch 7/50\n",
      "9086/9086 [==============================] - 31s 3ms/step - loss: 0.2567 - acc: 0.8969 - val_loss: 0.2591 - val_acc: 0.9042\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.92073\n",
      "Epoch 8/50\n",
      "9086/9086 [==============================] - 31s 3ms/step - loss: 0.2516 - acc: 0.9007 - val_loss: 0.2145 - val_acc: 0.9310\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.92073 to 0.93103, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 9/50\n",
      "9086/9086 [==============================] - 31s 3ms/step - loss: 0.2442 - acc: 0.9024 - val_loss: 0.2808 - val_acc: 0.9046\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.93103\n",
      "Epoch 10/50\n",
      "9086/9086 [==============================] - 31s 3ms/step - loss: 0.2418 - acc: 0.9050 - val_loss: 0.1990 - val_acc: 0.9373\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.93103 to 0.93730, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 11/50\n",
      "9086/9086 [==============================] - 31s 3ms/step - loss: 0.2307 - acc: 0.9093 - val_loss: 0.2122 - val_acc: 0.9207\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.93730\n",
      "Epoch 12/50\n",
      "9086/9086 [==============================] - 32s 3ms/step - loss: 0.2261 - acc: 0.9122 - val_loss: 0.2767 - val_acc: 0.8939\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.93730\n",
      "Epoch 13/50\n",
      "9086/9086 [==============================] - 31s 3ms/step - loss: 0.2261 - acc: 0.9115 - val_loss: 0.2088 - val_acc: 0.9288\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.93730\n",
      "Epoch 14/50\n",
      "9086/9086 [==============================] - 31s 3ms/step - loss: 0.2282 - acc: 0.9062 - val_loss: 0.3552 - val_acc: 0.8657\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.93730\n",
      "Epoch 15/50\n",
      "9086/9086 [==============================] - 31s 3ms/step - loss: 0.2206 - acc: 0.9126 - val_loss: 0.2572 - val_acc: 0.9154\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.93730\n",
      "Epoch 16/50\n",
      "9086/9086 [==============================] - 31s 3ms/step - loss: 0.2174 - acc: 0.9140 - val_loss: 0.2039 - val_acc: 0.9292\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.93730\n",
      "Epoch 17/50\n",
      "9086/9086 [==============================] - 31s 3ms/step - loss: 0.1914 - acc: 0.9252 - val_loss: 0.2136 - val_acc: 0.9292\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.93730\n",
      "Epoch 18/50\n",
      "9086/9086 [==============================] - 31s 3ms/step - loss: 0.1952 - acc: 0.9226 - val_loss: 0.2489 - val_acc: 0.9095\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.93730\n",
      "Epoch 19/50\n",
      "9086/9086 [==============================] - 31s 3ms/step - loss: 0.1866 - acc: 0.9309 - val_loss: 0.2930 - val_acc: 0.8822\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.93730\n",
      "Epoch 20/50\n",
      "9086/9086 [==============================] - 31s 3ms/step - loss: 0.1910 - acc: 0.9279 - val_loss: 0.2212 - val_acc: 0.9212\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.93730\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "### fit model\n",
    "#############\n",
    "\n",
    "# setup training callbacks\n",
    "stopper_patience = 10\n",
    "results['fit_stopper_patience'] = stopper_patience\n",
    "callback_stopper = EarlyStopping(monitor='val_acc', patience=stopper_patience, verbose=0)\n",
    "callback_csvlogger = CSVLogger(path_model + 'training.log')\n",
    "callback_checkpointer = ModelCheckpoint(path_model +  'model.h5', monitor='val_acc', \n",
    "                             save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "          validation_data=(x_valid,y_valid),\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          callbacks=[callback_stopper, callback_checkpointer, callback_csvlogger],\n",
    "          shuffle=True,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9086 samples, validate on 2233 samples\n",
      "Epoch 1/50\n",
      "9086/9086 [==============================] - 19s 2ms/step - loss: 0.5982 - acc: 0.7006 - val_loss: 0.3854 - val_acc: 0.8795\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.87953, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 2/50\n",
      "9086/9086 [==============================] - 18s 2ms/step - loss: 0.4910 - acc: 0.7761 - val_loss: 0.2980 - val_acc: 0.9033\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.87953 to 0.90327, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 3/50\n",
      "9086/9086 [==============================] - 18s 2ms/step - loss: 0.4234 - acc: 0.8101 - val_loss: 0.3401 - val_acc: 0.8961\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.90327\n",
      "Epoch 4/50\n",
      "9086/9086 [==============================] - 18s 2ms/step - loss: 0.4153 - acc: 0.8152 - val_loss: 0.4229 - val_acc: 0.8992\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.90327\n",
      "Epoch 5/50\n",
      "9086/9086 [==============================] - 19s 2ms/step - loss: 0.3849 - acc: 0.8338 - val_loss: 0.2970 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.90327 to 0.91939, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 6/50\n",
      "9086/9086 [==============================] - 18s 2ms/step - loss: 0.3828 - acc: 0.8327 - val_loss: 0.2379 - val_acc: 0.9225\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.91939 to 0.92253, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 7/50\n",
      "9086/9086 [==============================] - 18s 2ms/step - loss: 0.3816 - acc: 0.8352 - val_loss: 0.4064 - val_acc: 0.8164\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.92253\n",
      "Epoch 8/50\n",
      "9086/9086 [==============================] - 18s 2ms/step - loss: 0.3773 - acc: 0.8360 - val_loss: 0.2671 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.92253\n",
      "Epoch 9/50\n",
      "9086/9086 [==============================] - 18s 2ms/step - loss: 0.3892 - acc: 0.8344 - val_loss: 0.2580 - val_acc: 0.9189\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.92253\n",
      "Epoch 10/50\n",
      "9086/9086 [==============================] - 18s 2ms/step - loss: 0.4676 - acc: 0.7900 - val_loss: 0.5400 - val_acc: 0.7989\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.92253\n",
      "Epoch 11/50\n",
      "9086/9086 [==============================] - 18s 2ms/step - loss: 0.4092 - acc: 0.8263 - val_loss: 0.3103 - val_acc: 0.8898\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.92253\n",
      "Epoch 12/50\n",
      "9086/9086 [==============================] - 18s 2ms/step - loss: 0.4053 - acc: 0.8286 - val_loss: 0.5009 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.92253\n",
      "Epoch 13/50\n",
      "9086/9086 [==============================] - 18s 2ms/step - loss: 0.3812 - acc: 0.8394 - val_loss: 0.3183 - val_acc: 0.8992\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.92253\n",
      "Epoch 14/50\n",
      "9086/9086 [==============================] - 18s 2ms/step - loss: 0.4012 - acc: 0.8316 - val_loss: 0.2493 - val_acc: 0.9216\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.92253\n",
      "Epoch 15/50\n",
      "9086/9086 [==============================] - 18s 2ms/step - loss: 0.3983 - acc: 0.8291 - val_loss: 0.3145 - val_acc: 0.9086\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.92253\n",
      "Epoch 16/50\n",
      "9086/9086 [==============================] - 18s 2ms/step - loss: 0.3772 - acc: 0.8402 - val_loss: 0.4392 - val_acc: 0.8567\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.92253\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "### fit model\n",
    "#############\n",
    "\n",
    "# setup training callbacks\n",
    "stopper_patience = 10\n",
    "results['fit_stopper_patience'] = stopper_patience\n",
    "callback_stopper = EarlyStopping(monitor='val_acc', patience=stopper_patience, verbose=0)\n",
    "callback_csvlogger = CSVLogger(path_model + 'training.log')\n",
    "callback_checkpointer = ModelCheckpoint(path_model +  'model.h5', monitor='val_acc', \n",
    "                             save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "          validation_data=(x_valid,y_valid),\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          callbacks=[callback_stopper, callback_checkpointer, callback_csvlogger],\n",
    "          shuffle=True,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacked simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9086 samples, validate on 2233 samples\n",
      "Epoch 1/50\n",
      "9086/9086 [==============================] - 14s 1ms/step - loss: 0.5599 - acc: 0.7124 - val_loss: 0.4046 - val_acc: 0.9069\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.90685, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 2/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.4468 - acc: 0.7965 - val_loss: 0.3334 - val_acc: 0.9060\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.90685\n",
      "Epoch 3/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.4118 - acc: 0.8134 - val_loss: 0.3303 - val_acc: 0.8939\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.90685\n",
      "Epoch 4/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.4080 - acc: 0.8172 - val_loss: 0.4440 - val_acc: 0.9046\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.90685\n",
      "Epoch 5/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.3890 - acc: 0.8282 - val_loss: 0.2766 - val_acc: 0.9145\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.90685 to 0.91446, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 6/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.3973 - acc: 0.8257 - val_loss: 0.4332 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.91446\n",
      "Epoch 7/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.3941 - acc: 0.8257 - val_loss: 0.3356 - val_acc: 0.8880\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.91446\n",
      "Epoch 8/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.3792 - acc: 0.8388 - val_loss: 0.3322 - val_acc: 0.8916\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.91446\n",
      "Epoch 9/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.3647 - acc: 0.8449 - val_loss: 0.2521 - val_acc: 0.9140\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.91446\n",
      "Epoch 10/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.3540 - acc: 0.8529 - val_loss: 0.3012 - val_acc: 0.9131\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.91446\n",
      "Epoch 11/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.3505 - acc: 0.8544 - val_loss: 0.2855 - val_acc: 0.9154\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.91446 to 0.91536, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 12/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.3319 - acc: 0.8664 - val_loss: 0.2448 - val_acc: 0.9140\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.91536\n",
      "Epoch 13/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.3442 - acc: 0.8566 - val_loss: 0.3560 - val_acc: 0.8934\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.91536\n",
      "Epoch 14/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.3450 - acc: 0.8520 - val_loss: 0.3426 - val_acc: 0.9064\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.91536\n",
      "Epoch 15/50\n",
      "9086/9086 [==============================] - 13s 1ms/step - loss: 0.3330 - acc: 0.8679 - val_loss: 0.2255 - val_acc: 0.9176\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.91536 to 0.91760, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 16/50\n",
      "9086/9086 [==============================] - 13s 1ms/step - loss: 0.3315 - acc: 0.8610 - val_loss: 0.2575 - val_acc: 0.9252\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.91760 to 0.92521, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 17/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.3399 - acc: 0.8576 - val_loss: 0.3028 - val_acc: 0.9145\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.92521\n",
      "Epoch 18/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.3200 - acc: 0.8691 - val_loss: 0.3408 - val_acc: 0.8822\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.92521\n",
      "Epoch 19/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.4026 - acc: 0.8247 - val_loss: 0.2501 - val_acc: 0.9104\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.92521\n",
      "Epoch 20/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.3722 - acc: 0.8439 - val_loss: 0.4078 - val_acc: 0.8907\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.92521\n",
      "Epoch 21/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.3341 - acc: 0.8652 - val_loss: 0.2799 - val_acc: 0.9149\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.92521\n",
      "Epoch 22/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.3360 - acc: 0.8648 - val_loss: 0.2698 - val_acc: 0.9001\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.92521\n",
      "Epoch 23/50\n",
      "9086/9086 [==============================] - 13s 1ms/step - loss: 0.3322 - acc: 0.8635 - val_loss: 0.2528 - val_acc: 0.9122\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.92521\n",
      "Epoch 24/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.3342 - acc: 0.8633 - val_loss: 0.2513 - val_acc: 0.9145\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.92521\n",
      "Epoch 25/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.3950 - acc: 0.8298 - val_loss: 0.2636 - val_acc: 0.9145\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.92521\n",
      "Epoch 26/50\n",
      "9086/9086 [==============================] - 12s 1ms/step - loss: 0.3675 - acc: 0.8498 - val_loss: 0.3473 - val_acc: 0.8768\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.92521\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "### fit model\n",
    "#############\n",
    "\n",
    "# setup training callbacks\n",
    "stopper_patience = 10\n",
    "results['fit_stopper_patience'] = stopper_patience\n",
    "callback_stopper = EarlyStopping(monitor='val_acc', patience=stopper_patience, verbose=0)\n",
    "callback_csvlogger = CSVLogger(path_model + 'training.log')\n",
    "callback_checkpointer = ModelCheckpoint(path_model +  'model.h5', monitor='val_acc', \n",
    "                             save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "          validation_data=(x_valid,y_valid),\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          callbacks=[callback_stopper, callback_checkpointer, callback_csvlogger],\n",
    "          shuffle=True,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit: simpleRNN single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9086 samples, validate on 2233 samples\n",
      "Epoch 1/50\n",
      "9086/9086 [==============================] - 8s 896us/step - loss: 0.5561 - acc: 0.7091 - val_loss: 0.4485 - val_acc: 0.8657\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.86565, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 2/50\n",
      "9086/9086 [==============================] - 7s 773us/step - loss: 0.4393 - acc: 0.7987 - val_loss: 0.4703 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.86565\n",
      "Epoch 3/50\n",
      "9086/9086 [==============================] - 7s 770us/step - loss: 0.4142 - acc: 0.8170 - val_loss: 0.3722 - val_acc: 0.9019\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.86565 to 0.90193, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 4/50\n",
      "9086/9086 [==============================] - 7s 773us/step - loss: 0.3955 - acc: 0.8315 - val_loss: 0.3905 - val_acc: 0.9033\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.90193 to 0.90327, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 5/50\n",
      "9086/9086 [==============================] - 7s 772us/step - loss: 0.4037 - acc: 0.8238 - val_loss: 0.3263 - val_acc: 0.8898\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.90327\n",
      "Epoch 6/50\n",
      "9086/9086 [==============================] - 7s 775us/step - loss: 0.3855 - acc: 0.8296 - val_loss: 0.5066 - val_acc: 0.7192\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.90327\n",
      "Epoch 7/50\n",
      "9086/9086 [==============================] - 7s 775us/step - loss: 0.3711 - acc: 0.8350 - val_loss: 0.3294 - val_acc: 0.9176\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.90327 to 0.91760, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 8/50\n",
      "9086/9086 [==============================] - 7s 772us/step - loss: 0.3797 - acc: 0.8335 - val_loss: 0.3381 - val_acc: 0.9033\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.91760\n",
      "Epoch 9/50\n",
      "9086/9086 [==============================] - 7s 774us/step - loss: 0.3585 - acc: 0.8491 - val_loss: 0.2793 - val_acc: 0.9069\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.91760\n",
      "Epoch 10/50\n",
      "9086/9086 [==============================] - 7s 770us/step - loss: 0.3531 - acc: 0.8503 - val_loss: 0.3474 - val_acc: 0.8813\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.91760\n",
      "Epoch 11/50\n",
      "9086/9086 [==============================] - 7s 768us/step - loss: 0.3621 - acc: 0.8423 - val_loss: 0.4500 - val_acc: 0.7819\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.91760\n",
      "Epoch 12/50\n",
      "9086/9086 [==============================] - 7s 768us/step - loss: 0.3489 - acc: 0.8514 - val_loss: 0.4724 - val_acc: 0.8339\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.91760\n",
      "Epoch 13/50\n",
      "9086/9086 [==============================] - 7s 771us/step - loss: 0.3441 - acc: 0.8560 - val_loss: 0.2571 - val_acc: 0.9225\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.91760 to 0.92253, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 14/50\n",
      "9086/9086 [==============================] - 7s 772us/step - loss: 0.3404 - acc: 0.8552 - val_loss: 0.2299 - val_acc: 0.9176\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.92253\n",
      "Epoch 15/50\n",
      "9086/9086 [==============================] - 7s 773us/step - loss: 0.3447 - acc: 0.8578 - val_loss: 0.2923 - val_acc: 0.9163\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.92253\n",
      "Epoch 16/50\n",
      "9086/9086 [==============================] - 7s 778us/step - loss: 0.3450 - acc: 0.8564 - val_loss: 0.3331 - val_acc: 0.9127\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.92253\n",
      "Epoch 17/50\n",
      "9086/9086 [==============================] - 7s 776us/step - loss: 0.3547 - acc: 0.8545 - val_loss: 0.2322 - val_acc: 0.9234\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.92253 to 0.92342, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 18/50\n",
      "9086/9086 [==============================] - 7s 774us/step - loss: 0.3475 - acc: 0.8512 - val_loss: 0.2619 - val_acc: 0.9095\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.92342\n",
      "Epoch 19/50\n",
      "9086/9086 [==============================] - 7s 774us/step - loss: 0.3480 - acc: 0.8482 - val_loss: 0.3199 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.92342\n",
      "Epoch 20/50\n",
      "9086/9086 [==============================] - 7s 775us/step - loss: 0.3491 - acc: 0.8500 - val_loss: 0.3254 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.92342\n",
      "Epoch 21/50\n",
      "9086/9086 [==============================] - 7s 781us/step - loss: 0.3329 - acc: 0.8590 - val_loss: 0.2572 - val_acc: 0.9189\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.92342\n",
      "Epoch 22/50\n",
      "9086/9086 [==============================] - 7s 775us/step - loss: 0.3319 - acc: 0.8668 - val_loss: 0.3385 - val_acc: 0.8849\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.92342\n",
      "Epoch 23/50\n",
      "9086/9086 [==============================] - 7s 771us/step - loss: 0.3310 - acc: 0.8675 - val_loss: 0.3848 - val_acc: 0.8419\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.92342\n",
      "Epoch 24/50\n",
      "9086/9086 [==============================] - 7s 776us/step - loss: 0.3278 - acc: 0.8657 - val_loss: 0.2728 - val_acc: 0.9140\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.92342\n",
      "Epoch 25/50\n",
      "9086/9086 [==============================] - 7s 772us/step - loss: 0.3289 - acc: 0.8622 - val_loss: 0.2485 - val_acc: 0.9180\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.92342\n",
      "Epoch 26/50\n",
      "9086/9086 [==============================] - 7s 773us/step - loss: 0.3290 - acc: 0.8648 - val_loss: 0.2187 - val_acc: 0.9369\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.92342 to 0.93686, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 27/50\n",
      " 992/9086 [==>...........................] - ETA: 5s - loss: 0.3228 - acc: 0.8629"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-b4c2e9cce6a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback_stopper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_checkpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_csvlogger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m           verbose=1)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#############\n",
    "### fit model\n",
    "#############\n",
    "\n",
    "# setup training callbacks\n",
    "stopper_patience = 10\n",
    "results['fit_stopper_patience'] = stopper_patience\n",
    "callback_stopper = EarlyStopping(monitor='val_acc', patience=stopper_patience, verbose=0)\n",
    "callback_csvlogger = CSVLogger(path_model + 'training.log')\n",
    "callback_checkpointer = ModelCheckpoint(path_model +  'model.h5', monitor='val_acc', \n",
    "                             save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "          validation_data=(x_valid,y_valid),\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          callbacks=[callback_stopper, callback_checkpointer, callback_csvlogger],\n",
    "          shuffle=True,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparison: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9086 samples, validate on 2233 samples\n",
      "Epoch 1/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.3640 - acc: 0.8393 - val_loss: 0.3798 - val_acc: 0.8665\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.86655, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 2/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.3300 - acc: 0.8606 - val_loss: 0.2510 - val_acc: 0.9225\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.86655 to 0.92253, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 3/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.3238 - acc: 0.8661 - val_loss: 0.2940 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.92253\n",
      "Epoch 4/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.3127 - acc: 0.8717 - val_loss: 0.2513 - val_acc: 0.9301\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.92253 to 0.93014, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 5/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2916 - acc: 0.8849 - val_loss: 0.2290 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.93014 to 0.93059, saving model to /mnt/seals/models/9999/model.h5\n",
      "Epoch 6/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2909 - acc: 0.8885 - val_loss: 0.3092 - val_acc: 0.8916\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.93059\n",
      "Epoch 7/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2782 - acc: 0.8884 - val_loss: 0.3735 - val_acc: 0.8751\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.93059\n",
      "Epoch 8/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2750 - acc: 0.8866 - val_loss: 0.3862 - val_acc: 0.8764\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.93059\n",
      "Epoch 9/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2761 - acc: 0.8862 - val_loss: 0.2357 - val_acc: 0.9118\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.93059\n",
      "Epoch 10/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2554 - acc: 0.8995 - val_loss: 0.3492 - val_acc: 0.8791\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.93059\n",
      "Epoch 11/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2691 - acc: 0.8972 - val_loss: 0.6129 - val_acc: 0.7058\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.93059\n",
      "Epoch 12/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2570 - acc: 0.9000 - val_loss: 0.2174 - val_acc: 0.9234\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.93059\n",
      "Epoch 13/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2486 - acc: 0.9064 - val_loss: 0.2029 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.93059\n",
      "Epoch 14/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2531 - acc: 0.9024 - val_loss: 0.2309 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.93059\n",
      "Epoch 15/50\n",
      "9086/9086 [==============================] - 16s 2ms/step - loss: 0.2395 - acc: 0.9068 - val_loss: 0.2627 - val_acc: 0.9261\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.93059\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "### fit model\n",
    "#############\n",
    "\n",
    "# setup training callbacks\n",
    "stopper_patience = 10\n",
    "results['fit_stopper_patience'] = stopper_patience\n",
    "callback_stopper = EarlyStopping(monitor='val_acc', patience=stopper_patience, verbose=0)\n",
    "callback_csvlogger = CSVLogger(path_model + 'training.log')\n",
    "callback_checkpointer = ModelCheckpoint(path_model +  'model.h5', monitor='val_acc', \n",
    "                             save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "          validation_data=(x_valid,y_valid),\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          callbacks=[callback_stopper, callback_checkpointer, callback_csvlogger],\n",
    "          shuffle=True,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-18T08:17:57.641949Z",
     "start_time": "2018-12-18T08:17:56.905227Z"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for folder, subs, files in os.walk(path_models):        \n",
    "    for filename in files:\n",
    "        if filename == 'params.json':\n",
    "            with open(os.path.abspath(os.path.join(folder, filename))) as f:\n",
    "                data = json.load(f)\n",
    "            results.append(data)\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "results.sort_values(\"fit_test_acc\", inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-18T08:17:57.649079Z",
     "start_time": "2018-12-18T08:17:57.644711Z"
    }
   },
   "outputs": [],
   "source": [
    "results.sort_values(\"fit_val_acc\", inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-18T08:17:57.720191Z",
     "start_time": "2018-12-18T08:17:57.651760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>284</th>\n",
       "      <th>325</th>\n",
       "      <th>75</th>\n",
       "      <th>345</th>\n",
       "      <th>78</th>\n",
       "      <th>176</th>\n",
       "      <th>22</th>\n",
       "      <th>180</th>\n",
       "      <th>375</th>\n",
       "      <th>54</th>\n",
       "      <th>369</th>\n",
       "      <th>414</th>\n",
       "      <th>243</th>\n",
       "      <th>139</th>\n",
       "      <th>356</th>\n",
       "      <th>278</th>\n",
       "      <th>14</th>\n",
       "      <th>418</th>\n",
       "      <th>302</th>\n",
       "      <th>394</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt_duration_seconds</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>155</td>\n",
       "      <td>829</td>\n",
       "      <td>734</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>794</td>\n",
       "      <td>423</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt_end</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-16 19:15:04</td>\n",
       "      <td>2018-12-16 22:20:45</td>\n",
       "      <td>2018-12-16 23:24:46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-16 21:03:18</td>\n",
       "      <td>2018-12-17 09:56:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-17 09:24:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-16 17:37:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt_start</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-16 19:12:29</td>\n",
       "      <td>2018-12-16 22:06:55</td>\n",
       "      <td>2018-12-16 23:12:32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-16 20:50:04</td>\n",
       "      <td>2018-12-17 09:49:36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-17 09:19:33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-16 17:35:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_batch_size</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_num_classes</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_num_epochs</th>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>37</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>35</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_stopped_early</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_stopper_patience</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_test_acc</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.94717</td>\n",
       "      <td>0.97333</td>\n",
       "      <td>0.94182</td>\n",
       "      <td>0.89831</td>\n",
       "      <td>0.92982</td>\n",
       "      <td>0.86441</td>\n",
       "      <td>0.82274</td>\n",
       "      <td>0.92308</td>\n",
       "      <td>0.86288</td>\n",
       "      <td>0.94983</td>\n",
       "      <td>0.94314</td>\n",
       "      <td>0.88302</td>\n",
       "      <td>0.87291</td>\n",
       "      <td>0.88294</td>\n",
       "      <td>0.91228</td>\n",
       "      <td>0.89091</td>\n",
       "      <td>0.87625</td>\n",
       "      <td>0.89298</td>\n",
       "      <td>0.92977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_test_dt_duration</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_test_dt_end</th>\n",
       "      <td>2018-12-18 02:39:43</td>\n",
       "      <td>2018-12-18 01:46:12</td>\n",
       "      <td>2018-12-18 02:01:47</td>\n",
       "      <td>2018-12-18 02:12:21</td>\n",
       "      <td>2018-12-17 15:48:50</td>\n",
       "      <td>2018-12-17 23:47:25</td>\n",
       "      <td>2018-12-17 23:39:53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-17 23:43:17</td>\n",
       "      <td>2018-12-17 16:04:42</td>\n",
       "      <td>2018-12-18 02:21:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-17 23:34:26</td>\n",
       "      <td>2018-12-18 01:12:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-17 12:13:04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_test_dt_start</th>\n",
       "      <td>2018-12-18 02:39:41</td>\n",
       "      <td>2018-12-18 01:46:10</td>\n",
       "      <td>2018-12-18 02:01:45</td>\n",
       "      <td>2018-12-18 02:12:19</td>\n",
       "      <td>2018-12-17 15:48:45</td>\n",
       "      <td>2018-12-17 23:47:24</td>\n",
       "      <td>2018-12-17 23:39:53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-17 23:43:16</td>\n",
       "      <td>2018-12-17 16:04:37</td>\n",
       "      <td>2018-12-18 02:21:23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-17 23:34:25</td>\n",
       "      <td>2018-12-18 01:12:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-17 12:13:02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_train_acc</th>\n",
       "      <td>0.9148</td>\n",
       "      <td>0.92868</td>\n",
       "      <td>0.93374</td>\n",
       "      <td>0.92613</td>\n",
       "      <td>0.94435</td>\n",
       "      <td>0.92347</td>\n",
       "      <td>0.94745</td>\n",
       "      <td>0.92075</td>\n",
       "      <td>0.92054</td>\n",
       "      <td>0.9103</td>\n",
       "      <td>0.94348</td>\n",
       "      <td>0.93099</td>\n",
       "      <td>0.91228</td>\n",
       "      <td>0.91808</td>\n",
       "      <td>0.9146</td>\n",
       "      <td>0.93548</td>\n",
       "      <td>0.9328</td>\n",
       "      <td>0.90877</td>\n",
       "      <td>0.90303</td>\n",
       "      <td>0.92965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_train_dt_duration</th>\n",
       "      <td>1090</td>\n",
       "      <td>501</td>\n",
       "      <td>913</td>\n",
       "      <td>629</td>\n",
       "      <td>208</td>\n",
       "      <td>241</td>\n",
       "      <td>323</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>98</td>\n",
       "      <td>539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>302</td>\n",
       "      <td>426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_train_dt_end</th>\n",
       "      <td>2018-12-18 02:39:41</td>\n",
       "      <td>2018-12-18 01:46:10</td>\n",
       "      <td>2018-12-18 02:01:45</td>\n",
       "      <td>2018-12-18 02:12:19</td>\n",
       "      <td>2018-12-17 15:48:45</td>\n",
       "      <td>2018-12-17 23:47:24</td>\n",
       "      <td>2018-12-17 23:39:53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-17 23:43:16</td>\n",
       "      <td>2018-12-17 16:04:37</td>\n",
       "      <td>2018-12-18 02:21:23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-17 23:34:25</td>\n",
       "      <td>2018-12-18 01:12:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-17 12:13:02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_train_dt_start</th>\n",
       "      <td>2018-12-18 02:21:31</td>\n",
       "      <td>2018-12-18 01:37:49</td>\n",
       "      <td>2018-12-18 01:46:31</td>\n",
       "      <td>2018-12-18 02:01:49</td>\n",
       "      <td>2018-12-17 15:45:17</td>\n",
       "      <td>2018-12-17 23:43:23</td>\n",
       "      <td>2018-12-17 23:34:29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-17 23:39:56</td>\n",
       "      <td>2018-12-17 16:02:59</td>\n",
       "      <td>2018-12-18 02:12:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-17 23:29:23</td>\n",
       "      <td>2018-12-18 01:05:17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-12-17 12:09:36</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_train_loss</th>\n",
       "      <td>0.21827</td>\n",
       "      <td>0.19212</td>\n",
       "      <td>0.17478</td>\n",
       "      <td>0.19617</td>\n",
       "      <td>0.15696</td>\n",
       "      <td>0.20721</td>\n",
       "      <td>0.13856</td>\n",
       "      <td>0.21412</td>\n",
       "      <td>0.21287</td>\n",
       "      <td>0.23878</td>\n",
       "      <td>0.15368</td>\n",
       "      <td>0.18937</td>\n",
       "      <td>0.22929</td>\n",
       "      <td>0.21444</td>\n",
       "      <td>0.22405</td>\n",
       "      <td>0.1624</td>\n",
       "      <td>0.17155</td>\n",
       "      <td>0.24227</td>\n",
       "      <td>0.25558</td>\n",
       "      <td>0.19381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_val_acc</th>\n",
       "      <td>0.95773</td>\n",
       "      <td>0.95567</td>\n",
       "      <td>0.95495</td>\n",
       "      <td>0.95206</td>\n",
       "      <td>0.95203</td>\n",
       "      <td>0.95117</td>\n",
       "      <td>0.94898</td>\n",
       "      <td>0.94524</td>\n",
       "      <td>0.94481</td>\n",
       "      <td>0.94307</td>\n",
       "      <td>0.9422</td>\n",
       "      <td>0.9422</td>\n",
       "      <td>0.94133</td>\n",
       "      <td>0.94133</td>\n",
       "      <td>0.94133</td>\n",
       "      <td>0.94105</td>\n",
       "      <td>0.94097</td>\n",
       "      <td>0.9409</td>\n",
       "      <td>0.9409</td>\n",
       "      <td>0.9409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_val_loss</th>\n",
       "      <td>0.14383</td>\n",
       "      <td>0.16507</td>\n",
       "      <td>0.16033</td>\n",
       "      <td>0.16596</td>\n",
       "      <td>0.1965</td>\n",
       "      <td>0.15679</td>\n",
       "      <td>0.1626</td>\n",
       "      <td>0.19217</td>\n",
       "      <td>0.21216</td>\n",
       "      <td>0.22584</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.18176</td>\n",
       "      <td>0.18862</td>\n",
       "      <td>0.18752</td>\n",
       "      <td>0.19348</td>\n",
       "      <td>0.20287</td>\n",
       "      <td>0.19909</td>\n",
       "      <td>0.21281</td>\n",
       "      <td>0.23413</td>\n",
       "      <td>0.18859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>504</td>\n",
       "      <td>500</td>\n",
       "      <td>501</td>\n",
       "      <td>502</td>\n",
       "      <td>389</td>\n",
       "      <td>483</td>\n",
       "      <td>481</td>\n",
       "      <td>245</td>\n",
       "      <td>273</td>\n",
       "      <td>281</td>\n",
       "      <td>482</td>\n",
       "      <td>400</td>\n",
       "      <td>503</td>\n",
       "      <td>263</td>\n",
       "      <td>309</td>\n",
       "      <td>480</td>\n",
       "      <td>496</td>\n",
       "      <td>301</td>\n",
       "      <td>351</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_architecture</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>MLP</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>MLP</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>MLP</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>MLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_dropout</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_layer_1_sizefactor</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_layer_2_sizefactor</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_layer_3_sizefactor</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_num_features</th>\n",
       "      <td>512</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>1536</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>1536</td>\n",
       "      <td>1536</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>1536</td>\n",
       "      <td>1536</td>\n",
       "      <td>1280</td>\n",
       "      <td>1280</td>\n",
       "      <td>1536</td>\n",
       "      <td>1536</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_sequence_length</th>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_count</th>\n",
       "      <td>336578</td>\n",
       "      <td>5376770</td>\n",
       "      <td>5376770</td>\n",
       "      <td>336578</td>\n",
       "      <td>6047042</td>\n",
       "      <td>5376770</td>\n",
       "      <td>5376770</td>\n",
       "      <td>3983618</td>\n",
       "      <td>19471490</td>\n",
       "      <td>19176002</td>\n",
       "      <td>5376770</td>\n",
       "      <td>6555394</td>\n",
       "      <td>336578</td>\n",
       "      <td>18883586</td>\n",
       "      <td>7524866</td>\n",
       "      <td>2100962</td>\n",
       "      <td>2100962</td>\n",
       "      <td>7967618</td>\n",
       "      <td>3062018</td>\n",
       "      <td>3687746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pretrained_model_name</th>\n",
       "      <td>vgg16</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>vgg16</td>\n",
       "      <td>inception_resnet_v2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>inception_resnet_v2</td>\n",
       "      <td>inception_resnet_v2</td>\n",
       "      <td>inception_resnet_v2</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>vgg16</td>\n",
       "      <td>inception_resnet_v2</td>\n",
       "      <td>inception_resnet_v2</td>\n",
       "      <td>mobilenetv2_1.00_224</td>\n",
       "      <td>mobilenetv2_1.00_224</td>\n",
       "      <td>inception_resnet_v2</td>\n",
       "      <td>inception_resnet_v2</td>\n",
       "      <td>inception_resnet_v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pretrained_model_pooling</th>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "      <td>avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shape_x_test</th>\n",
       "      <td>(225, 40, 512)</td>\n",
       "      <td>(265, 20, 2048)</td>\n",
       "      <td>(225, 40, 2048)</td>\n",
       "      <td>(275, 15, 512)</td>\n",
       "      <td>(295, 5, 1536)</td>\n",
       "      <td>(285, 10, 2048)</td>\n",
       "      <td>(295, 5, 2048)</td>\n",
       "      <td>(299, 3, 1536)</td>\n",
       "      <td>(299, 3, 1536)</td>\n",
       "      <td>(299, 3, 1536)</td>\n",
       "      <td>(299, 3, 2048)</td>\n",
       "      <td>(299, 3, 2048)</td>\n",
       "      <td>(265, 20, 512)</td>\n",
       "      <td>(299, 3, 1536)</td>\n",
       "      <td>(299, 3, 1536)</td>\n",
       "      <td>(285, 10, 1280)</td>\n",
       "      <td>(275, 15, 1280)</td>\n",
       "      <td>(299, 3, 1536)</td>\n",
       "      <td>(299, 3, 1536)</td>\n",
       "      <td>(299, 3, 1536)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shape_x_train</th>\n",
       "      <td>(8286, 40, 512)</td>\n",
       "      <td>(9086, 20, 2048)</td>\n",
       "      <td>(8286, 40, 2048)</td>\n",
       "      <td>(9286, 15, 512)</td>\n",
       "      <td>(9686, 5, 1536)</td>\n",
       "      <td>(9486, 10, 2048)</td>\n",
       "      <td>(9686, 5, 2048)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(9766, 3, 2048)</td>\n",
       "      <td>(9766, 3, 2048)</td>\n",
       "      <td>(9086, 20, 512)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(9486, 10, 1280)</td>\n",
       "      <td>(9286, 15, 1280)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(9766, 3, 1536)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shape_x_valid</th>\n",
       "      <td>(2153, 40, 512)</td>\n",
       "      <td>(2233, 20, 2048)</td>\n",
       "      <td>(2153, 40, 2048)</td>\n",
       "      <td>(2253, 15, 512)</td>\n",
       "      <td>(2293, 5, 1536)</td>\n",
       "      <td>(2273, 10, 2048)</td>\n",
       "      <td>(2293, 5, 2048)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2301, 3, 2048)</td>\n",
       "      <td>(2301, 3, 2048)</td>\n",
       "      <td>(2233, 20, 512)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2273, 10, 1280)</td>\n",
       "      <td>(2253, 15, 1280)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(2301, 3, 1536)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shape_y_train</th>\n",
       "      <td>(8286, 2)</td>\n",
       "      <td>(9086, 2)</td>\n",
       "      <td>(8286, 2)</td>\n",
       "      <td>(9286, 2)</td>\n",
       "      <td>(9686, 2)</td>\n",
       "      <td>(9486, 2)</td>\n",
       "      <td>(9686, 2)</td>\n",
       "      <td>(9766, 2)</td>\n",
       "      <td>(9766, 2)</td>\n",
       "      <td>(9766, 2)</td>\n",
       "      <td>(9766, 2)</td>\n",
       "      <td>(9766, 2)</td>\n",
       "      <td>(9086, 2)</td>\n",
       "      <td>(9766, 2)</td>\n",
       "      <td>(9766, 2)</td>\n",
       "      <td>(9486, 2)</td>\n",
       "      <td>(9286, 2)</td>\n",
       "      <td>(9766, 2)</td>\n",
       "      <td>(9766, 2)</td>\n",
       "      <td>(9766, 2)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          284                  325  \\\n",
       "dt_duration_seconds                       NaN                  NaN   \n",
       "dt_end                                    NaN                  NaN   \n",
       "dt_start                                  NaN                  NaN   \n",
       "fit_batch_size                             32                   32   \n",
       "fit_num_classes                             2                    2   \n",
       "fit_num_epochs                             28                    9   \n",
       "fit_stopped_early                        True                 True   \n",
       "fit_stopper_patience                       10                   10   \n",
       "fit_test_acc                             0.92              0.94717   \n",
       "fit_test_dt_duration                        2                    1   \n",
       "fit_test_dt_end           2018-12-18 02:39:43  2018-12-18 01:46:12   \n",
       "fit_test_dt_start         2018-12-18 02:39:41  2018-12-18 01:46:10   \n",
       "fit_train_acc                          0.9148              0.92868   \n",
       "fit_train_dt_duration                    1090                  501   \n",
       "fit_train_dt_end          2018-12-18 02:39:41  2018-12-18 01:46:10   \n",
       "fit_train_dt_start        2018-12-18 02:21:31  2018-12-18 01:37:49   \n",
       "fit_train_loss                        0.21827              0.19212   \n",
       "fit_val_acc                           0.95773              0.95567   \n",
       "fit_val_loss                          0.14383              0.16507   \n",
       "id                                        504                  500   \n",
       "model_architecture                       LSTM                 LSTM   \n",
       "model_dropout                             0.2                  0.2   \n",
       "model_layer_1_sizefactor                    4                    4   \n",
       "model_layer_2_sizefactor                    8                    8   \n",
       "model_layer_3_sizefactor                    0                    0   \n",
       "model_num_features                        512                 2048   \n",
       "model_sequence_length                      40                   20   \n",
       "param_count                            336578              5376770   \n",
       "pretrained_model_name                   vgg16             resnet50   \n",
       "pretrained_model_pooling                  avg                  avg   \n",
       "shape_x_test                   (225, 40, 512)      (265, 20, 2048)   \n",
       "shape_x_train                 (8286, 40, 512)     (9086, 20, 2048)   \n",
       "shape_x_valid                 (2153, 40, 512)     (2233, 20, 2048)   \n",
       "shape_y_train                       (8286, 2)            (9086, 2)   \n",
       "\n",
       "                                          75                   345  \\\n",
       "dt_duration_seconds                       NaN                  NaN   \n",
       "dt_end                                    NaN                  NaN   \n",
       "dt_start                                  NaN                  NaN   \n",
       "fit_batch_size                             32                   32   \n",
       "fit_num_classes                             2                    2   \n",
       "fit_num_epochs                             11                   37   \n",
       "fit_stopped_early                        True                 True   \n",
       "fit_stopper_patience                       10                   10   \n",
       "fit_test_acc                          0.97333              0.94182   \n",
       "fit_test_dt_duration                        2                    1   \n",
       "fit_test_dt_end           2018-12-18 02:01:47  2018-12-18 02:12:21   \n",
       "fit_test_dt_start         2018-12-18 02:01:45  2018-12-18 02:12:19   \n",
       "fit_train_acc                         0.93374              0.92613   \n",
       "fit_train_dt_duration                     913                  629   \n",
       "fit_train_dt_end          2018-12-18 02:01:45  2018-12-18 02:12:19   \n",
       "fit_train_dt_start        2018-12-18 01:46:31  2018-12-18 02:01:49   \n",
       "fit_train_loss                        0.17478              0.19617   \n",
       "fit_val_acc                           0.95495              0.95206   \n",
       "fit_val_loss                          0.16033              0.16596   \n",
       "id                                        501                  502   \n",
       "model_architecture                       LSTM                 LSTM   \n",
       "model_dropout                             0.2                  0.2   \n",
       "model_layer_1_sizefactor                    4                    4   \n",
       "model_layer_2_sizefactor                    8                    8   \n",
       "model_layer_3_sizefactor                    0                    0   \n",
       "model_num_features                       2048                  512   \n",
       "model_sequence_length                      40                   15   \n",
       "param_count                           5376770               336578   \n",
       "pretrained_model_name                resnet50                vgg16   \n",
       "pretrained_model_pooling                  avg                  avg   \n",
       "shape_x_test                  (225, 40, 2048)       (275, 15, 512)   \n",
       "shape_x_train                (8286, 40, 2048)      (9286, 15, 512)   \n",
       "shape_x_valid                (2153, 40, 2048)      (2253, 15, 512)   \n",
       "shape_y_train                       (8286, 2)            (9286, 2)   \n",
       "\n",
       "                                          78                   176  \\\n",
       "dt_duration_seconds                       NaN                  NaN   \n",
       "dt_end                                    NaN                  NaN   \n",
       "dt_start                                  NaN                  NaN   \n",
       "fit_batch_size                             32                   32   \n",
       "fit_num_classes                             2                    2   \n",
       "fit_num_epochs                             32                    5   \n",
       "fit_stopped_early                        True                 True   \n",
       "fit_stopper_patience                       10                   10   \n",
       "fit_test_acc                          0.89831              0.92982   \n",
       "fit_test_dt_duration                        4                    0   \n",
       "fit_test_dt_end           2018-12-17 15:48:50  2018-12-17 23:47:25   \n",
       "fit_test_dt_start         2018-12-17 15:48:45  2018-12-17 23:47:24   \n",
       "fit_train_acc                         0.94435              0.92347   \n",
       "fit_train_dt_duration                     208                  241   \n",
       "fit_train_dt_end          2018-12-17 15:48:45  2018-12-17 23:47:24   \n",
       "fit_train_dt_start        2018-12-17 15:45:17  2018-12-17 23:43:23   \n",
       "fit_train_loss                        0.15696              0.20721   \n",
       "fit_val_acc                           0.95203              0.95117   \n",
       "fit_val_loss                           0.1965              0.15679   \n",
       "id                                        389                  483   \n",
       "model_architecture                        MLP                 LSTM   \n",
       "model_dropout                             0.2                  0.2   \n",
       "model_layer_1_sizefactor                    2                    4   \n",
       "model_layer_2_sizefactor                    8                    8   \n",
       "model_layer_3_sizefactor                    0                    0   \n",
       "model_num_features                       1536                 2048   \n",
       "model_sequence_length                       5                   10   \n",
       "param_count                           6047042              5376770   \n",
       "pretrained_model_name     inception_resnet_v2             resnet50   \n",
       "pretrained_model_pooling                  avg                  avg   \n",
       "shape_x_test                   (295, 5, 1536)      (285, 10, 2048)   \n",
       "shape_x_train                 (9686, 5, 1536)     (9486, 10, 2048)   \n",
       "shape_x_valid                 (2293, 5, 1536)     (2273, 10, 2048)   \n",
       "shape_y_train                       (9686, 2)            (9486, 2)   \n",
       "\n",
       "                                          22                   180  \\\n",
       "dt_duration_seconds                       NaN                  155   \n",
       "dt_end                                    NaN  2018-12-16 19:15:04   \n",
       "dt_start                                  NaN  2018-12-16 19:12:29   \n",
       "fit_batch_size                             32                   32   \n",
       "fit_num_classes                             2                    2   \n",
       "fit_num_epochs                             22                   14   \n",
       "fit_stopped_early                        True                 True   \n",
       "fit_stopper_patience                       10                    5   \n",
       "fit_test_acc                          0.86441              0.82274   \n",
       "fit_test_dt_duration                        0                  NaN   \n",
       "fit_test_dt_end           2018-12-17 23:39:53                  NaN   \n",
       "fit_test_dt_start         2018-12-17 23:39:53                  NaN   \n",
       "fit_train_acc                         0.94745              0.92075   \n",
       "fit_train_dt_duration                     323                  NaN   \n",
       "fit_train_dt_end          2018-12-17 23:39:53                  NaN   \n",
       "fit_train_dt_start        2018-12-17 23:34:29                  NaN   \n",
       "fit_train_loss                        0.13856              0.21412   \n",
       "fit_val_acc                           0.94898              0.94524   \n",
       "fit_val_loss                           0.1626              0.19217   \n",
       "id                                        481                  245   \n",
       "model_architecture                       LSTM                  MLP   \n",
       "model_dropout                             0.2                  0.2   \n",
       "model_layer_1_sizefactor                    4                    8   \n",
       "model_layer_2_sizefactor                    8                    4   \n",
       "model_layer_3_sizefactor                    0                    4   \n",
       "model_num_features                       2048                 1536   \n",
       "model_sequence_length                       5                    3   \n",
       "param_count                           5376770              3983618   \n",
       "pretrained_model_name                resnet50  inception_resnet_v2   \n",
       "pretrained_model_pooling                  avg                  avg   \n",
       "shape_x_test                   (295, 5, 2048)       (299, 3, 1536)   \n",
       "shape_x_train                 (9686, 5, 2048)                  NaN   \n",
       "shape_x_valid                 (2293, 5, 2048)                  NaN   \n",
       "shape_y_train                       (9686, 2)            (9766, 2)   \n",
       "\n",
       "                                          375                  54   \\\n",
       "dt_duration_seconds                       829                  734   \n",
       "dt_end                    2018-12-16 22:20:45  2018-12-16 23:24:46   \n",
       "dt_start                  2018-12-16 22:06:55  2018-12-16 23:12:32   \n",
       "fit_batch_size                             32                   32   \n",
       "fit_num_classes                             2                    2   \n",
       "fit_num_epochs                             35                   28   \n",
       "fit_stopped_early                        True                 True   \n",
       "fit_stopper_patience                       10                   10   \n",
       "fit_test_acc                          0.92308              0.86288   \n",
       "fit_test_dt_duration                      NaN                  NaN   \n",
       "fit_test_dt_end                           NaN                  NaN   \n",
       "fit_test_dt_start                         NaN                  NaN   \n",
       "fit_train_acc                         0.92054               0.9103   \n",
       "fit_train_dt_duration                     NaN                  NaN   \n",
       "fit_train_dt_end                          NaN                  NaN   \n",
       "fit_train_dt_start                        NaN                  NaN   \n",
       "fit_train_loss                        0.21287              0.23878   \n",
       "fit_val_acc                           0.94481              0.94307   \n",
       "fit_val_loss                          0.21216              0.22584   \n",
       "id                                        273                  281   \n",
       "model_architecture                       LSTM                 LSTM   \n",
       "model_dropout                             0.2                  0.2   \n",
       "model_layer_1_sizefactor                    1                    1   \n",
       "model_layer_2_sizefactor                    4                    8   \n",
       "model_layer_3_sizefactor                    0                    0   \n",
       "model_num_features                       1536                 1536   \n",
       "model_sequence_length                       3                    3   \n",
       "param_count                          19471490             19176002   \n",
       "pretrained_model_name     inception_resnet_v2  inception_resnet_v2   \n",
       "pretrained_model_pooling                  avg                  avg   \n",
       "shape_x_test                   (299, 3, 1536)       (299, 3, 1536)   \n",
       "shape_x_train                             NaN                  NaN   \n",
       "shape_x_valid                             NaN                  NaN   \n",
       "shape_y_train                       (9766, 2)            (9766, 2)   \n",
       "\n",
       "                                          369                  414  \\\n",
       "dt_duration_seconds                       NaN                  NaN   \n",
       "dt_end                                    NaN                  NaN   \n",
       "dt_start                                  NaN                  NaN   \n",
       "fit_batch_size                             32                   32   \n",
       "fit_num_classes                             2                    2   \n",
       "fit_num_epochs                             15                    5   \n",
       "fit_stopped_early                        True                 True   \n",
       "fit_stopper_patience                       10                   10   \n",
       "fit_test_acc                          0.94983              0.94314   \n",
       "fit_test_dt_duration                        0                    4   \n",
       "fit_test_dt_end           2018-12-17 23:43:17  2018-12-17 16:04:42   \n",
       "fit_test_dt_start         2018-12-17 23:43:16  2018-12-17 16:04:37   \n",
       "fit_train_acc                         0.94348              0.93099   \n",
       "fit_train_dt_duration                     200                   98   \n",
       "fit_train_dt_end          2018-12-17 23:43:16  2018-12-17 16:04:37   \n",
       "fit_train_dt_start        2018-12-17 23:39:56  2018-12-17 16:02:59   \n",
       "fit_train_loss                        0.15368              0.18937   \n",
       "fit_val_acc                            0.9422               0.9422   \n",
       "fit_val_loss                            0.171              0.18176   \n",
       "id                                        482                  400   \n",
       "model_architecture                       LSTM                  MLP   \n",
       "model_dropout                             0.2                  0.2   \n",
       "model_layer_1_sizefactor                    4                    2   \n",
       "model_layer_2_sizefactor                    8                    8   \n",
       "model_layer_3_sizefactor                    0                    0   \n",
       "model_num_features                       2048                 2048   \n",
       "model_sequence_length                       3                    3   \n",
       "param_count                           5376770              6555394   \n",
       "pretrained_model_name                resnet50             resnet50   \n",
       "pretrained_model_pooling                  avg                  avg   \n",
       "shape_x_test                   (299, 3, 2048)       (299, 3, 2048)   \n",
       "shape_x_train                 (9766, 3, 2048)      (9766, 3, 2048)   \n",
       "shape_x_valid                 (2301, 3, 2048)      (2301, 3, 2048)   \n",
       "shape_y_train                       (9766, 2)            (9766, 2)   \n",
       "\n",
       "                                          243                  139  \\\n",
       "dt_duration_seconds                       NaN                  794   \n",
       "dt_end                                    NaN  2018-12-16 21:03:18   \n",
       "dt_start                                  NaN  2018-12-16 20:50:04   \n",
       "fit_batch_size                             32                   32   \n",
       "fit_num_classes                             2                    2   \n",
       "fit_num_epochs                             22                   35   \n",
       "fit_stopped_early                        True                 True   \n",
       "fit_stopper_patience                       10                   10   \n",
       "fit_test_acc                          0.88302              0.87291   \n",
       "fit_test_dt_duration                        2                  NaN   \n",
       "fit_test_dt_end           2018-12-18 02:21:26                  NaN   \n",
       "fit_test_dt_start         2018-12-18 02:21:23                  NaN   \n",
       "fit_train_acc                         0.91228              0.91808   \n",
       "fit_train_dt_duration                     539                  NaN   \n",
       "fit_train_dt_end          2018-12-18 02:21:23                  NaN   \n",
       "fit_train_dt_start        2018-12-18 02:12:24                  NaN   \n",
       "fit_train_loss                        0.22929              0.21444   \n",
       "fit_val_acc                           0.94133              0.94133   \n",
       "fit_val_loss                          0.18862              0.18752   \n",
       "id                                        503                  263   \n",
       "model_architecture                       LSTM                 LSTM   \n",
       "model_dropout                             0.2                  0.2   \n",
       "model_layer_1_sizefactor                    4                    1   \n",
       "model_layer_2_sizefactor                    8                    0   \n",
       "model_layer_3_sizefactor                    0                    8   \n",
       "model_num_features                        512                 1536   \n",
       "model_sequence_length                      20                    3   \n",
       "param_count                            336578             18883586   \n",
       "pretrained_model_name                   vgg16  inception_resnet_v2   \n",
       "pretrained_model_pooling                  avg                  avg   \n",
       "shape_x_test                   (265, 20, 512)       (299, 3, 1536)   \n",
       "shape_x_train                 (9086, 20, 512)                  NaN   \n",
       "shape_x_valid                 (2233, 20, 512)                  NaN   \n",
       "shape_y_train                       (9086, 2)            (9766, 2)   \n",
       "\n",
       "                                          356                   278  \\\n",
       "dt_duration_seconds                       423                   NaN   \n",
       "dt_end                    2018-12-17 09:56:39                   NaN   \n",
       "dt_start                  2018-12-17 09:49:36                   NaN   \n",
       "fit_batch_size                             32                    32   \n",
       "fit_num_classes                             2                     2   \n",
       "fit_num_epochs                             36                    16   \n",
       "fit_stopped_early                        True                  True   \n",
       "fit_stopper_patience                       10                    10   \n",
       "fit_test_acc                          0.88294               0.91228   \n",
       "fit_test_dt_duration                      NaN                     0   \n",
       "fit_test_dt_end                           NaN   2018-12-17 23:34:26   \n",
       "fit_test_dt_start                         NaN   2018-12-17 23:34:25   \n",
       "fit_train_acc                          0.9146               0.93548   \n",
       "fit_train_dt_duration                     NaN                   302   \n",
       "fit_train_dt_end                          NaN   2018-12-17 23:34:25   \n",
       "fit_train_dt_start                        NaN   2018-12-17 23:29:23   \n",
       "fit_train_loss                        0.22405                0.1624   \n",
       "fit_val_acc                           0.94133               0.94105   \n",
       "fit_val_loss                          0.19348               0.20287   \n",
       "id                                        309                   480   \n",
       "model_architecture                       LSTM                  LSTM   \n",
       "model_dropout                             0.2                   0.2   \n",
       "model_layer_1_sizefactor                    2                     4   \n",
       "model_layer_2_sizefactor                    4                     8   \n",
       "model_layer_3_sizefactor                    4                     0   \n",
       "model_num_features                       1536                  1280   \n",
       "model_sequence_length                       3                    10   \n",
       "param_count                           7524866               2100962   \n",
       "pretrained_model_name     inception_resnet_v2  mobilenetv2_1.00_224   \n",
       "pretrained_model_pooling                  avg                   avg   \n",
       "shape_x_test                   (299, 3, 1536)       (285, 10, 1280)   \n",
       "shape_x_train                             NaN      (9486, 10, 1280)   \n",
       "shape_x_valid                             NaN      (2273, 10, 1280)   \n",
       "shape_y_train                       (9766, 2)             (9486, 2)   \n",
       "\n",
       "                                           14                   418  \\\n",
       "dt_duration_seconds                        NaN                  278   \n",
       "dt_end                                     NaN  2018-12-17 09:24:11   \n",
       "dt_start                                   NaN  2018-12-17 09:19:33   \n",
       "fit_batch_size                              32                   32   \n",
       "fit_num_classes                              2                    2   \n",
       "fit_num_epochs                              17                   20   \n",
       "fit_stopped_early                         True                 True   \n",
       "fit_stopper_patience                        10                   10   \n",
       "fit_test_acc                           0.89091              0.87625   \n",
       "fit_test_dt_duration                         1                  NaN   \n",
       "fit_test_dt_end            2018-12-18 01:12:26                  NaN   \n",
       "fit_test_dt_start          2018-12-18 01:12:24                  NaN   \n",
       "fit_train_acc                           0.9328              0.90877   \n",
       "fit_train_dt_duration                      426                  NaN   \n",
       "fit_train_dt_end           2018-12-18 01:12:24                  NaN   \n",
       "fit_train_dt_start         2018-12-18 01:05:17                  NaN   \n",
       "fit_train_loss                         0.17155              0.24227   \n",
       "fit_val_acc                            0.94097               0.9409   \n",
       "fit_val_loss                           0.19909              0.21281   \n",
       "id                                         496                  301   \n",
       "model_architecture                        LSTM                 LSTM   \n",
       "model_dropout                              0.2                  0.2   \n",
       "model_layer_1_sizefactor                     4                    2   \n",
       "model_layer_2_sizefactor                     8                    2   \n",
       "model_layer_3_sizefactor                     0                    4   \n",
       "model_num_features                        1280                 1536   \n",
       "model_sequence_length                       15                    3   \n",
       "param_count                            2100962              7967618   \n",
       "pretrained_model_name     mobilenetv2_1.00_224  inception_resnet_v2   \n",
       "pretrained_model_pooling                   avg                  avg   \n",
       "shape_x_test                   (275, 15, 1280)       (299, 3, 1536)   \n",
       "shape_x_train                 (9286, 15, 1280)                  NaN   \n",
       "shape_x_valid                 (2253, 15, 1280)                  NaN   \n",
       "shape_y_train                        (9286, 2)            (9766, 2)   \n",
       "\n",
       "                                          302                  394  \n",
       "dt_duration_seconds                       NaN                  126  \n",
       "dt_end                                    NaN  2018-12-16 17:37:25  \n",
       "dt_start                                  NaN  2018-12-16 17:35:18  \n",
       "fit_batch_size                             32                   32  \n",
       "fit_num_classes                             2                    2  \n",
       "fit_num_epochs                             18                   16  \n",
       "fit_stopped_early                        True                 True  \n",
       "fit_stopper_patience                       10                    5  \n",
       "fit_test_acc                          0.89298              0.92977  \n",
       "fit_test_dt_duration                        2                  NaN  \n",
       "fit_test_dt_end           2018-12-17 12:13:04                  NaN  \n",
       "fit_test_dt_start         2018-12-17 12:13:02                  NaN  \n",
       "fit_train_acc                         0.90303              0.92965  \n",
       "fit_train_dt_duration                     206                  NaN  \n",
       "fit_train_dt_end          2018-12-17 12:13:02                  NaN  \n",
       "fit_train_dt_start        2018-12-17 12:09:36                  NaN  \n",
       "fit_train_loss                        0.25558              0.19381  \n",
       "fit_val_acc                            0.9409               0.9409  \n",
       "fit_val_loss                          0.23413              0.18859  \n",
       "id                                        351                  185  \n",
       "model_architecture                       LSTM                  MLP  \n",
       "model_dropout                             0.2                  0.2  \n",
       "model_layer_1_sizefactor                    4                    2  \n",
       "model_layer_2_sizefactor                    8                    8  \n",
       "model_layer_3_sizefactor                    8                    0  \n",
       "model_num_features                       1536                 1536  \n",
       "model_sequence_length                       3                    3  \n",
       "param_count                           3062018              3687746  \n",
       "pretrained_model_name     inception_resnet_v2  inception_resnet_v2  \n",
       "pretrained_model_pooling                  avg                  avg  \n",
       "shape_x_test                   (299, 3, 1536)       (299, 3, 1536)  \n",
       "shape_x_train                 (9766, 3, 1536)                  NaN  \n",
       "shape_x_valid                 (2301, 3, 1536)                  NaN  \n",
       "shape_y_train                       (9766, 2)            (9766, 2)  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head(20).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T15:58:58.842435Z",
     "start_time": "2018-12-17T15:44:23.459Z"
    }
   },
   "outputs": [],
   "source": [
    "results.to_csv(pwd+'results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results of balanced vs unbalanced fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T11:35:16.972918Z",
     "start_time": "2018-12-17T10:16:08.307Z"
    }
   },
   "outputs": [],
   "source": [
    "# results2 = pd.read_csv(pwd + 'results/results1.csv', index_col=0)\n",
    "\n",
    "# results['type'] = 'unbalanced'\n",
    "# results2['type'] = 'balanced'\n",
    "\n",
    "# results = pd.concat([results,results2],axis=0)\n",
    "\n",
    "# results.head().T"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f3d3a388-7e2a-4151-9b50-c20498fceacc",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe47bd48-3414-4657-92e7-8b8d6cb0df00",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "nav_menu": {
    "height": "148px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
