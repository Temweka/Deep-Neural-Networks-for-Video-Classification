{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T10:24:47.052452Z",
     "start_time": "2020-04-07T10:24:47.050075Z"
    }
   },
   "outputs": [],
   "source": [
    "# whether to log each feature and sequence status\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T10:25:19.075001Z",
     "start_time": "2020-04-07T10:25:19.070734Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import sys\n",
    "from shutil import rmtree\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T10:24:47.873657Z",
     "start_time": "2020-04-07T10:24:47.870522Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup paths\n",
    "pwd = os.getcwd().replace(\"notebooks\",\"\")\n",
    "path_cache = pwd + 'cache/'\n",
    "path_data = pwd + 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T10:24:48.276736Z",
     "start_time": "2020-04-07T10:24:48.271518Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup logging\n",
    "# any explicit log messages or uncaught errors to stdout and file /logs.log\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"{0}/{1}.log\".format(pwd, \"logs\")),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "# init logger\n",
    "logger = logging.getLogger()\n",
    "# make logger aware of any uncaught exceptions\n",
    "def handle_exception(exc_type, exc_value, exc_traceback):\n",
    "    if issubclass(exc_type, KeyboardInterrupt):\n",
    "        sys.__excepthook__(exc_type, exc_value, exc_traceback)\n",
    "        return\n",
    "\n",
    "    logger.error(\"Uncaught exception\", exc_info=(exc_type, exc_value, exc_traceback))\n",
    "sys.excepthook = handle_exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T10:24:50.775381Z",
     "start_time": "2020-04-07T10:24:49.070563Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from deepvideoclassification.architectures import Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T10:24:50.780879Z",
     "start_time": "2020-04-07T10:24:50.777677Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment = {\n",
    "             'architecture': 'video_lrcnn_frozen',\n",
    "             'dropout': 0.2,\n",
    "             'layer_1_size': 256,\n",
    "             'layer_2_size': 512,\n",
    "             'layer_3_size': 256,\n",
    "             'model_id': 1,\n",
    "             'pooling': 'max',\n",
    "             'pretrained_model_name': 'vgg16',\n",
    "             'sequence_length': 20,\n",
    "             'sequence_model': \"LSTM\",\n",
    "             'sequence_model_layers': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T10:26:17.086186Z",
     "start_time": "2020-04-07T10:26:17.082021Z"
    }
   },
   "outputs": [],
   "source": [
    "# delete existing results\n",
    "if os.path.exists(pwd + 'models/' + str(experiment[\"model_id\"]) + '/results.json'):\n",
    "    rmtree(pwd + 'models/' + str(experiment[\"model_id\"]) + '/')\n",
    "# create models folder if doesn't exist\n",
    "if not os.path.exists(pwd + 'models/'):\n",
    "    os.makedirs(pwd + 'models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T10:50:34.170692Z",
     "start_time": "2020-04-07T10:26:45.017987Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-07 10:26:45,021 [MainThread  ] [INFO ]  Model folder exists but no results found - potential error in previous model training\n",
      "2020-04-07 10:26:45,022 [MainThread  ] [INFO ]  Loading data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "{'sequence_model': 'LSTM', 'layer_2_size': 512, 'pooling': 'max', 'sequence_length': 20, 'layer_3_size': 256, 'dropout': 0.2, 'layer_1_size': 256, 'pretrained_model_name': 'vgg16', 'sequence_model_layers': 2, 'architecture': 'video_lrcnn_frozen', 'model_id': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-07 10:26:45,284 [MainThread  ] [INFO ]  resizing vid 1/46 to 224x224\n",
      "2020-04-07 10:26:46,895 [MainThread  ] [INFO ]  resizing vid 2/46 to 224x224\n",
      "2020-04-07 10:26:49,150 [MainThread  ] [INFO ]  resizing vid 3/46 to 224x224\n",
      "2020-04-07 10:26:50,781 [MainThread  ] [INFO ]  resizing vid 4/46 to 224x224\n",
      "2020-04-07 10:26:52,447 [MainThread  ] [INFO ]  resizing vid 5/46 to 224x224\n",
      "2020-04-07 10:26:56,208 [MainThread  ] [INFO ]  resizing vid 6/46 to 224x224\n",
      "2020-04-07 10:26:57,549 [MainThread  ] [INFO ]  resizing vid 7/46 to 224x224\n",
      "2020-04-07 10:27:00,981 [MainThread  ] [INFO ]  resizing vid 8/46 to 224x224\n",
      "2020-04-07 10:27:02,854 [MainThread  ] [INFO ]  resizing vid 9/46 to 224x224\n",
      "2020-04-07 10:27:05,834 [MainThread  ] [INFO ]  resizing vid 10/46 to 224x224\n",
      "2020-04-07 10:27:09,989 [MainThread  ] [INFO ]  resizing vid 11/46 to 224x224\n",
      "2020-04-07 10:27:13,975 [MainThread  ] [INFO ]  resizing vid 12/46 to 224x224\n",
      "2020-04-07 10:27:16,138 [MainThread  ] [INFO ]  resizing vid 13/46 to 224x224\n",
      "2020-04-07 10:27:17,800 [MainThread  ] [INFO ]  resizing vid 14/46 to 224x224\n",
      "2020-04-07 10:27:21,326 [MainThread  ] [INFO ]  resizing vid 15/46 to 224x224\n",
      "2020-04-07 10:27:22,980 [MainThread  ] [INFO ]  resizing vid 16/46 to 224x224\n",
      "2020-04-07 10:27:24,458 [MainThread  ] [INFO ]  resizing vid 17/46 to 224x224\n",
      "2020-04-07 10:27:27,194 [MainThread  ] [INFO ]  resizing vid 18/46 to 224x224\n",
      "2020-04-07 10:27:31,769 [MainThread  ] [INFO ]  resizing vid 19/46 to 224x224\n",
      "2020-04-07 10:27:39,147 [MainThread  ] [INFO ]  resizing vid 20/46 to 224x224\n",
      "2020-04-07 10:27:41,111 [MainThread  ] [INFO ]  resizing vid 21/46 to 224x224\n",
      "2020-04-07 10:27:43,276 [MainThread  ] [INFO ]  resizing vid 22/46 to 224x224\n",
      "2020-04-07 10:27:46,468 [MainThread  ] [INFO ]  resizing vid 23/46 to 224x224\n",
      "2020-04-07 10:27:48,519 [MainThread  ] [INFO ]  resizing vid 24/46 to 224x224\n",
      "2020-04-07 10:27:50,798 [MainThread  ] [INFO ]  resizing vid 25/46 to 224x224\n",
      "2020-04-07 10:27:54,270 [MainThread  ] [INFO ]  resizing vid 26/46 to 224x224\n",
      "2020-04-07 10:27:55,802 [MainThread  ] [INFO ]  resizing vid 27/46 to 224x224\n",
      "2020-04-07 10:27:57,513 [MainThread  ] [INFO ]  resizing vid 28/46 to 224x224\n",
      "2020-04-07 10:27:59,141 [MainThread  ] [INFO ]  resizing vid 29/46 to 224x224\n",
      "2020-04-07 10:28:03,348 [MainThread  ] [INFO ]  resizing vid 30/46 to 224x224\n",
      "2020-04-07 10:28:06,680 [MainThread  ] [INFO ]  resizing vid 31/46 to 224x224\n",
      "2020-04-07 10:28:13,420 [MainThread  ] [INFO ]  resizing vid 32/46 to 224x224\n",
      "2020-04-07 10:28:15,926 [MainThread  ] [INFO ]  resizing vid 33/46 to 224x224\n",
      "2020-04-07 10:28:17,681 [MainThread  ] [INFO ]  resizing vid 34/46 to 224x224\n",
      "2020-04-07 10:28:20,663 [MainThread  ] [INFO ]  resizing vid 35/46 to 224x224\n",
      "2020-04-07 10:28:23,408 [MainThread  ] [INFO ]  resizing vid 36/46 to 224x224\n",
      "2020-04-07 10:28:24,975 [MainThread  ] [INFO ]  resizing vid 37/46 to 224x224\n",
      "2020-04-07 10:28:28,380 [MainThread  ] [INFO ]  resizing vid 38/46 to 224x224\n",
      "2020-04-07 10:28:32,837 [MainThread  ] [INFO ]  resizing vid 39/46 to 224x224\n",
      "2020-04-07 10:28:40,056 [MainThread  ] [INFO ]  resizing vid 40/46 to 224x224\n",
      "2020-04-07 10:28:42,190 [MainThread  ] [INFO ]  resizing vid 41/46 to 224x224\n",
      "2020-04-07 10:28:58,140 [MainThread  ] [INFO ]  resizing vid 42/46 to 224x224\n",
      "2020-04-07 10:28:59,602 [MainThread  ] [INFO ]  resizing vid 43/46 to 224x224\n",
      "2020-04-07 10:29:01,572 [MainThread  ] [INFO ]  resizing vid 44/46 to 224x224\n",
      "2020-04-07 10:29:03,134 [MainThread  ] [INFO ]  resizing vid 45/46 to 224x224\n",
      "2020-04-07 10:29:04,588 [MainThread  ] [INFO ]  resizing vid 46/46 to 224x224\n",
      "2020-04-07 10:29:06,639 [MainThread  ] [INFO ]  Computing pretrained model features for video 1/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:29:11,279 [MainThread  ] [INFO ]  Computing pretrained model features for video 2/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:29:17,591 [MainThread  ] [INFO ]  Computing pretrained model features for video 3/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:29:22,054 [MainThread  ] [INFO ]  Computing pretrained model features for video 4/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:29:26,256 [MainThread  ] [INFO ]  Computing pretrained model features for video 5/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:29:35,705 [MainThread  ] [INFO ]  Computing pretrained model features for video 6/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:29:39,115 [MainThread  ] [INFO ]  Computing pretrained model features for video 7/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:29:47,790 [MainThread  ] [INFO ]  Computing pretrained model features for video 8/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:29:52,587 [MainThread  ] [INFO ]  Computing pretrained model features for video 9/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:30:00,234 [MainThread  ] [INFO ]  Computing pretrained model features for video 10/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:30:10,758 [MainThread  ] [INFO ]  Computing pretrained model features for video 11/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:30:20,833 [MainThread  ] [INFO ]  Computing pretrained model features for video 12/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:30:26,394 [MainThread  ] [INFO ]  Computing pretrained model features for video 13/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:30:30,616 [MainThread  ] [INFO ]  Computing pretrained model features for video 14/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:30:39,608 [MainThread  ] [INFO ]  Computing pretrained model features for video 15/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:30:43,822 [MainThread  ] [INFO ]  Computing pretrained model features for video 16/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:30:47,521 [MainThread  ] [INFO ]  Computing pretrained model features for video 17/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:30:54,360 [MainThread  ] [INFO ]  Computing pretrained model features for video 18/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:31:05,950 [MainThread  ] [INFO ]  Computing pretrained model features for video 19/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:31:24,409 [MainThread  ] [INFO ]  Computing pretrained model features for video 20/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:31:29,163 [MainThread  ] [INFO ]  Computing pretrained model features for video 21/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:31:34,691 [MainThread  ] [INFO ]  Computing pretrained model features for video 22/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:31:42,859 [MainThread  ] [INFO ]  Computing pretrained model features for video 23/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:31:48,133 [MainThread  ] [INFO ]  Computing pretrained model features for video 24/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:31:53,922 [MainThread  ] [INFO ]  Computing pretrained model features for video 25/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:32:02,600 [MainThread  ] [INFO ]  Computing pretrained model features for video 26/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:32:06,291 [MainThread  ] [INFO ]  Computing pretrained model features for video 27/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:32:10,745 [MainThread  ] [INFO ]  Computing pretrained model features for video 28/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:32:14,901 [MainThread  ] [INFO ]  Computing pretrained model features for video 29/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:32:25,563 [MainThread  ] [INFO ]  Computing pretrained model features for video 30/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:32:33,876 [MainThread  ] [INFO ]  Computing pretrained model features for video 31/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:32:50,594 [MainThread  ] [INFO ]  Computing pretrained model features for video 32/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:32:56,910 [MainThread  ] [INFO ]  Computing pretrained model features for video 33/46 using pretrained model: vgg16, pooling: max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-07 10:33:01,354 [MainThread  ] [INFO ]  Computing pretrained model features for video 34/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:33:08,858 [MainThread  ] [INFO ]  Computing pretrained model features for video 35/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:33:15,339 [MainThread  ] [INFO ]  Computing pretrained model features for video 36/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:33:19,228 [MainThread  ] [INFO ]  Computing pretrained model features for video 37/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:33:27,792 [MainThread  ] [INFO ]  Computing pretrained model features for video 38/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:33:38,942 [MainThread  ] [INFO ]  Computing pretrained model features for video 39/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:33:56,804 [MainThread  ] [INFO ]  Computing pretrained model features for video 40/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:34:01,998 [MainThread  ] [INFO ]  Computing pretrained model features for video 41/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:34:41,602 [MainThread  ] [INFO ]  Computing pretrained model features for video 42/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:34:45,228 [MainThread  ] [INFO ]  Computing pretrained model features for video 43/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:34:50,132 [MainThread  ] [INFO ]  Computing pretrained model features for video 44/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:34:54,019 [MainThread  ] [INFO ]  Computing pretrained model features for video 45/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:34:57,657 [MainThread  ] [INFO ]  Computing pretrained model features for video 46/46 using pretrained model: vgg16, pooling: max\n",
      "2020-04-07 10:35:01,327 [MainThread  ] [INFO ]  Loading features sequence data into memory [may take a few minutes]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done initializing data with #samples: train=10034, valid=1285, test=265\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.84202, saving model to /mnt/seals/models/1/model_round_1.h5\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.84202 to 0.85681, saving model to /mnt/seals/models/1/model_round_1.h5\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.85681\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.85681\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.85681 to 0.88560, saving model to /mnt/seals/models/1/model_round_1.h5\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.88560 to 0.88716, saving model to /mnt/seals/models/1/model_round_1.h5\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.88716\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.88716 to 0.88872, saving model to /mnt/seals/models/1/model_round_1.h5\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.88872 to 0.90973, saving model to /mnt/seals/models/1/model_round_1.h5\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.90973\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.90973\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.90973 to 0.91751, saving model to /mnt/seals/models/1/model_round_1.h5\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.91751\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.91751\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.91751\n",
      "Epoch 00015: early stopping\n",
      "H1 {'val_loss': [0.3964063693345289, 0.4221856226021214, 0.40744583137072476, 0.49187836215653774, 0.31102154981997227, 0.33003283085062346, 0.33480238275537233, 0.3627343856870896, 0.26089804090182606, 0.32857445742833474, 0.2741344766858023, 0.24668834508624987, 0.2720128998682193, 0.27982389471178387, 0.2647152622154251], 'val_acc': [0.8420233463498868, 0.8568093385214007, 0.8490272373540856, 0.7252918287937743, 0.8856031128404669, 0.8871595330739299, 0.8692607003891051, 0.888715953307393, 0.909727626459144, 0.8856031128404669, 0.8988326848249028, 0.9175097276264591, 0.9042801556420234, 0.8910505836575876, 0.9066147859922179], 'acc': [0.698923659545624, 0.7942993820652154, 0.8058600757068339, 0.8209089097188768, 0.8377516443852483, 0.8367550329000607, 0.8414391070479579, 0.850308949547696, 0.8572852301816197, 0.8595774366795268, 0.864759816575958, 0.8657564281086677, 0.8658560892607506, 0.8712377915326309, 0.872134741830094], 'loss': [0.5878440727507044, 0.46203166828560405, 0.43828128336767097, 0.4130541387427868, 0.3798760510973658, 0.390013800315798, 0.37161141977334894, 0.36519527293924364, 0.35097591807957734, 0.345981280012912, 0.34255637192847316, 0.33180701751644354, 0.32906865534232105, 0.3293887556894191, 0.3232703570121309]}\n",
      "stopped_epoch1 12\n",
      "15\n",
      "0.9042801556420234\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.90506, saving model to /mnt/seals/models/1/model_round_2.h5\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.90506 to 0.91128, saving model to /mnt/seals/models/1/model_round_2.h5\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.91128\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.91128 to 0.91440, saving model to /mnt/seals/models/1/model_round_2.h5\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.91440\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.91440\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.91440\n",
      "Epoch 00007: early stopping\n",
      "H2 {'val_loss': [0.26854059930441443, 0.2562722786043405, 0.286140984609897, 0.24683879263670064, 0.2591565214242453, 0.2585471670683256, 0.26312237337869426], 'val_acc': [0.9050583658051398, 0.911284046738992, 0.8957198443579767, 0.914396887205918, 0.9081712062720658, 0.9035019455716767, 0.9027237354549452], 'acc': [0.8821008570502663, 0.8832967908752614, 0.8827984851504883, 0.8892764599883559, 0.8860873031335831, 0.8880805262227636, 0.884393063583815], 'loss': [0.29967889266349607, 0.29657601623485735, 0.2934410130586167, 0.28500797663263433, 0.29312126794991467, 0.2848027353864612, 0.2879371271532371]}\n",
      "stopped_epoch2 4\n",
      "7\n",
      "0.9081712062720658\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.90272, saving model to /mnt/seals/models/1/model_round_3.h5\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.90272 to 0.90506, saving model to /mnt/seals/models/1/model_round_3.h5\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.90506 to 0.90739, saving model to /mnt/seals/models/1/model_round_3.h5\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.90739 to 0.90817, saving model to /mnt/seals/models/1/model_round_3.h5\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.90817 to 0.91128, saving model to /mnt/seals/models/1/model_round_3.h5\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.91128\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.91128\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.91128\n",
      "Epoch 00008: early stopping\n",
      "H3 {'val_loss': [0.25985459608790473, 0.25721931707998197, 0.25364922523962385, 0.249165579572262, 0.25016639448325456, 0.252977742005415, 0.24844750080127195, 0.2490739108755431], 'val_acc': [0.9027237354549452, 0.9050583658051398, 0.9073929961553343, 0.9081712062720658, 0.911284046738992, 0.9097276265055289, 0.911284046738992, 0.911284046738992], 'acc': [0.8845923858879808, 0.8891767989075562, 0.8889774765439876, 0.8907713772933608, 0.888578831935656, 0.8900737492763025, 0.8901734103571022, 0.8888778153800242], 'loss': [0.2861345240603839, 0.28570928177036137, 0.2830753653843564, 0.27679748648734165, 0.28475549804610706, 0.2812534705062822, 0.28033625702946363, 0.28823870907557875]}\n",
      "stopped_epoch3 5\n",
      "8\n",
      "0.9097276265055289\n",
      "best fit round 3 0.9097276265055289\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 20, 256)           787456    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 20, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10240)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2621696   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 4,984,578\n",
      "Trainable params: 4,984,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "265/265 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-07 10:50:31,519 [MainThread  ] [INFO ]  {\n",
      "    \"architecture\": \"video_lrcnn_frozen\",\n",
      "    \"batch_size\": 32,\n",
      "    \"convolution_kernel_size\": 3,\n",
      "    \"data_total_rows_test\": 265,\n",
      "    \"data_total_rows_train\": 10034,\n",
      "    \"data_total_rows_valid\": 1285,\n",
      "    \"dropout\": 0.2,\n",
      "    \"fit_best_round\": 3,\n",
      "    \"fit_dt_test_duration_seconds\": \"0\",\n",
      "    \"fit_dt_test_end\": \"2020-04-07 10:50:30\",\n",
      "    \"fit_dt_test_start\": \"2020-04-07 10:50:29\",\n",
      "    \"fit_dt_train_duration_seconds\": \"925\",\n",
      "    \"fit_dt_train_end\": \"2020-04-07 10:50:28\",\n",
      "    \"fit_dt_train_start\": \"2020-04-07 10:35:02\",\n",
      "    \"fit_num_epochs\": 24,\n",
      "    \"fit_stopped_epoch1\": 12,\n",
      "    \"fit_stopped_epoch2\": 4,\n",
      "    \"fit_stopped_epoch3\": 5,\n",
      "    \"fit_test_acc\": 0.7962264150943397,\n",
      "    \"fit_train_acc\": 0.8900737492763025,\n",
      "    \"fit_train_loss\": 0.2812534705062822,\n",
      "    \"fit_val_acc\": 0.9097276265055289,\n",
      "    \"fit_val_loss\": 0.252977742005415,\n",
      "    \"frame_size\": [\n",
      "        224,\n",
      "        224\n",
      "    ],\n",
      "    \"layer_1_size\": 256,\n",
      "    \"layer_2_size\": 512,\n",
      "    \"layer_3_size\": 256,\n",
      "    \"model_id\": 1,\n",
      "    \"model_param_count\": 4984578,\n",
      "    \"model_weights_path\": null,\n",
      "    \"num_features\": 512,\n",
      "    \"path_model\": \"/mnt/seals/models/1/\",\n",
      "    \"pooling\": \"max\",\n",
      "    \"pretrained_model_name\": \"vgg16\",\n",
      "    \"sequence_length\": 20,\n",
      "    \"sequence_model\": \"LSTM\",\n",
      "    \"sequence_model_layers\": 2,\n",
      "    \"verbose\": true\n",
      "}\n",
      "2020-04-07 10:50:31,521 [MainThread  ] [INFO ]  model 1 test acc: 0.7962264150943397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path_model': '/mnt/seals/models/1/', 'fit_num_epochs': 24, 'fit_dt_train_duration_seconds': '925', 'sequence_length': 20, 'model_param_count': 4984578, 'fit_dt_test_start': '2020-04-07 10:50:29', 'num_features': 512, 'fit_dt_test_duration_seconds': '0', 'data_total_rows_test': 265, 'sequence_model': 'LSTM', 'data_total_rows_train': 10034, 'fit_best_round': 3, 'fit_train_acc': 0.8900737492763025, 'fit_train_loss': 0.2812534705062822, 'pretrained_model_name': 'vgg16', 'convolution_kernel_size': 3, 'layer_2_size': 512, 'frame_size': (224, 224), 'fit_val_loss': 0.252977742005415, 'pooling': 'max', 'fit_dt_train_start': '2020-04-07 10:35:02', 'model_weights_path': None, 'fit_stopped_epoch1': 12, 'fit_dt_train_end': '2020-04-07 10:50:28', 'model_id': 1, 'fit_test_acc': 0.7962264150943397, 'verbose': True, 'dropout': 0.2, 'sequence_model_layers': 2, 'fit_stopped_epoch3': 5, 'data_total_rows_valid': 1285, 'fit_stopped_epoch2': 4, 'layer_3_size': 256, 'batch_size': 32, 'layer_1_size': 256, 'fit_val_acc': 0.9097276265055289, 'architecture': 'video_lrcnn_frozen', 'fit_dt_test_end': '2020-04-07 10:50:30'}\n",
      "gsutil -m rsync -r /mnt/seals/models/1/ gs://thesis-penguins/models/1/\n",
      "XX\n",
      "upload error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3209"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(str(experiment[\"model_id\"]) + \"   \" + \"X\"*60)\n",
    "print(experiment)\n",
    "\n",
    "architecture = Architecture(model_id = experiment['model_id'], \n",
    "                            architecture = experiment['architecture'], \n",
    "                            sequence_length = experiment['sequence_length'], \n",
    "                            pretrained_model_name = experiment['pretrained_model_name'],\n",
    "                            pooling = experiment['pooling'],\n",
    "                            sequence_model = experiment['sequence_model'],\n",
    "                            sequence_model_layers = experiment['sequence_model_layers'],\n",
    "                            layer_1_size = experiment['layer_1_size'],\n",
    "                            layer_2_size = experiment['layer_2_size'],\n",
    "                            layer_3_size = experiment['layer_3_size'],\n",
    "                            dropout = experiment['dropout'],\n",
    "                            verbose=True)\n",
    "\n",
    "architecture.train_model()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
