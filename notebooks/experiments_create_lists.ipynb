{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T17:28:08.736443Z",
     "start_time": "2019-01-21T17:28:08.732656Z"
    }
   },
   "outputs": [],
   "source": [
    "WORKER_COUNT = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T17:28:09.087540Z",
     "start_time": "2019-01-21T17:28:09.084194Z"
    }
   },
   "outputs": [],
   "source": [
    "# whether to log each feature and sequence status\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T17:28:09.309702Z",
     "start_time": "2019-01-21T17:28:09.305498Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 5000\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T17:28:09.485655Z",
     "start_time": "2019-01-21T17:28:09.481715Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup paths\n",
    "pwd = os.getcwd().replace(\"notebooks\",\"\")\n",
    "path_cache = pwd + 'cache/'\n",
    "path_data = pwd + 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T17:28:09.706339Z",
     "start_time": "2019-01-21T17:28:09.700577Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup logging\n",
    "# any explicit log messages or uncaught errors to stdout and file /logs.log\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"{0}/{1}.log\".format(pwd, \"logs\")),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "# init logger\n",
    "logger = logging.getLogger()\n",
    "# make logger aware of any uncaught exceptions\n",
    "def handle_exception(exc_type, exc_value, exc_traceback):\n",
    "    if issubclass(exc_type, KeyboardInterrupt):\n",
    "        sys.__excepthook__(exc_type, exc_value, exc_traceback)\n",
    "        return\n",
    "\n",
    "    logger.error(\"Uncaught exception\", exc_info=(exc_type, exc_value, exc_traceback))\n",
    "sys.excepthook = handle_exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T17:28:09.851709Z",
     "start_time": "2019-01-21T17:28:09.848397Z"
    }
   },
   "outputs": [],
   "source": [
    "from deepvideoclassification.pretrained_CNNs import pretrained_model_names, pretrained_model_names_bucketed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create list of experiments to be run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* batch 1 = run frozen image MLP, LRCNNs and concat models on 1 of each pretrained_model_name in buckets (bucketed on feature sizes and limited to max sequence_length of 10)\n",
    "\n",
    "* batch 2 = for best configurations from batch 1, run other pretrained models in buckets and run longer sequence lengths, maybe try different convolution kernel sizes\n",
    "\n",
    "* batch 3 = run trainable MLP and LRCNN on best performing frozen variants\n",
    "\n",
    "* batch 4 = run trainable but initializing with best CNN weights\n",
    "\n",
    "* batch 5 = run C3D models\n",
    "\n",
    "* batch 6 = analyze effect of dropout and pooling with best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:56:43.877275Z",
     "start_time": "2019-01-21T15:56:43.873616Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiment_batch_name = 'experiment_batch_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:39:03.960050Z",
     "start_time": "2019-01-21T15:39:03.957037Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# init model id - need to make sure we pick up where we leave off don't overwrite it between batches\n",
    "model_id_start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:39:04.123577Z",
     "start_time": "2019-01-21T15:39:04.120281Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# init list of experiments\n",
    "experiments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:39:04.277111Z",
     "start_time": "2019-01-21T15:39:04.272931Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pooling = 'max'\n",
    "layer_sizes = [512, 256, 128, 0]\n",
    "dropouts = [0.2]\n",
    "sequence_lengths = [3,5,10]\n",
    "sequence_models = [\"LSTM\", \"SimpleRNN\", \"GRU\", \"Convolution1D\"]\n",
    "sequence_model_layer_counts = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:39:04.793370Z",
     "start_time": "2019-01-21T15:39:04.787530Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "### image_MLP_frozen \n",
    "####################\n",
    "\n",
    "for pretrained_model_name in pretrained_model_names_bucketed:\n",
    "    for layer_1_size in layer_sizes:\n",
    "        for layer_2_size in layer_sizes:\n",
    "            for layer_3_size in layer_sizes:\n",
    "                for dropout in dropouts:\n",
    "\n",
    "                    # build experiment parameters\n",
    "                    experiment = {}\n",
    "                    \n",
    "                    experiment['architecture'] = 'image_MLP_frozen'\n",
    "                    experiment['sequence_length'] = 1\n",
    "                    experiment['pretrained_model_name'] = pretrained_model_name\n",
    "                    experiment['layer_1_size'] = layer_1_size\n",
    "                    experiment['layer_2_size'] = layer_2_size\n",
    "                    experiment['layer_3_size'] = layer_3_size\n",
    "                    experiment['dropout'] = dropout\n",
    "                    experiment['pooling'] = 'max' # outperforms avg across all parameters\n",
    "                    \n",
    "                    # add to list of experiments\n",
    "                    experiments.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:39:05.377534Z",
     "start_time": "2019-01-21T15:39:05.371586Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "### video_MLP_concat\n",
    "####################\n",
    "\n",
    "for sequence_length in sequence_lengths:\n",
    "    for pretrained_model_name in pretrained_model_names_bucketed:\n",
    "        for layer_1_size in layer_sizes:\n",
    "            for layer_2_size in layer_sizes:\n",
    "                for layer_3_size in layer_sizes:\n",
    "                    for dropout in dropouts:\n",
    "\n",
    "                        # build experiment parameters\n",
    "                        experiment = {}\n",
    "\n",
    "                        experiment['architecture'] = 'video_MLP_concat'\n",
    "                        experiment['pretrained_model_name'] = pretrained_model_name\n",
    "                        experiment['layer_1_size'] = layer_1_size\n",
    "                        experiment['layer_2_size'] = layer_2_size\n",
    "                        experiment['layer_3_size'] = layer_3_size\n",
    "                        experiment['dropout'] = dropout\n",
    "                        experiment['pooling'] = 'max' # outperforms avg across all parameters\n",
    "                        experiment['sequence_length'] = sequence_length\n",
    "\n",
    "                        # add to list of experiments\n",
    "                        experiments.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:39:05.700591Z",
     "start_time": "2019-01-21T15:39:05.684593Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "### video_LRCNN_frozen\n",
    "######################\n",
    "\n",
    "for sequence_length in sequence_lengths:\n",
    "    for pretrained_model_name in pretrained_model_names_bucketed:\n",
    "        for layer_1_size in layer_sizes:\n",
    "            for layer_2_size in layer_sizes:\n",
    "                for layer_3_size in layer_sizes:\n",
    "                    for dropout in dropouts:\n",
    "                        for sequence_model in sequence_models:\n",
    "                            for sequence_model_layers in sequence_model_layer_counts:\n",
    "\n",
    "                                # build experiment parameters\n",
    "                                experiment = {}\n",
    "\n",
    "                                experiment['architecture'] = 'video_LRCNN_frozen'\n",
    "                                experiment['pretrained_model_name'] = pretrained_model_name\n",
    "                                experiment['layer_1_size'] = layer_1_size\n",
    "                                experiment['layer_2_size'] = layer_2_size\n",
    "                                experiment['layer_3_size'] = layer_3_size\n",
    "                                experiment['dropout'] = dropout\n",
    "                                experiment['pooling'] = 'max' # outperforms avg across all parameters\n",
    "                                experiment['sequence_model'] = sequence_model\n",
    "                                experiment['sequence_model_layers'] = sequence_model_layers\n",
    "                                experiment['sequence_length'] = sequence_length\n",
    "\n",
    "                                # add to list of experiments\n",
    "                                experiments.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:39:06.499582Z",
     "start_time": "2019-01-21T15:39:06.471039Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "### convert to dataframe\n",
    "########################\n",
    "\n",
    "experiments = pd.DataFrame(experiments)\n",
    "\n",
    "### create model id column for this experiment batch\n",
    "model_id_list = list(range(0,len(experiments)))\n",
    "experiments['model_id'] = model_id_list\n",
    "\n",
    "# assign to workers\n",
    "experiments['WORKER'] = experiments['model_id'].apply(lambda x: x % WORKER_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:39:07.059741Z",
     "start_time": "2019-01-21T15:39:07.053022Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:39:09.296388Z",
     "start_time": "2019-01-21T15:39:09.279746Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "### remove invalid experiment configurations\n",
    "############################################\n",
    "\n",
    "# Just won't run experiments for those model_ids - not an error that model ids not congituous count from 0!\n",
    "\n",
    "# delete video experiments with 0 neurons in a layer with nonzero neurons in later layers\n",
    "experiments = experiments[~((experiments['layer_1_size'] == 0) & (experiments['layer_2_size'] > 0))]\n",
    "experiments = experiments[~((experiments['layer_1_size'] == 0) & (experiments['layer_3_size'] > 0))]\n",
    "experiments = experiments[~((experiments['layer_2_size'] == 0) & (experiments['layer_3_size'] > 0))]\n",
    "\n",
    "# delete video experiments where convolution_kernel_size > sequence_length (convolution_kernel_size defaults to 3 and not set in this batch)\n",
    "experiments = experiments[~((experiments['sequence_model'] == 'Convolution1D') & (experiments['sequence_length']<=3))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:40:39.755684Z",
     "start_time": "2019-01-21T15:40:39.748309Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# delete LRCNN_frozen experiments with layer_1_size == 0\n",
    "experiments = experiments[~((experiments['architecture'] == 'video_LRCNN_frozen') & (experiments['layer_1_size']==0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:40:41.913560Z",
     "start_time": "2019-01-21T15:40:41.874417Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "### output experiment batch to CSV\n",
    "##################################\n",
    "print(experiment_batch_name)\n",
    "experiments.to_csv(pwd + \"experiments/\" + experiment_batch_name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:40:42.561035Z",
     "start_time": "2019-01-21T15:40:42.541721Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(experiments.shape)\n",
    "experiments.tail().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:40:45.398609Z",
     "start_time": "2019-01-21T15:40:44.575461Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# upload to s3\n",
    "response = os.system(\"aws s3 cp \" + pwd + \"experiments/\" + experiment_batch_name + '.csv s3://thesisvids/penguins/' + experiment_batch_name + '.csv')\n",
    "if response == 0:\n",
    "    print(\"upload success\")\n",
    "else:\n",
    "    print(\"upload error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:40:46.564818Z",
     "start_time": "2019-01-21T15:40:45.472397Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls s3://thesisvids/penguins/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:57:35.710487Z",
     "start_time": "2019-01-20T16:57:35.706908Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run other pretrained models for best configurations from batch 1\n",
    "# and run longer sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:58:56.239178Z",
     "start_time": "2019-01-20T16:58:56.235173Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiment_batch_name = 'experiment_batch_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:58:56.471873Z",
     "start_time": "2019-01-20T16:58:56.452184Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# init model id - need to make sure we pick up where we leave off don't overwrite it between batches\n",
    "model_id_start = pd.read_csv(pwd + \"experiments/experiment_batch_1.csv\")['model_id'].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:58:57.387780Z",
     "start_time": "2019-01-20T16:58:57.383611Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# init list of experiments\n",
    "experiments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:58:59.675371Z",
     "start_time": "2019-01-20T16:58:59.666792Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "### convert to dataframe\n",
    "########################\n",
    "\n",
    "experiments = pd.DataFrame(experiments)\n",
    "\n",
    "### create model id column for this experiment batch\n",
    "model_id_list = list(range(0,len(experiments)))\n",
    "experiments['model_id'] = model_id_list\n",
    "\n",
    "# assign to workers\n",
    "experiments['WORKER'] = experiments['model_id'].apply(lambda x: x % WORKER_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "### output experiment batch to CSV\n",
    "##################################\n",
    "print(experiment_batch_name)\n",
    "experiments.to_csv(pwd + \"experiments/\" + experiment_batch_name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Batch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run trainable MLP and LRCNN on best performing frozen variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "### image_MLP_trainable\n",
    "#######################\n",
    "\n",
    "architecture = 'video_LRCNN_trainable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "### video_LRCNN_trainable\n",
    "#########################\n",
    "\n",
    "architecture = 'video_LRCNN_trainable'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Batch 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run trainable but initializing with best CNN weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Batch 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## load results.json for all models into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T07:59:51.070455Z",
     "start_time": "2019-01-21T07:59:49.963214Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path_models = pwd + 'models/'\n",
    "\n",
    "results = []\n",
    "\n",
    "for folder, subs, files in os.walk(path_models):\n",
    "    for filename in files:\n",
    "        if 'results.json' in filename:\n",
    "            with open(os.path.abspath(os.path.join(folder, filename))) as f:\n",
    "                data = json.load(f)\n",
    "            results.append(data)\n",
    "\n",
    "results = pd.DataFrame(results)        \n",
    "results.sort_values(\"fit_val_acc\", inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T07:59:51.101580Z",
     "start_time": "2019-01-21T07:59:51.073254Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results.head(10).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Merge done status onto experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T07:59:52.668066Z",
     "start_time": "2019-01-21T07:59:52.648347Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments = pd.merge(experiments, results[['model_id','fit_val_acc']], left_on='model_id', right_on='model_id', how='left')\n",
    "experiments['done'] = (experiments['fit_val_acc']>0).astype(int)\n",
    "del experiments['fit_val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T07:59:52.966412Z",
     "start_time": "2019-01-21T07:59:52.950563Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T07:59:53.438851Z",
     "start_time": "2019-01-21T07:59:53.430904Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"{}/{} experiments done\".format(experiments[experiments['done'] == 1].shape[0], len(experiments)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## total experiments, split by architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:09:13.293581Z",
     "start_time": "2019-01-20T21:09:13.284694Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments['architecture'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## analyze remaining experiments, split on architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:08:54.917496Z",
     "start_time": "2019-01-20T21:08:54.905927Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments[experiments['done']==0]['architecture'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:08:55.676464Z",
     "start_time": "2019-01-20T21:08:55.405782Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments[experiments['architecture'] == 'video_MLP_concat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:08:57.070721Z",
     "start_time": "2019-01-20T21:08:57.043811Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments[experiments['architecture'] == 'video_MLP_concat'].sort_values(\"sequence_length\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Copy experiment files to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:24:23.507935Z",
     "start_time": "2019-01-21T15:24:23.329850Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_ids = list(results['model_id'])\n",
    "model_ids.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T14:58:58.451122Z",
     "start_time": "2019-01-20T14:56:49.740152Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i, model_id in enumerate(model_ids):\n",
    "    \n",
    "    path_model = pwd + '/models/' + str(model_id) + '/'\n",
    "\n",
    "    # aws s3 ls on path returns 0 if it exists so check if doesn't exist, then sync\n",
    "    if os.system(\"aws s3 ls s3://thesisvids/penguins/models/\" + str(model_id) + \"/\") > 0:\n",
    "        print(\"Synching {}/{} - model_id={}\".format(i+1,len(model_ids),model_id))\n",
    "        response = os.system(\"aws s3 sync \" + path_model + \" s3://thesisvids/penguins/models/\" + str(model_id) + \"/\")\n",
    "        if response != 0:\n",
    "            print(\"ERROR syncing model_id = {}\".format(model_id))\n",
    "    else:\n",
    "        print(\"Already synched {}/{} - model_id={}\".format(i+1,len(model_ids),model_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Copy experiment files from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TODO \n",
    "# FIRST SYNCH TO S3 THEN DELETE MODELS FOLDER AND SYNC FROM S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug experiment worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T17:28:12.208257Z",
     "start_time": "2019-01-21T17:28:12.204723Z"
    }
   },
   "outputs": [],
   "source": [
    "from deepvideoclassification.architectures import Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T17:28:12.435251Z",
     "start_time": "2019-01-21T17:28:12.431562Z"
    }
   },
   "outputs": [],
   "source": [
    "WORKER_ID = 0\n",
    "GPU_ID = 7\n",
    "experiment_batch_name = 'experiment_batch_1'\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(WORKER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T17:28:14.288506Z",
     "start_time": "2019-01-21T17:28:14.282930Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-21 17:28:14,285 [MainThread  ] [INFO ]  Start worker 0 (GPU=7) processing experiment_batch_1\n"
     ]
    }
   ],
   "source": [
    "# setup logging\n",
    "# separate log file for each worker\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s, [%(levelname)-8s] [%(filename)s:%(lineno)d] %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"{0}/{1}.log\".format(pwd, \"logs_\" + str(WORKER_ID))),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "# init logger - will pass this to our architecture\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logger.info(\"Start worker {} (GPU={}) processing {}\".format(WORKER_ID, GPU_ID, experiment_batch_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T17:28:15.722158Z",
     "start_time": "2019-01-21T17:28:15.705072Z"
    }
   },
   "outputs": [],
   "source": [
    "# load list of experiments\n",
    "experiments = pd.read_csv(pwd + \"experiments/\" + experiment_batch_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T17:28:39.202194Z",
     "start_time": "2019-01-21T17:28:39.183884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>architecture</th>\n",
       "      <th>dropout</th>\n",
       "      <th>layer_1_size</th>\n",
       "      <th>layer_2_size</th>\n",
       "      <th>layer_3_size</th>\n",
       "      <th>pooling</th>\n",
       "      <th>pretrained_model_name</th>\n",
       "      <th>sequence_length</th>\n",
       "      <th>sequence_model</th>\n",
       "      <th>sequence_model_layers</th>\n",
       "      <th>model_id</th>\n",
       "      <th>WORKER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>video_LRCNN_frozen</td>\n",
       "      <td>0.2</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>max</td>\n",
       "      <td>inception_resnet_v2</td>\n",
       "      <td>5</td>\n",
       "      <td>Convolution1D</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2359</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>video_LRCNN_frozen</td>\n",
       "      <td>0.2</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>max</td>\n",
       "      <td>inception_resnet_v2</td>\n",
       "      <td>5</td>\n",
       "      <td>Convolution1D</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2391</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>video_LRCNN_frozen</td>\n",
       "      <td>0.2</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>max</td>\n",
       "      <td>inception_resnet_v2</td>\n",
       "      <td>5</td>\n",
       "      <td>Convolution1D</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2447</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            architecture  dropout  layer_1_size  layer_2_size  layer_3_size  \\\n",
       "1237  video_LRCNN_frozen      0.2           512           256           128   \n",
       "1269  video_LRCNN_frozen      0.2           512           128           128   \n",
       "1301  video_LRCNN_frozen      0.2           256           512           256   \n",
       "\n",
       "     pooling pretrained_model_name  sequence_length sequence_model  \\\n",
       "1237     max   inception_resnet_v2                5  Convolution1D   \n",
       "1269     max   inception_resnet_v2                5  Convolution1D   \n",
       "1301     max   inception_resnet_v2                5  Convolution1D   \n",
       "\n",
       "      sequence_model_layers  model_id  WORKER  \n",
       "1237                    2.0      2359       7  \n",
       "1269                    2.0      2391      15  \n",
       "1301                    2.0      2447      23  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments[experiments['model_id'].isin([2359,2391, 2447])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T17:28:55.078339Z",
     "start_time": "2019-01-21T17:28:55.072337Z"
    }
   },
   "outputs": [],
   "source": [
    "# for row in experiments.values:\n",
    "debug_model_id = 2339\n",
    "\n",
    "row = list(experiments[experiments['model_id'] == debug_model_id].values[0])\n",
    "\n",
    "# get experiment params from dataframe row\n",
    "experiment = dict(zip(experiments.columns, row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T17:29:04.950190Z",
     "start_time": "2019-01-21T17:29:04.945436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'WORKER': 11,\n",
       " 'architecture': 'video_LRCNN_frozen',\n",
       " 'dropout': 0.2,\n",
       " 'layer_1_size': 512,\n",
       " 'layer_2_size': 256,\n",
       " 'layer_3_size': 512,\n",
       " 'model_id': 2339,\n",
       " 'pooling': 'max',\n",
       " 'pretrained_model_name': 'inception_resnet_v2',\n",
       " 'sequence_length': 5,\n",
       " 'sequence_model': 'SimpleRNN',\n",
       " 'sequence_model_layers': 2.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T17:29:07.542967Z",
     "start_time": "2019-01-21T17:29:07.531433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1217</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>architecture</th>\n",
       "      <td>video_LRCNN_frozen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropout</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer_1_size</th>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer_2_size</th>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer_3_size</th>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pooling</th>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pretrained_model_name</th>\n",
       "      <td>inception_resnet_v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence_length</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence_model</th>\n",
       "      <td>SimpleRNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence_model_layers</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_id</th>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WORKER</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      1217\n",
       "architecture            video_LRCNN_frozen\n",
       "dropout                                0.2\n",
       "layer_1_size                           512\n",
       "layer_2_size                           256\n",
       "layer_3_size                           512\n",
       "pooling                                max\n",
       "pretrained_model_name  inception_resnet_v2\n",
       "sequence_length                          5\n",
       "sequence_model                   SimpleRNN\n",
       "sequence_model_layers                    2\n",
       "model_id                              2339\n",
       "WORKER                                  11"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments[experiments['model_id'] == debug_model_id].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T17:29:15.272038Z",
     "start_time": "2019-01-21T17:29:09.670118Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-21 17:29:09,673 [MainThread  ] [INFO ]  Begin experiment for model_id=2339 on GPU:0 \n",
      "2019-01-21 17:29:09,676 [MainThread  ] [INFO ]  Loading data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2339   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "{'sequence_length': 5, 'sequence_model_layers': 2.0, 'WORKER': 11, 'layer_1_size': 512, 'architecture': 'video_LRCNN_frozen', 'sequence_model': 'SimpleRNN', 'dropout': 0.2, 'model_id': 2339, 'layer_3_size': 512, 'pretrained_model_name': 'inception_resnet_v2', 'pooling': 'max', 'layer_2_size': 256}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-21 17:29:10,639 [MainThread  ] [INFO ]  Features already cached: /mnt/seals/cache/features/inception_resnet_v2/max/\n",
      "2019-01-21 17:29:10,640 [MainThread  ] [INFO ]  Loading features sequence data into memory [may take a few minutes]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done initializing data with #samples: train=60509, valid=6408, test=3135\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer flatten_2: expected min_ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-fddb9ac89cf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                             \u001b[0mlayer_3_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layer_3_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                             \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dropout'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                             verbose=True)\n\u001b[0m",
      "\u001b[0;32m/mnt/seals/deepvideoclassification/architectures.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_id, architecture, sequence_length, frame_size, pretrained_model_name, pooling, sequence_model, sequence_model_layers, layer_1_size, layer_2_size, layer_3_size, dropout, convolution_kernel_size, model_weights_path, batch_size, verbose, logger)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0;31m# final flatten if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;31m# classifier layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    325\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected min_ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0;31m# Check dtype.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer flatten_2: expected min_ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "print(str(experiment[\"model_id\"]) + \"   \" + \"X\"*60)\n",
    "logging.info(\"Begin experiment for model_id={} on GPU:{} \".format(experiment['model_id'], os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n",
    "print(experiment)\n",
    "\n",
    "architecture = Architecture(model_id = experiment['model_id'], \n",
    "                            architecture = experiment['architecture'], \n",
    "                            sequence_length = experiment['sequence_length'], \n",
    "                            pretrained_model_name = experiment['pretrained_model_name'],\n",
    "                            pooling = experiment['pooling'],\n",
    "                            sequence_model = experiment['sequence_model'],\n",
    "                            sequence_model_layers = experiment['sequence_model_layers'],\n",
    "                            layer_1_size = experiment['layer_1_size'],\n",
    "                            layer_2_size = experiment['layer_2_size'],\n",
    "                            layer_3_size = experiment['layer_3_size'],\n",
    "                            dropout = experiment['dropout'],\n",
    "                            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T17:12:34.256364Z",
     "start_time": "2019-01-21T17:12:20.789720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60509 samples, validate on 6408 samples\n",
      "Epoch 1/20\n",
      "15552/60509 [======>.......................] - ETA: 36s - loss: 4.1813 - acc: 0.7388"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4f87c84f2c3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marchitecture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/seals/deepvideoclassification/architectures.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, epochs, patience)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;31m# do first round of fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0mhistory1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopped_epoch1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'H1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/seals/deepvideoclassification/architectures.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, fit_round, learning_rate, epochs, patience)\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 verbose=self.verbose)\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0;31m# get number of epochs actually trained (might have early stopped)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "architecture.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
