{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:36:06.722465Z",
     "start_time": "2019-01-20T16:36:06.718736Z"
    }
   },
   "outputs": [],
   "source": [
    "WORKER_COUNT = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:37:17.852085Z",
     "start_time": "2019-01-20T16:37:17.848590Z"
    }
   },
   "outputs": [],
   "source": [
    "# whether to log each feature and sequence status\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:37:18.508796Z",
     "start_time": "2019-01-20T16:37:18.047192Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 5000\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:37:18.514600Z",
     "start_time": "2019-01-20T16:37:18.511377Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup paths\n",
    "pwd = os.getcwd().replace(\"notebooks\",\"\")\n",
    "path_cache = pwd + 'cache/'\n",
    "path_data = pwd + 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:38:31.094408Z",
     "start_time": "2019-01-20T16:38:31.089649Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup logging\n",
    "# any explicit log messages or uncaught errors to stdout and file /logs.log\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s, [%(levelname)-8s] [%(filename)s:%(lineno)d] %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"{0}/{1}.log\".format(pwd, \"logs\")),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "# init logger\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:37:19.905290Z",
     "start_time": "2019-01-20T16:37:18.625961Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from deepvideoclassification.pretrained_CNNs import pretrained_model_names, pretrained_model_names_bucketed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:37:19.911143Z",
     "start_time": "2019-01-20T16:37:19.907979Z"
    }
   },
   "outputs": [],
   "source": [
    "from deepvideoclassification.pretrained_CNNs import test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create experiments lists and run experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* batch 1 = run frozen image MLP, LRCNNs and concat models on 1 of each pretrained_model_name in buckets (bucketed on feature sizes and limited to max sequence_length of 10)\n",
    "\n",
    "* batch 2 = for best configurations from batch 1, run other pretrained models in buckets and run longer sequence lengths\n",
    "\n",
    "* batch 3 = run trainable MLP and LRCNN on best performing frozen variants\n",
    "\n",
    "* batch 4 = run trainable but initializing with best CNN weights\n",
    "\n",
    "* batch 5 = run C3D models\n",
    "\n",
    "* batch 6 = analyze effect of dropout and pooling with best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T14:21:34.955525Z",
     "start_time": "2019-01-20T14:21:34.951959Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment_batch_name = 'experiment_batch_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T14:21:35.131482Z",
     "start_time": "2019-01-20T14:21:35.127988Z"
    }
   },
   "outputs": [],
   "source": [
    "# init list of experiments\n",
    "experiments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T14:21:35.321991Z",
     "start_time": "2019-01-20T14:21:35.318130Z"
    }
   },
   "outputs": [],
   "source": [
    "pooling = 'max'\n",
    "layer_sizes = [512, 256, 128, 0]\n",
    "dropouts = [0.2]\n",
    "sequence_lengths = [3,5,10]\n",
    "sequence_models = [\"LSTM\", \"SimpleRNN\", \"GRU\", \"Convolution1D\"]\n",
    "sequence_model_layer_counts = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T14:21:35.581960Z",
     "start_time": "2019-01-20T14:21:35.576694Z"
    }
   },
   "outputs": [],
   "source": [
    "####################\n",
    "### image_MLP_frozen \n",
    "####################\n",
    "\n",
    "for pretrained_model_name in pretrained_model_names_bucketed:\n",
    "    for layer_1_size in layer_sizes:\n",
    "        for layer_2_size in layer_sizes:\n",
    "            for layer_3_size in layer_sizes:\n",
    "                for dropout in dropouts:\n",
    "\n",
    "                    # build experiment parameters\n",
    "                    experiment = {}\n",
    "                    \n",
    "                    experiment['architecture'] = 'image_MLP_frozen'\n",
    "                    experiment['sequence_length'] = 1\n",
    "                    experiment['pretrained_model_name'] = pretrained_model_name\n",
    "                    experiment['layer_1_size'] = layer_1_size\n",
    "                    experiment['layer_2_size'] = layer_2_size\n",
    "                    experiment['layer_3_size'] = layer_3_size\n",
    "                    experiment['dropout'] = dropout\n",
    "                    experiment['pooling'] = 'max' # outperforms avg across all parameters\n",
    "                    \n",
    "                    # add to list of experiments\n",
    "                    experiments.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T14:21:36.051409Z",
     "start_time": "2019-01-20T14:21:36.045838Z"
    }
   },
   "outputs": [],
   "source": [
    "####################\n",
    "### video_MLP_concat\n",
    "####################\n",
    "\n",
    "for sequence_length in sequence_lengths:\n",
    "    for pretrained_model_name in pretrained_model_names_bucketed:\n",
    "        for layer_1_size in layer_sizes:\n",
    "            for layer_2_size in layer_sizes:\n",
    "                for layer_3_size in layer_sizes:\n",
    "                    for dropout in dropouts:\n",
    "\n",
    "                        # build experiment parameters\n",
    "                        experiment = {}\n",
    "\n",
    "                        experiment['architecture'] = 'video_MLP_concat'\n",
    "                        experiment['pretrained_model_name'] = pretrained_model_name\n",
    "                        experiment['layer_1_size'] = layer_1_size\n",
    "                        experiment['layer_2_size'] = layer_2_size\n",
    "                        experiment['layer_3_size'] = layer_3_size\n",
    "                        experiment['dropout'] = dropout\n",
    "                        experiment['pooling'] = 'max' # outperforms avg across all parameters\n",
    "                        experiment['sequence_length'] = sequence_length\n",
    "\n",
    "                        # add to list of experiments\n",
    "                        experiments.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T14:21:36.276754Z",
     "start_time": "2019-01-20T14:21:36.265262Z"
    }
   },
   "outputs": [],
   "source": [
    "######################\n",
    "### video_LRCNN_frozen\n",
    "######################\n",
    "\n",
    "for sequence_length in sequence_lengths:\n",
    "    for pretrained_model_name in pretrained_model_names_bucketed:\n",
    "        for layer_1_size in layer_sizes:\n",
    "            for layer_2_size in layer_sizes:\n",
    "                for layer_3_size in layer_sizes:\n",
    "                    for dropout in dropouts:\n",
    "                        for sequence_model in sequence_models:\n",
    "                            for sequence_model_layers in sequence_model_layer_counts:\n",
    "\n",
    "                                # build experiment parameters\n",
    "                                experiment = {}\n",
    "\n",
    "                                experiment['architecture'] = 'video_LRCNN_frozen'\n",
    "                                experiment['pretrained_model_name'] = pretrained_model_name\n",
    "                                experiment['layer_1_size'] = layer_1_size\n",
    "                                experiment['layer_2_size'] = layer_2_size\n",
    "                                experiment['layer_3_size'] = layer_3_size\n",
    "                                experiment['dropout'] = dropout\n",
    "                                experiment['pooling'] = 'max' # outperforms avg across all parameters\n",
    "                                experiment['sequence_model'] = sequence_model\n",
    "                                experiment['sequence_model_layers'] = sequence_model_layers\n",
    "                                experiment['sequence_length'] = sequence_length\n",
    "\n",
    "                                # add to list of experiments\n",
    "                                experiments.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T14:21:37.288463Z",
     "start_time": "2019-01-20T14:21:37.267035Z"
    }
   },
   "outputs": [],
   "source": [
    "########################\n",
    "### convert to dataframe\n",
    "########################\n",
    "\n",
    "experiments = pd.DataFrame(experiments)\n",
    "experiments['model_id'] = experiments.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T14:21:37.875465Z",
     "start_time": "2019-01-20T14:21:37.867376Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments['WORKER'] = experiments['model_id'].apply(lambda x: x % WORKER_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T14:21:52.004174Z",
     "start_time": "2019-01-20T14:21:51.932909Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments.to_csv(pwd + \"experiments/\" + experiment_batch_name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T09:54:14.363139Z",
     "start_time": "2019-01-20T09:54:14.359514Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for best configurations from batch 1, run other pretrained models \n",
    "# in buckets and run longer sequence lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Batch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run trainable MLP and LRCNN on best performing frozen variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "### image_MLP_trainable\n",
    "#######################\n",
    "\n",
    "architecture = 'video_LRCNN_trainable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "### video_LRCNN_trainable\n",
    "#########################\n",
    "\n",
    "architecture = 'video_LRCNN_trainable'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Batch 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run trainable but initializing with best CNN weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Batch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#######\n",
    "### C3D\n",
    "#######\n",
    "\n",
    "architecture = 'C3D'\n",
    "\n",
    "############\n",
    "### C3Dsmall\n",
    "############\n",
    "\n",
    "architecture = 'C3Dsmall' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load results.json for all models into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T15:42:41.858105Z",
     "start_time": "2019-01-20T15:42:41.790468Z"
    }
   },
   "outputs": [],
   "source": [
    "path_models = pwd + 'models/'\n",
    "\n",
    "results = []\n",
    "\n",
    "for folder, subs, files in os.walk(path_models):\n",
    "    for filename in files:\n",
    "        if 'results.json' in filename:\n",
    "            with open(os.path.abspath(os.path.join(folder, filename))) as f:\n",
    "                data = json.load(f)\n",
    "            results.append(data)\n",
    "\n",
    "results = pd.DataFrame(results)        \n",
    "results.sort_values(\"fit_val_acc\", inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T15:42:42.619350Z",
     "start_time": "2019-01-20T15:42:42.577718Z"
    }
   },
   "outputs": [],
   "source": [
    "results.head(10).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge done status onto experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T15:00:08.753358Z",
     "start_time": "2019-01-20T15:00:08.733314Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments = pd.merge(experiments, results[['model_id','fit_val_acc']], left_on='model_id', right_on='model_id', how='left')\n",
    "experiments['done'] = (experiments['fit_val_acc']>0).astype(int)\n",
    "del experiments['fit_val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T15:00:08.923527Z",
     "start_time": "2019-01-20T15:00:08.903201Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T15:00:09.076728Z",
     "start_time": "2019-01-20T15:00:09.068471Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"{}/{} experiments done\".format(experiments[experiments['done'] == 1].shape[0], len(experiments)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyze remaining experiments, split on architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T15:00:10.838531Z",
     "start_time": "2019-01-20T15:00:10.826917Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments[experiments['done']==0]['architecture'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T15:00:13.841694Z",
     "start_time": "2019-01-20T15:00:13.625983Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments[experiments['architecture'] == 'video_MLP_concat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T15:00:14.111991Z",
     "start_time": "2019-01-20T15:00:14.090671Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments[experiments['architecture'] == 'video_MLP_concat'].sort_values(\"sequence_length\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy experiment files to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T14:56:48.170079Z",
     "start_time": "2019-01-20T14:56:48.165832Z"
    }
   },
   "outputs": [],
   "source": [
    "model_ids = list(results['model_id'])\n",
    "model_ids.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T14:58:58.451122Z",
     "start_time": "2019-01-20T14:56:49.740152Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, model_id in enumerate(model_ids):\n",
    "    \n",
    "    path_model = pwd + '/models/' + str(model_id) + '/'\n",
    "\n",
    "    # aws s3 ls on path returns 0 if it exists so check if doesn't exist, then sync\n",
    "    if os.system(\"aws s3 ls s3://thesisvids/penguins/models/\" + str(model_id) + \"/\") > 0:\n",
    "        print(\"Synching {}/{} - model_id={}\".format(i+1,len(model_ids),model_id))\n",
    "        response = os.system(\"aws s3 sync \" + path_model + \" s3://thesisvids/penguins/models/\" + str(model_id) + \"/\")\n",
    "        if response != 0:\n",
    "            print(\"ERROR syncing model_id = {}\".format(model_id))\n",
    "    else:\n",
    "        print(\"Already synched {}/{} - model_id={}\".format(i+1,len(model_ids),model_id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
