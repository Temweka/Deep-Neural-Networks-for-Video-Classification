{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T18:07:20.547192Z",
     "start_time": "2019-01-21T18:07:20.543633Z"
    }
   },
   "outputs": [],
   "source": [
    "WORKER_COUNT = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T18:07:20.773163Z",
     "start_time": "2019-01-21T18:07:20.770151Z"
    }
   },
   "outputs": [],
   "source": [
    "# whether to log each feature and sequence status\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T18:07:21.405825Z",
     "start_time": "2019-01-21T18:07:20.952353Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 5000\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T18:07:21.411678Z",
     "start_time": "2019-01-21T18:07:21.408398Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup paths\n",
    "pwd = os.getcwd().replace(\"notebooks\",\"\")\n",
    "path_cache = pwd + 'cache/'\n",
    "path_data = pwd + 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T18:07:21.419386Z",
     "start_time": "2019-01-21T18:07:21.414263Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup logging\n",
    "# any explicit log messages or uncaught errors to stdout and file /logs.log\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"{0}/{1}.log\".format(pwd, \"logs\")),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "# init logger\n",
    "logger = logging.getLogger()\n",
    "# make logger aware of any uncaught exceptions\n",
    "def handle_exception(exc_type, exc_value, exc_traceback):\n",
    "    if issubclass(exc_type, KeyboardInterrupt):\n",
    "        sys.__excepthook__(exc_type, exc_value, exc_traceback)\n",
    "        return\n",
    "\n",
    "    logger.error(\"Uncaught exception\", exc_info=(exc_type, exc_value, exc_traceback))\n",
    "sys.excepthook = handle_exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T18:07:22.802280Z",
     "start_time": "2019-01-21T18:07:21.495657Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from deepvideoclassification.pretrained_CNNs import pretrained_model_names, pretrained_model_names_bucketed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create list of experiments to be run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* batch 1 = run frozen image MLP, LRCNNs and concat models on 1 of each pretrained_model_name in buckets (bucketed on feature sizes and limited to max sequence_length of 10)\n",
    "\n",
    "* batch 2 = for best configurations from batch 1, run other pretrained models in buckets and run longer sequence lengths, maybe try different convolution kernel sizes\n",
    "\n",
    "* batch 3 = run trainable MLP and LRCNN on best performing frozen variants\n",
    "\n",
    "* batch 4 = run trainable but initializing with best CNN weights\n",
    "\n",
    "* batch 5 = run C3D models\n",
    "\n",
    "* batch 6 = analyze effect of dropout and pooling with best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:56:43.877275Z",
     "start_time": "2019-01-21T15:56:43.873616Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiment_batch_name = 'experiment_batch_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:39:03.960050Z",
     "start_time": "2019-01-21T15:39:03.957037Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# init model id - need to make sure we pick up where we leave off don't overwrite it between batches\n",
    "model_id_start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:39:04.123577Z",
     "start_time": "2019-01-21T15:39:04.120281Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# init list of experiments\n",
    "experiments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:39:04.277111Z",
     "start_time": "2019-01-21T15:39:04.272931Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pooling = 'max'\n",
    "layer_sizes = [512, 256, 128, 0]\n",
    "dropouts = [0.2]\n",
    "sequence_lengths = [3,5,10]\n",
    "sequence_models = [\"LSTM\", \"SimpleRNN\", \"GRU\", \"Convolution1D\"]\n",
    "sequence_model_layer_counts = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:39:04.793370Z",
     "start_time": "2019-01-21T15:39:04.787530Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "### image_MLP_frozen \n",
    "####################\n",
    "\n",
    "for pretrained_model_name in pretrained_model_names_bucketed:\n",
    "    for layer_1_size in layer_sizes:\n",
    "        for layer_2_size in layer_sizes:\n",
    "            for layer_3_size in layer_sizes:\n",
    "                for dropout in dropouts:\n",
    "\n",
    "                    # build experiment parameters\n",
    "                    experiment = {}\n",
    "                    \n",
    "                    experiment['architecture'] = 'image_MLP_frozen'\n",
    "                    experiment['sequence_length'] = 1\n",
    "                    experiment['pretrained_model_name'] = pretrained_model_name\n",
    "                    experiment['layer_1_size'] = layer_1_size\n",
    "                    experiment['layer_2_size'] = layer_2_size\n",
    "                    experiment['layer_3_size'] = layer_3_size\n",
    "                    experiment['dropout'] = dropout\n",
    "                    experiment['pooling'] = 'max' # outperforms avg across all parameters\n",
    "                    \n",
    "                    # add to list of experiments\n",
    "                    experiments.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:39:05.377534Z",
     "start_time": "2019-01-21T15:39:05.371586Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "### video_MLP_concat\n",
    "####################\n",
    "\n",
    "for sequence_length in sequence_lengths:\n",
    "    for pretrained_model_name in pretrained_model_names_bucketed:\n",
    "        for layer_1_size in layer_sizes:\n",
    "            for layer_2_size in layer_sizes:\n",
    "                for layer_3_size in layer_sizes:\n",
    "                    for dropout in dropouts:\n",
    "\n",
    "                        # build experiment parameters\n",
    "                        experiment = {}\n",
    "\n",
    "                        experiment['architecture'] = 'video_MLP_concat'\n",
    "                        experiment['pretrained_model_name'] = pretrained_model_name\n",
    "                        experiment['layer_1_size'] = layer_1_size\n",
    "                        experiment['layer_2_size'] = layer_2_size\n",
    "                        experiment['layer_3_size'] = layer_3_size\n",
    "                        experiment['dropout'] = dropout\n",
    "                        experiment['pooling'] = 'max' # outperforms avg across all parameters\n",
    "                        experiment['sequence_length'] = sequence_length\n",
    "\n",
    "                        # add to list of experiments\n",
    "                        experiments.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:39:05.700591Z",
     "start_time": "2019-01-21T15:39:05.684593Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "### video_LRCNN_frozen\n",
    "######################\n",
    "\n",
    "for sequence_length in sequence_lengths:\n",
    "    for pretrained_model_name in pretrained_model_names_bucketed:\n",
    "        for layer_1_size in layer_sizes:\n",
    "            for layer_2_size in layer_sizes:\n",
    "                for layer_3_size in layer_sizes:\n",
    "                    for dropout in dropouts:\n",
    "                        for sequence_model in sequence_models:\n",
    "                            for sequence_model_layers in sequence_model_layer_counts:\n",
    "\n",
    "                                # build experiment parameters\n",
    "                                experiment = {}\n",
    "\n",
    "                                experiment['architecture'] = 'video_LRCNN_frozen'\n",
    "                                experiment['pretrained_model_name'] = pretrained_model_name\n",
    "                                experiment['layer_1_size'] = layer_1_size\n",
    "                                experiment['layer_2_size'] = layer_2_size\n",
    "                                experiment['layer_3_size'] = layer_3_size\n",
    "                                experiment['dropout'] = dropout\n",
    "                                experiment['pooling'] = 'max' # outperforms avg across all parameters\n",
    "                                experiment['sequence_model'] = sequence_model\n",
    "                                experiment['sequence_model_layers'] = sequence_model_layers\n",
    "                                experiment['sequence_length'] = sequence_length\n",
    "\n",
    "                                # add to list of experiments\n",
    "                                experiments.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:39:06.499582Z",
     "start_time": "2019-01-21T15:39:06.471039Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "### convert to dataframe\n",
    "########################\n",
    "\n",
    "experiments = pd.DataFrame(experiments)\n",
    "\n",
    "### create model id column for this experiment batch\n",
    "model_id_list = list(range(0,len(experiments)))\n",
    "experiments['model_id'] = model_id_list\n",
    "\n",
    "# assign to workers\n",
    "experiments['WORKER'] = experiments['model_id'].apply(lambda x: x % WORKER_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:39:07.059741Z",
     "start_time": "2019-01-21T15:39:07.053022Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:39:09.296388Z",
     "start_time": "2019-01-21T15:39:09.279746Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "### remove invalid experiment configurations\n",
    "############################################\n",
    "\n",
    "# Just won't run experiments for those model_ids - not an error that model ids not congituous count from 0!\n",
    "\n",
    "# delete video experiments with 0 neurons in a layer with nonzero neurons in later layers\n",
    "experiments = experiments[~((experiments['layer_1_size'] == 0) & (experiments['layer_2_size'] > 0))]\n",
    "experiments = experiments[~((experiments['layer_1_size'] == 0) & (experiments['layer_3_size'] > 0))]\n",
    "experiments = experiments[~((experiments['layer_2_size'] == 0) & (experiments['layer_3_size'] > 0))]\n",
    "\n",
    "# delete video experiments where convolution_kernel_size > sequence_length (convolution_kernel_size defaults to 3 and not set in this batch)\n",
    "experiments = experiments[~((experiments['sequence_model'] == 'Convolution1D') & (experiments['sequence_length']<=3))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:40:39.755684Z",
     "start_time": "2019-01-21T15:40:39.748309Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# delete LRCNN_frozen experiments with layer_1_size == 0\n",
    "experiments = experiments[~((experiments['architecture'] == 'video_LRCNN_frozen') & (experiments['layer_1_size']==0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:40:41.913560Z",
     "start_time": "2019-01-21T15:40:41.874417Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "### output experiment batch to CSV\n",
    "##################################\n",
    "print(experiment_batch_name)\n",
    "experiments.to_csv(pwd + \"experiments/\" + experiment_batch_name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:40:42.561035Z",
     "start_time": "2019-01-21T15:40:42.541721Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(experiments.shape)\n",
    "experiments.tail().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:40:45.398609Z",
     "start_time": "2019-01-21T15:40:44.575461Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# upload to s3\n",
    "response = os.system(\"aws s3 cp \" + pwd + \"experiments/\" + experiment_batch_name + '.csv s3://thesisvids/penguins/' + experiment_batch_name + '.csv')\n",
    "if response == 0:\n",
    "    print(\"upload success\")\n",
    "else:\n",
    "    print(\"upload error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:40:46.564818Z",
     "start_time": "2019-01-21T15:40:45.472397Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls s3://thesisvids/penguins/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:57:35.710487Z",
     "start_time": "2019-01-20T16:57:35.706908Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run other pretrained models for best configurations from batch 1\n",
    "# and run longer sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:58:56.239178Z",
     "start_time": "2019-01-20T16:58:56.235173Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiment_batch_name = 'experiment_batch_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:58:56.471873Z",
     "start_time": "2019-01-20T16:58:56.452184Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# init model id - need to make sure we pick up where we leave off don't overwrite it between batches\n",
    "model_id_start = pd.read_csv(pwd + \"experiments/experiment_batch_1.csv\")['model_id'].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:58:57.387780Z",
     "start_time": "2019-01-20T16:58:57.383611Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# init list of experiments\n",
    "experiments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T16:58:59.675371Z",
     "start_time": "2019-01-20T16:58:59.666792Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "### convert to dataframe\n",
    "########################\n",
    "\n",
    "experiments = pd.DataFrame(experiments)\n",
    "\n",
    "### create model id column for this experiment batch\n",
    "model_id_list = list(range(0,len(experiments)))\n",
    "experiments['model_id'] = model_id_list\n",
    "\n",
    "# assign to workers\n",
    "experiments['WORKER'] = experiments['model_id'].apply(lambda x: x % WORKER_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "### output experiment batch to CSV\n",
    "##################################\n",
    "print(experiment_batch_name)\n",
    "experiments.to_csv(pwd + \"experiments/\" + experiment_batch_name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Batch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run trainable MLP and LRCNN on best performing frozen variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "### image_MLP_trainable\n",
    "#######################\n",
    "\n",
    "architecture = 'video_LRCNN_trainable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "### video_LRCNN_trainable\n",
    "#########################\n",
    "\n",
    "architecture = 'video_LRCNN_trainable'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Batch 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run trainable but initializing with best CNN weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Batch 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## load results.json for all models into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T07:59:51.070455Z",
     "start_time": "2019-01-21T07:59:49.963214Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path_models = pwd + 'models/'\n",
    "\n",
    "results = []\n",
    "\n",
    "for folder, subs, files in os.walk(path_models):\n",
    "    for filename in files:\n",
    "        if 'results.json' in filename:\n",
    "            with open(os.path.abspath(os.path.join(folder, filename))) as f:\n",
    "                data = json.load(f)\n",
    "            results.append(data)\n",
    "\n",
    "results = pd.DataFrame(results)        \n",
    "results.sort_values(\"fit_val_acc\", inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T07:59:51.101580Z",
     "start_time": "2019-01-21T07:59:51.073254Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results.head(10).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Merge done status onto experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T07:59:52.668066Z",
     "start_time": "2019-01-21T07:59:52.648347Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments = pd.merge(experiments, results[['model_id','fit_val_acc']], left_on='model_id', right_on='model_id', how='left')\n",
    "experiments['done'] = (experiments['fit_val_acc']>0).astype(int)\n",
    "del experiments['fit_val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T07:59:52.966412Z",
     "start_time": "2019-01-21T07:59:52.950563Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T07:59:53.438851Z",
     "start_time": "2019-01-21T07:59:53.430904Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"{}/{} experiments done\".format(experiments[experiments['done'] == 1].shape[0], len(experiments)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## total experiments, split by architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:09:13.293581Z",
     "start_time": "2019-01-20T21:09:13.284694Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments['architecture'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## analyze remaining experiments, split on architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:08:54.917496Z",
     "start_time": "2019-01-20T21:08:54.905927Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments[experiments['done']==0]['architecture'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:08:55.676464Z",
     "start_time": "2019-01-20T21:08:55.405782Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments[experiments['architecture'] == 'video_MLP_concat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:08:57.070721Z",
     "start_time": "2019-01-20T21:08:57.043811Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experiments[experiments['architecture'] == 'video_MLP_concat'].sort_values(\"sequence_length\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Copy experiment files to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T15:24:23.507935Z",
     "start_time": "2019-01-21T15:24:23.329850Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_ids = list(results['model_id'])\n",
    "model_ids.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T14:58:58.451122Z",
     "start_time": "2019-01-20T14:56:49.740152Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i, model_id in enumerate(model_ids):\n",
    "    \n",
    "    path_model = pwd + '/models/' + str(model_id) + '/'\n",
    "\n",
    "    # aws s3 ls on path returns 0 if it exists so check if doesn't exist, then sync\n",
    "    if os.system(\"aws s3 ls s3://thesisvids/penguins/models/\" + str(model_id) + \"/\") > 0:\n",
    "        print(\"Synching {}/{} - model_id={}\".format(i+1,len(model_ids),model_id))\n",
    "        response = os.system(\"aws s3 sync \" + path_model + \" s3://thesisvids/penguins/models/\" + str(model_id) + \"/\")\n",
    "        if response != 0:\n",
    "            print(\"ERROR syncing model_id = {}\".format(model_id))\n",
    "    else:\n",
    "        print(\"Already synched {}/{} - model_id={}\".format(i+1,len(model_ids),model_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Copy experiment files from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TODO \n",
    "# FIRST SYNCH TO S3 THEN DELETE MODELS FOLDER AND SYNC FROM S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug experiment worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T18:07:24.391080Z",
     "start_time": "2019-01-21T18:07:23.769368Z"
    }
   },
   "outputs": [],
   "source": [
    "from deepvideoclassification.architectures import Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T18:07:24.397401Z",
     "start_time": "2019-01-21T18:07:24.393964Z"
    }
   },
   "outputs": [],
   "source": [
    "WORKER_ID = 0\n",
    "GPU_ID = 7\n",
    "experiment_batch_name = 'experiment_batch_1'\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(WORKER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T18:07:24.405426Z",
     "start_time": "2019-01-21T18:07:24.399679Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-21 18:07:24,402 [MainThread  ] [INFO ]  Start worker 0 (GPU=7) processing experiment_batch_1\n"
     ]
    }
   ],
   "source": [
    "# setup logging\n",
    "# separate log file for each worker\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s, [%(levelname)-8s] [%(filename)s:%(lineno)d] %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"{0}/{1}.log\".format(pwd, \"logs_\" + str(WORKER_ID))),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "# init logger - will pass this to our architecture\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logger.info(\"Start worker {} (GPU={}) processing {}\".format(WORKER_ID, GPU_ID, experiment_batch_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T18:07:24.423317Z",
     "start_time": "2019-01-21T18:07:24.407741Z"
    }
   },
   "outputs": [],
   "source": [
    "# load list of experiments\n",
    "experiments = pd.read_csv(pwd + \"experiments/\" + experiment_batch_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T18:07:24.503057Z",
     "start_time": "2019-01-21T18:07:24.481268Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>architecture</th>\n",
       "      <th>dropout</th>\n",
       "      <th>layer_1_size</th>\n",
       "      <th>layer_2_size</th>\n",
       "      <th>layer_3_size</th>\n",
       "      <th>pooling</th>\n",
       "      <th>pretrained_model_name</th>\n",
       "      <th>sequence_length</th>\n",
       "      <th>sequence_model</th>\n",
       "      <th>sequence_model_layers</th>\n",
       "      <th>model_id</th>\n",
       "      <th>WORKER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>video_LRCNN_frozen</td>\n",
       "      <td>0.2</td>\n",
       "      <td>256</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>max</td>\n",
       "      <td>inception_resnet_v2</td>\n",
       "      <td>5</td>\n",
       "      <td>Convolution1D</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2463</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>video_LRCNN_frozen</td>\n",
       "      <td>0.2</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>max</td>\n",
       "      <td>inception_resnet_v2</td>\n",
       "      <td>5</td>\n",
       "      <td>Convolution1D</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2495</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            architecture  dropout  layer_1_size  layer_2_size  layer_3_size  \\\n",
       "1317  video_LRCNN_frozen      0.2           256           512             0   \n",
       "1349  video_LRCNN_frozen      0.2           256           256             0   \n",
       "\n",
       "     pooling pretrained_model_name  sequence_length sequence_model  \\\n",
       "1317     max   inception_resnet_v2                5  Convolution1D   \n",
       "1349     max   inception_resnet_v2                5  Convolution1D   \n",
       "\n",
       "      sequence_model_layers  model_id  WORKER  \n",
       "1317                    2.0      2463      15  \n",
       "1349                    2.0      2495      23  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments[experiments['model_id'].isin([2495, 2463])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T18:07:25.226778Z",
     "start_time": "2019-01-21T18:07:25.220726Z"
    }
   },
   "outputs": [],
   "source": [
    "# for row in experiments.values:\n",
    "debug_model_id = 2519\n",
    "\n",
    "row = list(experiments[experiments['model_id'] == debug_model_id].values[0])\n",
    "\n",
    "# get experiment params from dataframe row\n",
    "experiment = dict(zip(experiments.columns, row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T18:07:25.624941Z",
     "start_time": "2019-01-21T18:07:25.619841Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'WORKER': 23,\n",
       " 'architecture': 'video_LRCNN_frozen',\n",
       " 'dropout': 0.2,\n",
       " 'layer_1_size': 256,\n",
       " 'layer_2_size': 128,\n",
       " 'layer_3_size': 128,\n",
       " 'model_id': 2519,\n",
       " 'pooling': 'max',\n",
       " 'pretrained_model_name': 'inception_resnet_v2',\n",
       " 'sequence_length': 5,\n",
       " 'sequence_model': 'Convolution1D',\n",
       " 'sequence_model_layers': 2.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T18:07:26.027016Z",
     "start_time": "2019-01-21T18:07:26.015406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1373</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>architecture</th>\n",
       "      <td>video_LRCNN_frozen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropout</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer_1_size</th>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer_2_size</th>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer_3_size</th>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pooling</th>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pretrained_model_name</th>\n",
       "      <td>inception_resnet_v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence_length</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence_model</th>\n",
       "      <td>Convolution1D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence_model_layers</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_id</th>\n",
       "      <td>2519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WORKER</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      1373\n",
       "architecture            video_LRCNN_frozen\n",
       "dropout                                0.2\n",
       "layer_1_size                           256\n",
       "layer_2_size                           128\n",
       "layer_3_size                           128\n",
       "pooling                                max\n",
       "pretrained_model_name  inception_resnet_v2\n",
       "sequence_length                          5\n",
       "sequence_model               Convolution1D\n",
       "sequence_model_layers                    2\n",
       "model_id                              2519\n",
       "WORKER                                  23"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments[experiments['model_id'] == debug_model_id].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T18:07:31.474787Z",
     "start_time": "2019-01-21T18:07:26.397730Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-21 18:07:26,400 [MainThread  ] [INFO ]  Begin experiment for model_id=2519 on GPU:0 \n",
      "2019-01-21 18:07:26,403 [MainThread  ] [INFO ]  Model folder exists but no results found - potential error in previous model training\n",
      "2019-01-21 18:07:26,404 [MainThread  ] [INFO ]  Loading data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2519   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "{'pretrained_model_name': 'inception_resnet_v2', 'layer_2_size': 128, 'layer_3_size': 128, 'sequence_model': 'Convolution1D', 'sequence_model_layers': 2.0, 'layer_1_size': 256, 'WORKER': 23, 'architecture': 'video_LRCNN_frozen', 'dropout': 0.2, 'model_id': 2519, 'pooling': 'max', 'sequence_length': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-21 18:07:27,351 [MainThread  ] [INFO ]  Features already cached: /mnt/seals/cache/features/inception_resnet_v2/max/\n",
      "2019-01-21 18:07:27,352 [MainThread  ] [INFO ]  Loading features sequence data into memory [may take a few minutes]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done initializing data with #samples: train=60509, valid=6408, test=3135\n"
     ]
    }
   ],
   "source": [
    "print(str(experiment[\"model_id\"]) + \"   \" + \"X\"*60)\n",
    "logging.info(\"Begin experiment for model_id={} on GPU:{} \".format(experiment['model_id'], os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n",
    "print(experiment)\n",
    "\n",
    "architecture = Architecture(model_id = experiment['model_id'], \n",
    "                            architecture = experiment['architecture'], \n",
    "                            sequence_length = experiment['sequence_length'], \n",
    "                            pretrained_model_name = experiment['pretrained_model_name'],\n",
    "                            pooling = experiment['pooling'],\n",
    "                            sequence_model = experiment['sequence_model'],\n",
    "                            sequence_model_layers = experiment['sequence_model_layers'],\n",
    "                            layer_1_size = experiment['layer_1_size'],\n",
    "                            layer_2_size = experiment['layer_2_size'],\n",
    "                            layer_3_size = experiment['layer_3_size'],\n",
    "                            dropout = experiment['dropout'],\n",
    "                            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T18:07:31.482859Z",
     "start_time": "2019-01-21T18:07:31.477787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(architecture.model.layers[-1].output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-21T18:07:27.555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60509 samples, validate on 6408 samples\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "architecture.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-21T18:07:27.903Z"
    }
   },
   "outputs": [],
   "source": [
    "architecture.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
