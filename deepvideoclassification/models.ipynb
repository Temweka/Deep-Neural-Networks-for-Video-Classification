{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:43:15.829962Z",
     "start_time": "2019-01-18T22:43:15.353Z"
    }
   },
   "outputs": [],
   "source": [
    "### TODO\n",
    "# * 3-d CNN\n",
    "# * concat dense model\n",
    "# * fit_models create_architectures_list (append mode)\n",
    "# * fit_models worker if experiment id last digit in os environment var\n",
    "\n",
    "# * collect garbage between fitting models\n",
    "\n",
    "# * refactor custom_model_name and model_weights_path to instead use trained model id\n",
    "\n",
    "# delete cached sequences after each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:43:44.339445Z",
     "start_time": "2019-01-18T22:43:44.335963Z"
    }
   },
   "outputs": [],
   "source": [
    "# whether to log each feature and sequence status\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:43:44.538782Z",
     "start_time": "2019-01-18T22:43:44.535024Z"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# get number of processors for multiprocessing fit generators\n",
    "num_workers = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:43:45.111739Z",
     "start_time": "2019-01-18T22:43:44.759194Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:43:46.434019Z",
     "start_time": "2019-01-18T22:43:45.114638Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, TensorBoard\n",
    "from keras.layers import Dense, Flatten, Dropout, ZeroPadding3D, Input\n",
    "from keras.layers.recurrent import SimpleRNN, GRU, LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling3D, Conv3D, MaxPooling2D, Convolution1D, Convolution3D, MaxPooling3D, ZeroPadding3D\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:43:46.440336Z",
     "start_time": "2019-01-18T22:43:46.436915Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup paths\n",
    "pwd = os.getcwd().replace(\"deepvideoclassification\",\"\")\n",
    "path_cache = pwd + 'cache/'\n",
    "path_data = pwd + 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:43:46.449825Z",
     "start_time": "2019-01-18T22:43:46.443293Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup logging\n",
    "# any explicit log messages or uncaught errors to stdout and file /logs.log\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"{0}/{1}.log\".format(pwd, \"logs\")),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "# init logger\n",
    "logger = logging.getLogger()\n",
    "# make logger aware of any uncaught exceptions\n",
    "def handle_exception(exc_type, exc_value, exc_traceback):\n",
    "    if issubclass(exc_type, KeyboardInterrupt):\n",
    "        sys.__excepthook__(exc_type, exc_value, exc_traceback)\n",
    "        return\n",
    "\n",
    "    logger.error(\"Uncaught exception\", exc_info=(exc_type, exc_value, exc_traceback))\n",
    "sys.excepthook = handle_exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:43:47.076628Z",
     "start_time": "2019-01-18T22:43:46.452613Z"
    }
   },
   "outputs": [],
   "source": [
    "from deepvideoclassification.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained_CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:43:50.363906Z",
     "start_time": "2019-01-18T22:43:50.359413Z"
    }
   },
   "outputs": [],
   "source": [
    "# pretrained model shapes\n",
    "pretrained_model_len_features = {}\n",
    "#\n",
    "pretrained_model_len_features['vgg16'] = 512\n",
    "#\n",
    "pretrained_model_len_features['mobilenetv2_1.00_224'] = 1280\n",
    "#\n",
    "pretrained_model_len_features['inception_resnet_v2'] = 1536\n",
    "#\n",
    "pretrained_model_len_features['resnet50'] = 2048\n",
    "pretrained_model_len_features['xception'] = 2048\n",
    "pretrained_model_len_features['inception_v3'] = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:43:50.550605Z",
     "start_time": "2019-01-18T22:43:50.545956Z"
    }
   },
   "outputs": [],
   "source": [
    "# pretrained model shapes\n",
    "pretrained_model_sizes = {}\n",
    "#\n",
    "pretrained_model_sizes['vgg16'] = (224,224)\n",
    "pretrained_model_sizes['resnet50'] = (224,224)\n",
    "pretrained_model_sizes['mobilenetv2_1.00_224'] = (224,224)\n",
    "#\n",
    "pretrained_model_sizes['xception'] = (299,299)\n",
    "pretrained_model_sizes['inception_v3'] = (299,299)\n",
    "pretrained_model_sizes['inception_resnet_v2'] = (299,299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:43:50.759044Z",
     "start_time": "2019-01-18T22:43:50.755562Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model_names = [\"inception_resnet_v2\", \"inception_v3\", \"mobilenetv2_1.00_224\", \"resnet50\", \"vgg16\", \"xception\"]\n",
    "poolings = ['max','avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:43:50.986754Z",
     "start_time": "2019-01-18T22:43:50.978443Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_pretrained_model(pretrained_model_name, pooling, model_weights_path = None):\n",
    "    \"\"\" Load pretrained model with given pooling applied\n",
    "    \n",
    "    Args:\n",
    "        pretrained_model: name of pretrained model [\"Xception\", \"VGG16\", \"ResNet50\", \"InceptionV3\", \"InceptionResNetV2\", \"MobileNetV2\"]\n",
    "        pooling: pooling strategy for final pretrained model layer [\"max\",\"avg\"]\n",
    "        :model_weights_path: path to custom model weights if we want to load CNN model we've fine-tuned to produce features (e.g. for LRCNN)\n",
    "    \n",
    "    Returns:\n",
    "        Pretrained model object (excluding dense softmax 1000 ImageNet classes layer)\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize output\n",
    "    model = None\n",
    "    \n",
    "    pretrained_model_name = pretrained_model_name.lower()\n",
    "    \n",
    "    ###########################\n",
    "    ### import pretrained model\n",
    "    ###########################\n",
    "    if pretrained_model_name == \"xception\":   \n",
    "        from keras.applications.xception import Xception\n",
    "        model = Xception(include_top=False, weights='imagenet', pooling=pooling)\n",
    "    elif pretrained_model_name == \"vgg16\":   \n",
    "        from keras.applications.vgg16 import VGG16\n",
    "        model = VGG16(include_top=False, weights='imagenet', pooling=pooling)\n",
    "    elif pretrained_model_name == \"resnet50\":   \n",
    "        from keras.applications.resnet50 import ResNet50\n",
    "        model = ResNet50(include_top=False, weights='imagenet', pooling=pooling)\n",
    "    elif pretrained_model_name == \"inception_v3\":   \n",
    "        from keras.applications.inception_v3 import InceptionV3\n",
    "        model = InceptionV3(include_top=False, weights='imagenet', pooling=pooling)\n",
    "    elif pretrained_model_name == \"inception_resnet_v2\":   \n",
    "        from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "        model = InceptionResNetV2(include_top=False, weights='imagenet', pooling=pooling)\n",
    "    elif pretrained_model_name == \"mobilenetv2_1.00_224\":   \n",
    "        from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "        model = MobileNetV2(include_top=False, weights='imagenet', pooling=pooling)\n",
    "    else:\n",
    "        raise NameError('Invalid pretrained model name - must be one of [\"Xception\", \"VGG16\", \"ResNet50\", \"InceptionV3\", \"InceptionResNetV2\", \"MobileNetV2\"]')\n",
    "    \n",
    "    if model_weights_path is not None:\n",
    "        if os.path.exists(model_weights_path):\n",
    "            model.load_weights(model_weights_path)\n",
    "        else:\n",
    "            raise NameError('pretrained model weights not found')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:43:51.164753Z",
     "start_time": "2019-01-18T22:43:51.159523Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_pretrained_model_preprocessor(pretrained_model_name):\n",
    "    \"\"\"\n",
    "    Return preprocessing function for a given pretrained model\n",
    "    \"\"\"\n",
    "\n",
    "    preprocess_input = None\n",
    "\n",
    "    pretrained_model_name = pretrained_model_name.lower()\n",
    "        \n",
    "    if pretrained_model_name == \"xception\":   \n",
    "        from keras.applications.xception import preprocess_input\n",
    "    elif pretrained_model_name == \"vgg16\":   \n",
    "        from keras.applications.vgg16 import preprocess_input\n",
    "    elif pretrained_model_name == \"resnet50\":   \n",
    "        from keras.applications.resnet50 import preprocess_input\n",
    "    elif pretrained_model_name == \"inception_v3\":   \n",
    "        from keras.applications.inception_v3 import preprocess_input\n",
    "    elif pretrained_model_name == \"inception_resnet_v2\":   \n",
    "        from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "    elif pretrained_model_name == \"mobilenetv2_1.00_224\":   \n",
    "        from keras.applications.mobilenet_v2 import preprocess_input\n",
    "    else:\n",
    "        raise NameError('Invalid pretrained model name - must be one of [\"Xception\", \"VGG16\", \"ResNet50\", \"InceptionV3\", \"InceptionResNetV2\", \"MobileNetV2\"]')\n",
    "        \n",
    "    return preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:43:51.394664Z",
     "start_time": "2019-01-18T22:43:51.383395Z"
    }
   },
   "outputs": [],
   "source": [
    "def precompute_CNN_features(pretrained_model_name, pooling, model_weights_path = None, custom_model_name = None):\n",
    "    \"\"\" \n",
    "    Save pretrained features array computed over all frames of each video \n",
    "    using given pretrained model and pooling method\n",
    "    \n",
    "    :pretrained_model_name: pretrained model object loaded using `load_pretrained_model`\n",
    "    :pooling: pooling method used with pretrained model\n",
    "    :model_weights_path: path to custom model weights if we want to load CNN model we've fine-tuned to produce features (e.g. for LRCNN)\n",
    "    :custom_model_name: custom output name to append to pretrained model name\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    pretrained_model_name = pretrained_model_name.lower()\n",
    "    \n",
    "    # setup path to save features\n",
    "    path_features = path_cache + 'features/' + pretrained_model_name + \"/\" + pooling + '/'\n",
    "    \n",
    "    # store in custom directory if custom model name given (for when loading weights from fine-tuned CNN and precomputing features from that model)\n",
    "    if custom_model_name is not None and model_weights_path is not None:\n",
    "        path_features = path_cache + 'features/' + pretrained_model_name + \"__\" + custom_model_name + \"/\" + pooling + '/'\n",
    "    \n",
    "    if not os.path.exists(path_features):\n",
    "        \n",
    "        os.makedirs(path_features)\n",
    "        \n",
    "        # load pretrained model\n",
    "        pretrained_model = load_pretrained_model(pretrained_model_name, pooling, model_weights_path)\n",
    "\n",
    "        # load preprocessing function\n",
    "        preprocess_input = load_pretrained_model_preprocessor(pretrained_model_name)\n",
    "\n",
    "        # lookup pretrained model input shape\n",
    "        model_input_size = pretrained_model_sizes[pretrained_model_name]\n",
    "        \n",
    "        # precompute features for each video using pretrained model\n",
    "        from deepvideoclassification.data import get_video_paths\n",
    "        path_videos = get_video_paths()\n",
    "\n",
    "        for c, path_video in enumerate(path_videos):\n",
    "\n",
    "            if verbose:\n",
    "                logging.info(\"Computing pretrained model features for video {}/{} using pretrained model: {}, pooling: {}\".format(c+1,len(path_videos),pretrained_model_name, pooling))\n",
    "\n",
    "            # get video name from video path\n",
    "            video_name = path_video.split(\"/\")[-2]\n",
    "\n",
    "            # build output path\n",
    "            path_output = path_features + video_name\n",
    "\n",
    "            try:\n",
    "                if not os.path.exists(path_output + '.npy'):\n",
    "\n",
    "                    path_frames = path_data + video_name + \"/\"\n",
    "\n",
    "                    # initialize features list\n",
    "                    features = []\n",
    "\n",
    "                    frame_paths = os.listdir(path_frames)\n",
    "                    frame_paths = [path_frames + f for f in frame_paths if f != '.DS_Store']\n",
    "\n",
    "                    # sort paths in sequence (they were created with incrementing filenames through time)\n",
    "                    frame_paths.sort()\n",
    "\n",
    "                    # load each frame in vid and get features\n",
    "                    for j, frame_path in enumerate(frame_paths):\n",
    "\n",
    "                        # load image & preprocess\n",
    "                        image = cv2.imread(frame_path, cv2.COLOR_BGR2RGB)\n",
    "                        img = cv2.resize(image, model_input_size, interpolation=cv2.INTER_AREA)\n",
    "                        img = img_to_array(img)\n",
    "                        img = np.expand_dims(img, axis=0)\n",
    "                        img = preprocess_input(img)\n",
    "\n",
    "                        # get features from pretrained model\n",
    "                        feature = pretrained_model.predict(img).ravel()\n",
    "                        features.append(feature)\n",
    "\n",
    "                    # convert to arrays\n",
    "                    features = np.array(features)\n",
    "\n",
    "                    np.save(path_output, features)\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        logger.info(\"Features already cached: {}\".format(path_output))\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(\"Error precomputing features {} / {},{}\".format(video_namepretrained_model_name, pooling))\n",
    "                logging.fatal(e, exc_info=True)\n",
    "                \n",
    "    else:\n",
    "        if verbose:\n",
    "            logger.info(\"Features already cached: {}\".format(path_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture class (contains keras model object and train/evaluate method, writes training results to /models/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:55:41.208904Z",
     "start_time": "2019-01-18T22:55:41.160938Z"
    }
   },
   "outputs": [],
   "source": [
    "class Architecture(object):\n",
    "    \n",
    "    def __init__(self, model_id, architecture, sequence_length, num_classes, frame_size = None, \n",
    "                pretrained_model_name = None, pooling = None,\n",
    "                sequence_model = None, sequence_model_layers = 1,\n",
    "                layer_1_size = 0, layer_2_size = 0, layer_3_size = 0, \n",
    "                dropout = 0, convolution_kernel_size = 3, model_weights_path = None):\n",
    "        \"\"\"\n",
    "        Model object constructor. Contains Keras model object and training/evaluation methods. Writes model results to /models/_id_ folder\n",
    "        \n",
    "        Architecture can be one of: \n",
    "        image_MLP_frozen, image_MLP_trainable, video_MLP_concat, video_LRCNN_frozen, video_LRCNN_trainable, 3DCNN\n",
    "        \n",
    "        :model_id: integer identifier for this model e.g. 1337\n",
    "        \n",
    "        :architecture: architecture of model in [image_MLP_frozen, image_MLP_trainable, video_MLP_concat, video_LRCNN_frozen, video_LRCNN_trainable, 3DCNN]\n",
    "        \n",
    "        :sequence_length: number of frames in sequence to be returned by Data object\n",
    "        :num_classes: number of classes to predict\n",
    "        :frame_size: size that frames are resized to (different models / architectures accept different input sizes)\n",
    "\n",
    "        :pretrained_model_name: name of pretrained model (or None if not using pretrained model e.g. for 3D-CNN)\n",
    "        :pooling: name of pooling variant (or None if not using pretrained model e.g. for 3D-CNN or if fitting more non-dense layers on top of pretrained model base)\n",
    "        \n",
    "        :sequence_model: sequence model in [LSTM, SimpleRNN, GRU, Convolution1D]\n",
    "        :sequence_model_layers: default to 1, can be stacked 2 or 3 (but less than 4) layer sequence model (assume always stacking the same sequence model, not mixing LSTM and GRU, for example)\n",
    "        \n",
    "        :layer_1_size: number of neurons in layer 1\n",
    "        :layer_2_size: number of neurons in layer 2\n",
    "        :layer_3_size: number of neurons in layer 3 \n",
    "        \n",
    "        :dropout: amount of dropout to add (same applied throughout model - good default is 0.2) \n",
    "        \n",
    "        :convolution_kernel_size: size of 1-D convolutional kernel for 1-d conv sequence models (good default is 3)\n",
    "        \n",
    "        :model_weights_path: path to .h5 weights file to be loaded for pretrained CNN in LRCNN-trainable and in 3d-CNN. Can use custom CNN for other models but need to save features first then load them in data\n",
    "        \"\"\"\n",
    "    \n",
    "        # required params\n",
    "        self.sequence_length = sequence_length\n",
    "        self.frame_size = frame_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # model architecture params\n",
    "        self.architecture = architecture\n",
    "        self.pretrained_model_name = pretrained_model_name\n",
    "        self.pooling = pooling\n",
    "        self.sequence_model = sequence_model\n",
    "        self.sequence_model_layers = sequence_model_layers\n",
    "        #\n",
    "        self.layer_1_size = layer_1_size\n",
    "        self.layer_2_size = layer_2_size\n",
    "        self.layer_3_size = layer_3_size\n",
    "        #\n",
    "        self.dropout = dropout\n",
    "        #\n",
    "        self.convolution_kernel_size = convolution_kernel_size\n",
    "        #\n",
    "        self.model_weights_path = model_weights_path\n",
    "        \n",
    "        # fix case sensitivity\n",
    "        if type(self.architecture) == str:\n",
    "            self.architecture = self.architecture.lower()\n",
    "        if type(self.pretrained_model_name) == str:\n",
    "            self.pretrained_model_name = self.pretrained_model_name.lower()\n",
    "        if type(self.pooling) == str:\n",
    "            self.pooling = self.pooling.lower()\n",
    "        \n",
    "        # read num features from pretrained model\n",
    "        if pretrained_model_name is not None:\n",
    "            self.num_features = pretrained_model_len_features[pretrained_model_name]\n",
    "            self.frame_size = pretrained_model_sizes[pretrained_model_name]\n",
    "        \n",
    "        # check one of pretrained model and frame size is specified\n",
    "        assert (self.pretrained_model_name is not None and self.frame_size is not None), \"Must specify one of pretrained_model_name or frame_size\"\n",
    "            \n",
    "        model = None\n",
    "        \n",
    "        if architecture == \"image_MLP_frozen\":\n",
    "            \n",
    "            ####################\n",
    "            ### image_MLP_frozen\n",
    "            ####################\n",
    "            # image classification (single frame)\n",
    "            # train MLP on top of weights extracted from pretrained CNN with no fine-tuning\n",
    "            \n",
    "            # check inputs\n",
    "            assert self.sequence_length == 1, \"image_MLP_frozen requires sequence length of 1\"\n",
    "            assert self.pretrained_model_name is not None, \"image_MLP_frozen requires a pretrained_model_name input\" \n",
    "            assert self.pooling is not None, \"image_MLP_frozen requires a pooling input\" \n",
    "            \n",
    "            # init model\n",
    "            model = Sequential()\n",
    "\n",
    "            # 1st layer group\n",
    "            if self.layer_1_size > 0:\n",
    "                model.add(Dense(self.layer_1_size, activation='relu', input_shape=(self.num_features,)))\n",
    "                if self.dropout > 0:\n",
    "                    model.add(Dropout(self.dropout))\n",
    "                \n",
    "            # 2nd layer group\n",
    "            if self.layer_2_size > 0 and self.layer_1_size > 0:\n",
    "                model.add(Dense(self.layer_2_size, activation='relu'))\n",
    "                if self.dropout > 0:\n",
    "                    model.add(Dropout(self.dropout))\n",
    "\n",
    "            # 3rd layer group\n",
    "            if self.layer_3_size > 0 and self.layer_2_size > 0 and self.layer_1_size > 0:\n",
    "                model.add(Dense(self.layer_3_size, activation='relu'))\n",
    "                if dropout > 0:\n",
    "                    model.add(Dropout(self.dropout))\n",
    "\n",
    "            # classifier layer\n",
    "            model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        elif architecture == \"image_MLP_trainable\":\n",
    "            \n",
    "            #######################\n",
    "            ### image_MLP_trainable\n",
    "            #######################\n",
    "            # image classification (single frame)\n",
    "            # fine-tune pretrained CNN with MLP on top\n",
    "            #\n",
    "            # start off freezing base CNN layers then will unfreeze \n",
    "            # after each training round\n",
    "            #\n",
    "            # we will ultimately want to compare our best fine-tuned \n",
    "            # CNN as a feature extractor vs fixed ImageNet pretrained CNN features\n",
    "            \n",
    "            # check inputs\n",
    "            assert self.sequence_length == 1, \"image_MLP_trainable requires sequence length of 1\"\n",
    "            assert self.pretrained_model_name is not None, \"image_MLP_trainable requires a pretrained_model_name input\" \n",
    "            assert self.pooling is not None, \"image_MLP_trainable requires a pooling input\" \n",
    "            \n",
    "            \n",
    "            # create the base pre-trained model\n",
    "            model_base = load_pretrained_model(self.pretrained_model_name, pooling=self.pooling)\n",
    "            \n",
    "\n",
    "            # freeze base model layers (will unfreeze after train top)\n",
    "            for l in model_base.layers:\n",
    "                l.trainable=False\n",
    "\n",
    "            # use Keras functional API\n",
    "            model_top = model_base.output\n",
    "\n",
    "            # note layer names are there so we can exclude those layers \n",
    "            # when setting base model layers to trainable\n",
    "\n",
    "            # 1st layer group\n",
    "            if self.layer_1_size > 0:\n",
    "                model_top = Dense(self.layer_1_size, activation=\"relu\", name='top_a')(model_top)\n",
    "                if self.dropout > 0:\n",
    "                    model_top = Dropout(self.dropout, name='top_b')(model_top)\n",
    "\n",
    "            # 2nd layer group\n",
    "            if self.layer_2_size > 0 and self.layer_1_size > 0:\n",
    "                model_top = Dense(self.layer_2_size, activation=\"relu\", name='top_c')(model_top)\n",
    "                if self.dropout > 0:\n",
    "                    model_top = Dropout(self.dropout, name='top_d')(model_top)\n",
    "\n",
    "            # 3rd layer group\n",
    "            if self.layer_3_size > 0 and self.layer_2_size > 0 and self.layer_1_size > 0:\n",
    "                model_top = Dense(self.layer_3_size, activation=\"relu\", name='top_e')(model_top)\n",
    "                if self.dropout > 0:\n",
    "                    model_top = Dropout(self.dropout, name='top_f')(model_top)\n",
    "\n",
    "            # classifier layer\n",
    "            model_predictions = Dense(self.num_classes, activation=\"softmax\", name='top_g')(model_top)\n",
    "\n",
    "            # combine base and top models into single model object\n",
    "            model = Model(inputs=model_base.input, outputs=model_predictions)\n",
    "                \n",
    "        elif architecture == \"video_MLP_concat\":\n",
    "\n",
    "            ####################\n",
    "            ### video_MLP_concat\n",
    "            ####################\n",
    "            \n",
    "            assert self.sequence_length > 1, \"video_MLP_concat requires sequence length > 1\"\n",
    "            \n",
    "            # concatenate all frames in sequence and train MLP on top of concatenated frame input\n",
    "\n",
    "            # init model\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Flatten(input_shape=(self.sequence_length, self.num_features)))\n",
    "\n",
    "            # 1st layer group\n",
    "            if self.layer_1_size > 0:\n",
    "                model.add(Dense(self.layer_1_size, activation='relu', input_shape=(self.num_features,)))\n",
    "                if self.dropout > 0:\n",
    "                    model.add(Dropout(self.dropout))\n",
    "\n",
    "            # 2nd layer group\n",
    "            if self.layer_2_size > 0 and self.layer_1_size > 0:\n",
    "                model.add(Dense(self.layer_2_size, activation='relu'))\n",
    "                if self.dropout > 0:\n",
    "                    model.add(Dropout(self.dropout))\n",
    "\n",
    "            # 3rd layer group\n",
    "            if self.layer_3_size > 0 and self.layer_2_size > 0 and self.layer_1_size > 0:\n",
    "                model.add(Dense(self.layer_3_size, activation='relu'))\n",
    "                if self.dropout > 0:\n",
    "                    model.add(Dropout(self.dropout))\n",
    "\n",
    "            # classifier layer\n",
    "            model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "            \n",
    "        elif architecture == \"video_LRCNN_frozen\":\n",
    "\n",
    "            ######################\n",
    "            ### video_LRCNN_frozen\n",
    "            ######################\n",
    "            \n",
    "            # Implement:\n",
    "            # “Long-Term Recurrent Convolutional Networks for Visual Recognition and Description.”\n",
    "            # Donahue, Jeff, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, \n",
    "            # Sergio Guadarrama, Kate Saenko, and Trevor Darrell.  \n",
    "            # Proceedings of the IEEE Computer Society Conference on Computer Vision and \n",
    "            # Pattern Recognition, 2015, 2625–34.\n",
    "            #\n",
    "            # Essentially they extract features with fine-tuned CNN then fit recurrent models on top\n",
    "            # in the paper they only use LSTM but we will also try RNN, GRU and 1-D CNN\n",
    "            # \n",
    "            # note: no fine-tuning of CNN in this frozen LRCNN architecture\n",
    "            # \n",
    "            # implementation inspired by:\n",
    "            # https://github.com/sagarvegad/Video-Classification-CNN-and-LSTM-/blob/master/train_CNN_RNN.py\n",
    "\n",
    "            \n",
    "            # check inputs\n",
    "            assert self.sequence_length > 1, \"video_LRCNN_frozen requires sequence length > 1\"\n",
    "            assert self.layer_1_size > 0, \"video_LRCNN_frozen requires a layer_1_size > 0\" \n",
    "            assert self.pretrained_model_name is not None, \"video_LRCNN_frozen requires a pretrained_model_name input\" \n",
    "            assert self.pooling is not None, \"video_LRCNN_frozen requires a pooling input\" \n",
    "            assert self.sequence_model_layers >= 1, \"video_LRCNN_frozen requires sequence_model_layers >= 1\" \n",
    "            assert self.sequence_model_layers < 4, \"video_LRCNN_frozen requires sequence_model_layers <= 3\" \n",
    "            assert self.sequence_model is not None, \"video_LRCNN_frozen requires a sequence_model\" \n",
    "            if self.sequence_model == 'Convolution1D':\n",
    "                assert self.convolution_kernel_size > 0, \"Convolution1D sequence model requires convolution_kernel_size parameter > 0\"\n",
    "                assert self.convolution_kernel_size < self.sequence_length, \"convolution_kernel_size must be less than sequence_length\"\n",
    "\n",
    "            # set whether to return sequences for stacked sequence models\n",
    "            return_sequences_1, return_sequences_2 = False, False\n",
    "            if sequence_model_layers > 1 and layer_2_size > 0:\n",
    "                return_sequences_1 = True\n",
    "            if sequence_model_layers >= 2 and layer_3_size > 0 and layer_2_size > 0:\n",
    "                return_sequences_2 = True\n",
    "            \n",
    "            # init model\n",
    "            model = Sequential()\n",
    "\n",
    "            # layer 1 (sequence layer)\n",
    "            if sequence_model == \"LSTM\":\n",
    "                model.add(LSTM(self.layer_1_size, return_sequences=return_sequences_1, dropout=self.dropout, \n",
    "                               input_shape=(self.sequence_length, self.num_features)))\n",
    "            elif sequence_model == \"SimpleRNN\":\n",
    "                model.add(SimpleRNN(self.layer_1_size, return_sequences=return_sequences_1, dropout=self.dropout, \n",
    "                               input_shape=(self.sequence_length, self.num_features)))\n",
    "            elif sequence_model == \"GRU\":\n",
    "                model.add(GRU(self.layer_1_size, return_sequences=return_sequences_1, dropout=self.dropout, \n",
    "                               input_shape=(self.sequence_length, self.num_features)))\n",
    "            elif sequence_model == \"Convolution1D\":\n",
    "                model.add(Convolution1D(self.layer_1_size, kernel_size = self.convolution_kernel_size, padding = 'valid', \n",
    "                               input_shape=(self.sequence_length, self.num_features)))\n",
    "                if layer_2_size == 0 or sequence_model_layers == 1:\n",
    "                    model.add(Flatten())\n",
    "            else:\n",
    "                raise NameError('Invalid sequence_model - must be one of [LSTM, SimpleRNN, GRU, Convolution1D]')    \n",
    "\n",
    "            # layer 2 (sequential or dense)\n",
    "            if layer_2_size > 0:\n",
    "                if return_sequences_1 == False:\n",
    "                    model.add(Dense(self.layer_2_size, activation='relu'))\n",
    "                    model.add(Dropout(self.dropout))\n",
    "                else:\n",
    "                    if sequence_model == \"LSTM\":\n",
    "                        model.add(LSTM(self.layer_2_size, return_sequences=return_sequences_2, dropout=self.dropout))\n",
    "                    elif sequence_model == \"SimpleRNN\":\n",
    "                        model.add(SimpleRNN(self.layer_2_size, return_sequences=return_sequences_2, dropout=self.dropout))\n",
    "                    elif sequence_model == \"GRU\":\n",
    "                        model.add(GRU(self.layer_2_size, return_sequences=return_sequences_2, dropout=self.dropout))\n",
    "                    elif sequence_model == \"Convolution1D\":\n",
    "                        model.add(Convolution1D(self.layer_2_size, kernel_size = self.convolution_kernel_size, padding = 'valid'))\n",
    "                    else:\n",
    "                        raise NameError('Invalid sequence_model - must be one of [LSTM, SimpleRNN, GRU, Convolution1D]') \n",
    "\n",
    "            # layer 3 (sequential or dense)\n",
    "            if layer_3_size > 0:\n",
    "                if sequence_model_layers < 3:\n",
    "                    if sequence_model_layers == 2:\n",
    "                        model.add(Flatten())\n",
    "                    model.add(Dense(self.layer_3_size, activation='relu'))\n",
    "                    model.add(Dropout(self.dropout))\n",
    "                else:\n",
    "                    if sequence_model == \"LSTM\":\n",
    "                        model.add(LSTM(self.layer_3_size, return_sequences=False, dropout=self.dropout))\n",
    "                        model.add(Flatten())\n",
    "                    elif sequence_model == \"SimpleRNN\":\n",
    "                        model.add(SimpleRNN(self.layer_3_size, return_sequences=False, dropout=self.dropout))\n",
    "                        model.add(Flatten())\n",
    "                    elif sequence_model == \"GRU\":\n",
    "                        model.add(GRU(self.layer_3_size, return_sequences=False, dropout=self.dropout))\n",
    "                        model.add(Flatten())\n",
    "                    elif sequence_model == \"Convolution1D\":\n",
    "                        model.add(Convolution1D(self.layer_3_size, kernel_size = self.convolution_kernel_size, padding = 'valid'))\n",
    "                        model.add(Flatten())\n",
    "                    else:\n",
    "                        raise NameError('Invalid sequence_model - must be one of [LSTM, SimpleRNN, GRU, Convolution1D]') \n",
    "            else:\n",
    "                if return_sequences_2 == True: \n",
    "                    model.add(Flatten())\n",
    "\n",
    "            # classifier layer\n",
    "            if self.dropout > 0:\n",
    "                model.add(Dropout(self.dropout))\n",
    "            model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        elif architecture == \"video_LRCNN_trainable\":\n",
    "            \n",
    "            #########################\n",
    "            ### video_LRCNN_trainable\n",
    "            #########################\n",
    "            \n",
    "            # Same as above:\n",
    "            # “Long-Term Recurrent Convolutional Networks for Visual Recognition and Description.”\n",
    "            # Donahue, Jeff, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, \n",
    "            # Sergio Guadarrama, Kate Saenko, and Trevor Darrell.  \n",
    "            # Proceedings of the IEEE Computer Society Conference on Computer Vision and \n",
    "            # Pattern Recognition, 2015, 2625–34.\n",
    "            #\n",
    "            # But with fine-tuning of the CNNs that are input into the recurrent models\n",
    "            # \n",
    "            # note: will take long because not precomputing the CNN part so re-computed \n",
    "            # on each training pass\n",
    "\n",
    "            # implementation inspired by https://stackoverflow.com/questions/49535488/lstm-on-top-of-a-pre-trained-cnn\n",
    "\n",
    "            model_cnn = load_pretrained_model(self.pretrained_model_name, pooling=self.pooling)\n",
    "\n",
    "            # optionally load weights for pretrained architecture\n",
    "            # (will likely be better to first train CNN then load weights in LRCNN vs. use pretrained ImageNet CNN)\n",
    "            if self.model_weights_path is not None:\n",
    "                model_base.load_weights(self.model_weights_path)\n",
    "            \n",
    "            # freeze model_cnn layers (will unfreeze later after sequence model trained a while)\n",
    "            for l in model_cnn.layers:\n",
    "                l.trainable = False\n",
    "\n",
    "            # sequential component on top of CNN\n",
    "            frames = Input(shape=(self.sequence_length, self.frame_size[0], self.frame_size[1], 3))\n",
    "            x = TimeDistributed(model_cnn)(frames)\n",
    "            x = TimeDistributed(Flatten())(x)\n",
    "\n",
    "            # layer 1 sequence model\n",
    "            x = LSTM(self.layer_1_size, dropout=dropout)(x)\n",
    "\n",
    "            # classifier layer\n",
    "            out = Dense(self.num_classes)(x)\n",
    "\n",
    "            # join cnn frame model and LSTM top\n",
    "            model = Model(inputs=frames, outputs=out)\n",
    "            \n",
    "\n",
    "            \n",
    "        elif architecture == \"3DCNN\":\n",
    "            \n",
    "            #########\n",
    "            ### 3DCNN\n",
    "            #########\n",
    "            \n",
    "            # Implement:\n",
    "            \n",
    "            # “3D Convolutional Neural Networks for Human Action Recognition.” \n",
    "            # Ji, Shuiwang, Wei Xu, Ming Yang, and Kai Yu. \n",
    "            # IEEE Transactions on Pattern Analysis and Machine Intelligence \n",
    "            # 35, no. 1 (2013): 221–31. doi:10.1109/TPAMI.2012.59.\n",
    "            #\n",
    "            # They fit a 3-D convolutional model on top of stacked frame volumes\n",
    "            \n",
    "            # Implementation from: \n",
    "            # https://gist.github.com/albertomontesg/d8b21a179c1e6cca0480ebdf292c34d2\n",
    "            # note example has input shape as (channels, sequence_length, frame_size_1, frame_size_2)\n",
    "            \n",
    "            # init model\n",
    "            model = Sequential()\n",
    "            \n",
    "            # 1st layer group\n",
    "            model.add(Convolution3D(64, 3, 3, 3, activation='relu',  border_mode='same', name='conv1', subsample=(1, 1, 1), input_shape=(3, 16, 112, 112)))\n",
    "            model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), border_mode='valid', name='pool1'))\n",
    "\n",
    "            # 2nd layer group\n",
    "            model.add(Convolution3D(128, 3, 3, 3, activation='relu',border_mode='same', name='conv2', subsample=(1, 1, 1)))\n",
    "            model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2),  border_mode='valid', name='pool2'))\n",
    "\n",
    "            # 3rd layer group\n",
    "            model.add(Convolution3D(256, 3, 3, 3, activation='relu',border_mode='same', name='conv3a', subsample=(1, 1, 1)))\n",
    "            model.add(Convolution3D(256, 3, 3, 3, activation='relu',  border_mode='same', name='conv3b', subsample=(1, 1, 1)))\n",
    "            model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='valid', name='pool3'))\n",
    "\n",
    "            # 4th layer group\n",
    "            model.add(Convolution3D(512, 3, 3, 3, activation='relu',  border_mode='same', name='conv4a', subsample=(1, 1, 1)))\n",
    "            model.add(Convolution3D(512, 3, 3, 3, activation='relu',  border_mode='same', name='conv4b', subsample=(1, 1, 1)))\n",
    "            model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='valid', name='pool4'))\n",
    "\n",
    "            # 5th layer group\n",
    "            model.add(Convolution3D(512, 3, 3, 3, activation='relu',border_mode='same', name='conv5a', subsample=(1, 1, 1)))\n",
    "            model.add(Convolution3D(512, 3, 3, 3, activation='relu', border_mode='same', name='conv5b', subsample=(1, 1, 1)))\n",
    "            model.add(ZeroPadding3D(padding=(0, 1, 1)))\n",
    "            model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='valid', name='pool5'))\n",
    "            model.add(Flatten())\n",
    "            \n",
    "            # FC layers group\n",
    "            model.add(Dense(4096, activation='relu', name='fc6'))\n",
    "            model.add(Dropout(.5))\n",
    "            model.add(Dense(4096, activation='relu', name='fc7'))\n",
    "            model.add(Dropout(.5))\n",
    "            \n",
    "            \n",
    "            # if load weights from Sports1M model then need to load with 487 class classifier then pop it and add our own\n",
    "            if self.model_weights_path is not None:\n",
    "                model.add(Dense(487, activation='softmax', name='fc8'))\n",
    "                model.load_weights(self.model_weights_path)\n",
    "                model.layers.pop()\n",
    "                model.add(Dense(self.num_classes, activation='softmax'))\n",
    "            else:\n",
    "                model.add(Dense(self.num_classes, activation='softmax', name='fc8'))\n",
    "            \n",
    "        else:\n",
    "            raise NameError('Invalid architecture - must be one of [image_MLP_frozen, image_MLP_trainable, video_MLP_concat, video_LRCNN_frozen, video_LRCNN_trainable, 3DCNN]')    \n",
    "        \n",
    "        # set class model to constructed model\n",
    "        self.model = model\n",
    "        \n",
    "        \n",
    "        # TODO\n",
    "        # init data object\n",
    "    \n",
    "    \n",
    "    # TODO:\n",
    "    # define training functions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:43:53.529101Z",
     "start_time": "2019-01-18T22:43:53.524011Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_last_layers_trainable(model, num_layers):\n",
    "    \"\"\"\n",
    "    Set the last *num_layers* non-trainable layers to trainable  \n",
    "    \n",
    "    NB to be used with model_base and assumes name = \"top_xxx\" added to each top layer to know \n",
    "    to ignore that layer when looping through layers from top backwards\n",
    "    \n",
    "    :num_layers: number of layers from end of model (that are currently not trainable) to be set as trainable\n",
    "    \"\"\"\n",
    "    \n",
    "    # get index of last non-trainable layer\n",
    "    # (the layers we added on top of model_base are already trainable=True)\n",
    "    # ...\n",
    "    # need to find last layer of base model and set that (and previous num_layers)\n",
    "    # to trainable=True via this method\n",
    "    \n",
    "    # find last non-trainable layer index\n",
    "    idx_not_trainable = 0\n",
    "    for i, l in enumerate(model.layers):\n",
    "        if \"top\" not in l.name:\n",
    "            idx_not_trainable = i\n",
    "                \n",
    "    # set last non-trainable layer and num_layers prior to trainable=True\n",
    "    for i in reversed(range(idx_not_trainable-num_layers+1, idx_not_trainable+1)):\n",
    "        model.layers[i].trainable = True\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:52:57.801109Z",
     "start_time": "2019-01-18T22:52:57.792741Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit_history = train(model_id, model, data, learning_rate = 0.001, epochs = 30)\n",
    "# model.load_weights(path_model + \"model.h5\")\n",
    "# model = make_last_layers_trainable(model, 5)\n",
    "# fit_history = train(model_id, model, data, learning_rate = 0.0001, epochs = 30)\n",
    "\n",
    "def fit(model_id, model, data, learning_rate = 0.001, epochs = 20, batch_size = 32, patience=10, verbose = verbose):\n",
    "    \"\"\"\n",
    "    Compile and fit model for *epochs* rounds of training, dividing learning rate by 10 after each round\n",
    "    \n",
    "    Fitting will stop if val_acc does not improve for at least patience epochs\n",
    "    \n",
    "    Only the best weights will be kept\n",
    "    \n",
    "    The model is saved to /models/*model_id*/\n",
    "    \n",
    "    Good practice is to decrease the learning rate by a factor of 10 after each plateau and train some more \n",
    "    (after first re-loading best weights from previous training round)...\n",
    "\n",
    "    for example:\n",
    "        fit_history = train(model_id, model, data, learning_rate = 0.001, epochs = 30)\n",
    "        model.load_weights(path_model + \"model.h5\")\n",
    "        model = make_last_layers_trainable(model, 5)\n",
    "        fit_history = train(model_id, model, data, learning_rate = 0.0001, epochs = 30)\n",
    "    \n",
    "    :model_id: integet id for model (defines where model is saved to)\n",
    "    :model: model object to train\n",
    "    :data: data object\n",
    "    \n",
    "    :learning_rate: learning rate parameter for Adam optimizer (default is 0.001)\n",
    "    \n",
    "    :epochs: number of training epochs per fit round (subject to patience)\n",
    "    :batch_size: number of samples in each batch\n",
    "    :patience: how many epochs without val_acc improvement before stopping fit round\n",
    "    :verbose: print progress\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # create path based on model id\n",
    "    path_model = pwd + 'models/' + str(model_id) + '/'\n",
    "    if not os.path.exists(path_model):\n",
    "        os.makedirs(path_model)\n",
    "    \n",
    "    # create optimizer with given learning rate \n",
    "    opt = Adam(lr = learning_rate)\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # setup training callbacks\n",
    "    callback_stopper = EarlyStopping(monitor='val_acc', patience=patience, verbose=0)\n",
    "    callback_csvlogger = CSVLogger(path_model + 'training.log')\n",
    "    callback_checkpointer = ModelCheckpoint(path_model +  'model.h5', monitor='val_acc', save_best_only=True, verbose=verbose)\n",
    "    callbacks = [callback_stopper, callback_checkpointer, callback_csvlogger]\n",
    "    \n",
    "    # fit model\n",
    "    if data.return_generator == True:\n",
    "        # train using generator\n",
    "        history = model.fit_generator(generator=data.generator_train,\n",
    "                        validation_data=data.generator_valid,\n",
    "                        use_multiprocessing=True,\n",
    "                        workers=num_workers,\n",
    "                        epochs=epochs,\n",
    "                        callbacks=callbacks,\n",
    "                        shuffle=True,\n",
    "                        verbose=verbose)\n",
    "\n",
    "        return history\n",
    "    else:\n",
    "        # train using full dataset\n",
    "        history = model.fit(data.x_train, data.y_train, \n",
    "                  validation_data=(data.x_valid, data.y_valid),\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  callbacks=callbacks,\n",
    "                  shuffle=True,\n",
    "                  verbose=verbose)\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## image_MLP_trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:23:10.553902Z",
     "start_time": "2019-01-18T22:20:07.127527Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pretrained_model_name = \"vgg16\"\n",
    "pooling=\"max\"\n",
    "sequence_length = 2\n",
    "\n",
    "layer_1_size = 128\n",
    "layer_2_size = 64\n",
    "layer_3_size = 32\n",
    "dropout=0.20\n",
    "\n",
    "data = Data(sequence_length = 1, \n",
    "            return_CNN_features = False, \n",
    "            pretrained_model_name=pretrained_modesl_name,\n",
    "            pooling = pooling,\n",
    "            return_generator=True,\n",
    "            batch_size=32)\n",
    "\n",
    "num_classes = data.num_classes \n",
    "frame_size = data.frame_size\n",
    "num_features = pretrained_model_len_features[pretrained_model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:23:15.857922Z",
     "start_time": "2019-01-18T22:23:10.570598Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "architecture = Architecture(model_id = 1, \n",
    "                            architecture = 'image_MLP_trainable',\n",
    "                            sequence_length = 1, \n",
    "                            num_classes = num_classes, \n",
    "                            pretrained_model_name = pretrained_model_name, \n",
    "                            pooling = 'max', \n",
    "                            layer_1_size=128,\n",
    "                            layer_2_size=0, \n",
    "                            layer_3_size=0,\n",
    "                            dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:34:05.619518Z",
     "start_time": "2019-01-18T22:33:59.879932Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fit(model_id=1337, model = architecture.model, data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## image_MLP_frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:44:03.874794Z",
     "start_time": "2019-01-18T22:44:03.871357Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pretrained_model_name = \"vgg16\"\n",
    "pooling=\"max\"\n",
    "layer_1_size = 128\n",
    "layer_2_size = 64\n",
    "layer_3_size = 32\n",
    "dropout=0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:44:08.272087Z",
     "start_time": "2019-01-18T22:44:06.515992Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-18 22:44:07,490 [MainThread  ] [INFO ]  Features already cached: /mnt/seals/cache/features/vgg16/max/\n",
      "2019-01-18 22:44:07,491 [MainThread  ] [INFO ]  Loading features data into memory [may take a few minutes]\n",
      "2019-01-18 22:44:07,493 [MainThread  ] [INFO ]  Loading features data into memory: 1/25\n",
      "2019-01-18 22:44:07,532 [MainThread  ] [INFO ]  Loading features data into memory: 2/25\n",
      "2019-01-18 22:44:07,548 [MainThread  ] [INFO ]  Loading features data into memory: 3/25\n",
      "2019-01-18 22:44:07,570 [MainThread  ] [INFO ]  Loading features data into memory: 4/25\n",
      "2019-01-18 22:44:07,597 [MainThread  ] [INFO ]  Loading features data into memory: 5/25\n",
      "2019-01-18 22:44:07,608 [MainThread  ] [INFO ]  Loading features data into memory: 6/25\n",
      "2019-01-18 22:44:07,612 [MainThread  ] [INFO ]  Loading features data into memory: 7/25\n",
      "2019-01-18 22:44:07,628 [MainThread  ] [INFO ]  Loading features data into memory: 8/25\n",
      "2019-01-18 22:44:07,647 [MainThread  ] [INFO ]  Loading features data into memory: 9/25\n",
      "2019-01-18 22:44:07,664 [MainThread  ] [INFO ]  Loading features data into memory: 10/25\n",
      "2019-01-18 22:44:07,674 [MainThread  ] [INFO ]  Loading features data into memory: 11/25\n",
      "2019-01-18 22:44:07,690 [MainThread  ] [INFO ]  Loading features data into memory: 12/25\n",
      "2019-01-18 22:44:07,711 [MainThread  ] [INFO ]  Loading features data into memory: 13/25\n",
      "2019-01-18 22:44:07,732 [MainThread  ] [INFO ]  Loading features data into memory: 14/25\n",
      "2019-01-18 22:44:07,739 [MainThread  ] [INFO ]  Loading features data into memory: 15/25\n",
      "2019-01-18 22:44:07,766 [MainThread  ] [INFO ]  Loading features data into memory: 16/25\n",
      "2019-01-18 22:44:07,791 [MainThread  ] [INFO ]  Loading features data into memory: 17/25\n",
      "2019-01-18 22:44:07,819 [MainThread  ] [INFO ]  Loading features data into memory: 18/25\n",
      "2019-01-18 22:44:07,848 [MainThread  ] [INFO ]  Loading features data into memory: 19/25\n",
      "2019-01-18 22:44:07,861 [MainThread  ] [INFO ]  Loading features data into memory: 20/25\n",
      "2019-01-18 22:44:07,884 [MainThread  ] [INFO ]  Loading features data into memory: 21/25\n",
      "2019-01-18 22:44:07,888 [MainThread  ] [INFO ]  Loading features data into memory: 22/25\n",
      "2019-01-18 22:44:07,918 [MainThread  ] [INFO ]  Loading features data into memory: 23/25\n",
      "2019-01-18 22:44:07,947 [MainThread  ] [INFO ]  Loading features data into memory: 24/25\n",
      "2019-01-18 22:44:07,969 [MainThread  ] [INFO ]  Loading features data into memory: 25/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60597, 512) (60597, 8) 60597\n"
     ]
    }
   ],
   "source": [
    "data = Data(sequence_length = 1, \n",
    "            return_CNN_features = True, \n",
    "            pretrained_model_name=pretrained_model_name,\n",
    "            pooling = pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:44:10.606595Z",
     "start_time": "2019-01-18T22:44:10.602737Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_classes = data.num_classes \n",
    "frame_size = data.frame_size\n",
    "num_features = pretrained_model_len_features[pretrained_model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:44:18.893841Z",
     "start_time": "2019-01-18T22:44:18.827051Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "architecture = Architecture(model_id = 1, \n",
    "                            architecture = 'image_MLP_frozen',\n",
    "                            sequence_length = 1, \n",
    "                            num_classes = num_classes, \n",
    "                            pretrained_model_name = pretrained_model_name, \n",
    "                            pooling = 'max', \n",
    "                            layer_1_size=128,\n",
    "                            layer_2_size=0, \n",
    "                            layer_3_size=0,\n",
    "                            dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:45:03.797635Z",
     "start_time": "2019-01-18T22:44:51.952354Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60597 samples, validate on 6416 samples\n",
      "Epoch 1/10\n",
      "42784/60597 [====================>.........] - ETA: 4s - loss: 0.2732 - acc: 0.9305"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-41a5d3419de1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfit_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marchitecture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-b545e3e63f88>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model_id, model, data, learning_rate, epochs, batch_size, patience, verbose)\u001b[0m\n\u001b[1;32m     74\u001b[0m                   \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                   verbose=verbose)\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "fit_history = fit(model_id = 1, model = architecture.model, data = data, learning_rate = 0.001, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## video_MLP_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:45:22.314938Z",
     "start_time": "2019-01-18T22:45:22.310813Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model_name = \"vgg16\"\n",
    "pooling=\"max\"\n",
    "sequence_length = 3\n",
    "layer_1_size = 128\n",
    "layer_2_size = 64\n",
    "layer_3_size = 32\n",
    "dropout=0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:51:12.468934Z",
     "start_time": "2019-01-18T22:51:10.413810Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-18 22:51:11,428 [MainThread  ] [INFO ]  Features already cached: /mnt/seals/cache/features/vgg16/max/\n",
      "2019-01-18 22:51:11,429 [MainThread  ] [INFO ]  Loading features sequence data into memory [may take a few minutes]\n",
      "2019-01-18 22:51:11,431 [MainThread  ] [INFO ]  Loading features sequence data into memory 1/25\n",
      "2019-01-18 22:51:11,457 [MainThread  ] [INFO ]  Loading features sequence data into memory 2/25\n",
      "2019-01-18 22:51:11,483 [MainThread  ] [INFO ]  Loading features sequence data into memory 3/25\n",
      "2019-01-18 22:51:11,513 [MainThread  ] [INFO ]  Loading features sequence data into memory 4/25\n",
      "2019-01-18 22:51:11,548 [MainThread  ] [INFO ]  Loading features sequence data into memory 5/25\n",
      "2019-01-18 22:51:11,568 [MainThread  ] [INFO ]  Loading features sequence data into memory 6/25\n",
      "2019-01-18 22:51:11,572 [MainThread  ] [INFO ]  Loading features sequence data into memory 7/25\n",
      "2019-01-18 22:51:11,599 [MainThread  ] [INFO ]  Loading features sequence data into memory 8/25\n",
      "2019-01-18 22:51:11,630 [MainThread  ] [INFO ]  Loading features sequence data into memory 9/25\n",
      "2019-01-18 22:51:11,662 [MainThread  ] [INFO ]  Loading features sequence data into memory 10/25\n",
      "2019-01-18 22:51:11,667 [MainThread  ] [INFO ]  Loading features sequence data into memory 11/25\n",
      "2019-01-18 22:51:11,691 [MainThread  ] [INFO ]  Loading features sequence data into memory 12/25\n",
      "2019-01-18 22:51:11,719 [MainThread  ] [INFO ]  Loading features sequence data into memory 13/25\n",
      "2019-01-18 22:51:11,752 [MainThread  ] [INFO ]  Loading features sequence data into memory 14/25\n",
      "2019-01-18 22:51:11,756 [MainThread  ] [INFO ]  Loading features sequence data into memory 15/25\n",
      "2019-01-18 22:51:11,785 [MainThread  ] [INFO ]  Loading features sequence data into memory 16/25\n",
      "2019-01-18 22:51:11,810 [MainThread  ] [INFO ]  Loading features sequence data into memory 17/25\n",
      "2019-01-18 22:51:11,838 [MainThread  ] [INFO ]  Loading features sequence data into memory 18/25\n",
      "2019-01-18 22:51:11,867 [MainThread  ] [INFO ]  Loading features sequence data into memory 19/25\n",
      "2019-01-18 22:51:11,872 [MainThread  ] [INFO ]  Loading features sequence data into memory 20/25\n",
      "2019-01-18 22:51:11,900 [MainThread  ] [INFO ]  Loading features sequence data into memory 21/25\n",
      "2019-01-18 22:51:11,905 [MainThread  ] [INFO ]  Loading features sequence data into memory 22/25\n",
      "2019-01-18 22:51:11,934 [MainThread  ] [INFO ]  Loading features sequence data into memory 23/25\n",
      "2019-01-18 22:51:11,967 [MainThread  ] [INFO ]  Loading features sequence data into memory 24/25\n",
      "2019-01-18 22:51:11,990 [MainThread  ] [INFO ]  Loading features sequence data into memory 25/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60553, 3, 512) (60553, 8) 60597\n"
     ]
    }
   ],
   "source": [
    "data = Data(sequence_length = sequence_length, \n",
    "            return_CNN_features = True, \n",
    "            pretrained_model_name=pretrained_model_name,\n",
    "            pooling = pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:51:12.475398Z",
     "start_time": "2019-01-18T22:51:12.472110Z"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = data.num_classes \n",
    "frame_size = data.frame_size\n",
    "num_features = pretrained_model_len_features[pretrained_model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:51:20.295847Z",
     "start_time": "2019-01-18T22:51:20.233133Z"
    }
   },
   "outputs": [],
   "source": [
    "architecture = Architecture(model_id = 1, \n",
    "                            architecture = 'video_MLP_concat',\n",
    "                            sequence_length = 3, \n",
    "                            num_classes = num_classes, \n",
    "                            pretrained_model_name = pretrained_model_name, \n",
    "                            pooling = 'max', \n",
    "                            layer_1_size=128,\n",
    "                            layer_2_size=0, \n",
    "                            layer_3_size=0,\n",
    "                            dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:51:57.943254Z",
     "start_time": "2019-01-18T22:51:24.193829Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60553 samples, validate on 6412 samples\n",
      "Epoch 1/10\n",
      "60553/60553 [==============================] - 11s 182us/step - loss: 0.7410 - acc: 0.9109 - val_loss: 0.1974 - val_acc: 0.9171\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91707, saving model to /mnt/seals/models/1/model.h5\n",
      "Epoch 2/10\n",
      "60553/60553 [==============================] - 10s 172us/step - loss: 0.1284 - acc: 0.9502 - val_loss: 0.1790 - val_acc: 0.9229\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.91707 to 0.92292, saving model to /mnt/seals/models/1/model.h5\n",
      "Epoch 3/10\n",
      "60553/60553 [==============================] - 11s 174us/step - loss: 0.1131 - acc: 0.9550 - val_loss: 0.1677 - val_acc: 0.9286\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.92292 to 0.92863, saving model to /mnt/seals/models/1/model.h5\n",
      "Epoch 4/10\n",
      " 5984/60553 [=>............................] - ETA: 9s - loss: 0.1025 - acc: 0.958"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-41a5d3419de1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfit_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marchitecture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-b545e3e63f88>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model_id, model, data, learning_rate, epochs, batch_size, patience, verbose)\u001b[0m\n\u001b[1;32m     74\u001b[0m                   \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                   verbose=verbose)\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# will be handled by on_epoch_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    421\u001b[0m                 \u001b[0minfo\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36m_is_master_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_master_pid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "fit_history = fit(model_id = 1, model = architecture.model, data = data, learning_rate = 0.001, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## video_LRCNN_frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:55:46.310015Z",
     "start_time": "2019-01-18T22:55:46.305735Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pretrained_model_name = \"vgg16\"\n",
    "pooling=\"max\"\n",
    "sequence_length = 3\n",
    "layer_1_size = 128\n",
    "layer_2_size = 64\n",
    "layer_3_size = 32\n",
    "dropout=0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:55:48.261814Z",
     "start_time": "2019-01-18T22:55:46.495928Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-18 22:55:47,422 [MainThread  ] [INFO ]  Features already cached: /mnt/seals/cache/features/vgg16/max/\n",
      "2019-01-18 22:55:47,424 [MainThread  ] [INFO ]  Loading features sequence data into memory [may take a few minutes]\n",
      "2019-01-18 22:55:47,426 [MainThread  ] [INFO ]  Loading features sequence data into memory 1/25\n",
      "2019-01-18 22:55:47,441 [MainThread  ] [INFO ]  Loading features sequence data into memory 2/25\n",
      "2019-01-18 22:55:47,453 [MainThread  ] [INFO ]  Loading features sequence data into memory 3/25\n",
      "2019-01-18 22:55:47,467 [MainThread  ] [INFO ]  Loading features sequence data into memory 4/25\n",
      "2019-01-18 22:55:47,482 [MainThread  ] [INFO ]  Loading features sequence data into memory 5/25\n",
      "2019-01-18 22:55:47,491 [MainThread  ] [INFO ]  Loading features sequence data into memory 6/25\n",
      "2019-01-18 22:55:47,497 [MainThread  ] [INFO ]  Loading features sequence data into memory 7/25\n",
      "2019-01-18 22:55:47,511 [MainThread  ] [INFO ]  Loading features sequence data into memory 8/25\n",
      "2019-01-18 22:55:47,526 [MainThread  ] [INFO ]  Loading features sequence data into memory 9/25\n",
      "2019-01-18 22:55:47,541 [MainThread  ] [INFO ]  Loading features sequence data into memory 10/25\n",
      "2019-01-18 22:55:47,545 [MainThread  ] [INFO ]  Loading features sequence data into memory 11/25\n",
      "2019-01-18 22:55:47,556 [MainThread  ] [INFO ]  Loading features sequence data into memory 12/25\n",
      "2019-01-18 22:55:47,571 [MainThread  ] [INFO ]  Loading features sequence data into memory 13/25\n",
      "2019-01-18 22:55:47,586 [MainThread  ] [INFO ]  Loading features sequence data into memory 14/25\n",
      "2019-01-18 22:55:47,590 [MainThread  ] [INFO ]  Loading features sequence data into memory 15/25\n",
      "2019-01-18 22:55:47,606 [MainThread  ] [INFO ]  Loading features sequence data into memory 16/25\n",
      "2019-01-18 22:55:47,619 [MainThread  ] [INFO ]  Loading features sequence data into memory 17/25\n",
      "2019-01-18 22:55:47,634 [MainThread  ] [INFO ]  Loading features sequence data into memory 18/25\n",
      "2019-01-18 22:55:47,649 [MainThread  ] [INFO ]  Loading features sequence data into memory 19/25\n",
      "2019-01-18 22:55:47,654 [MainThread  ] [INFO ]  Loading features sequence data into memory 20/25\n",
      "2019-01-18 22:55:47,676 [MainThread  ] [INFO ]  Loading features sequence data into memory 21/25\n",
      "2019-01-18 22:55:47,680 [MainThread  ] [INFO ]  Loading features sequence data into memory 22/25\n",
      "2019-01-18 22:55:47,710 [MainThread  ] [INFO ]  Loading features sequence data into memory 23/25\n",
      "2019-01-18 22:55:47,741 [MainThread  ] [INFO ]  Loading features sequence data into memory 24/25\n",
      "2019-01-18 22:55:47,764 [MainThread  ] [INFO ]  Loading features sequence data into memory 25/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60553, 3, 512) (60553, 8) 60597\n"
     ]
    }
   ],
   "source": [
    "data = Data(sequence_length = sequence_length, \n",
    "            return_CNN_features = True, \n",
    "            pretrained_model_name=pretrained_model_name,\n",
    "            pooling = pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:55:48.786810Z",
     "start_time": "2019-01-18T22:55:48.264468Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "architecture = Architecture(model_id=1,\n",
    "                            architecture=\"video_LRCNN_frozen\", \n",
    "                            sequence_model = 'LSTM',\n",
    "                            sequence_model_layers = 1,\n",
    "                            sequence_length = sequence_length,\n",
    "                            num_classes = data.num_classes, \n",
    "                            frame_size = data.frame_size, \n",
    "                            pretrained_model_name='vgg16', \n",
    "                            pooling='max',\n",
    "                            layer_1_size=64, \n",
    "                            layer_2_size=32, \n",
    "                            layer_3_size=8, \n",
    "                            dropout=0.2,\n",
    "                            convolution_kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:01:40.463636Z",
     "start_time": "2019-01-18T22:56:13.574379Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60553 samples, validate on 6412 samples\n",
      "Epoch 1/10\n",
      "60553/60553 [==============================] - 34s 554us/step - loss: 0.2245 - acc: 0.9138 - val_loss: 0.1770 - val_acc: 0.9226\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.92263, saving model to /mnt/seals/models/1/model.h5\n",
      "Epoch 2/10\n",
      "60553/60553 [==============================] - 32s 532us/step - loss: 0.1888 - acc: 0.9246 - val_loss: 0.1734 - val_acc: 0.9220\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.92263\n",
      "Epoch 3/10\n",
      "60553/60553 [==============================] - 32s 535us/step - loss: 0.1836 - acc: 0.9276 - val_loss: 0.1857 - val_acc: 0.9212\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.92263\n",
      "Epoch 4/10\n",
      "60553/60553 [==============================] - 32s 536us/step - loss: 0.1740 - acc: 0.9318 - val_loss: 0.1523 - val_acc: 0.9390\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.92263 to 0.93902, saving model to /mnt/seals/models/1/model.h5\n",
      "Epoch 5/10\n",
      "60553/60553 [==============================] - 32s 532us/step - loss: 0.1712 - acc: 0.9337 - val_loss: 0.1496 - val_acc: 0.9418\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.93902 to 0.94185, saving model to /mnt/seals/models/1/model.h5\n",
      "Epoch 6/10\n",
      "60553/60553 [==============================] - 32s 533us/step - loss: 0.1697 - acc: 0.9347 - val_loss: 0.1738 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.94185\n",
      "Epoch 7/10\n",
      "60553/60553 [==============================] - 32s 530us/step - loss: 0.1663 - acc: 0.9365 - val_loss: 0.1594 - val_acc: 0.9417\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.94185\n",
      "Epoch 8/10\n",
      "60553/60553 [==============================] - 32s 536us/step - loss: 0.1621 - acc: 0.9379 - val_loss: 0.1546 - val_acc: 0.9394\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.94185\n",
      "Epoch 9/10\n",
      "60553/60553 [==============================] - 33s 540us/step - loss: 0.1613 - acc: 0.9387 - val_loss: 0.1978 - val_acc: 0.9182\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.94185\n",
      "Epoch 10/10\n",
      "60553/60553 [==============================] - 33s 540us/step - loss: 0.1569 - acc: 0.9400 - val_loss: 0.1593 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.94185\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "fit_history = fit(model_id=1, model=architecture.model, data=data, learning_rate = 0.001, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## video_LRCNN_trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T23:03:57.184348Z",
     "start_time": "2019-01-18T23:03:57.180436Z"
    }
   },
   "outputs": [],
   "source": [
    "sequence_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-18T23:03:57.348Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-18 23:03:58,752 [MainThread  ] [INFO ]  Computing frame sequence h5 files: /mnt/seals/cache/sequences/vgg16/ [may take a few minutes]\n",
      "2019-01-18 23:03:58,754 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 1/25 [precompute]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing partially created sequences cache file: /mnt/seals/cache/sequences/vgg16/h5_meta.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-18 23:04:00,306 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 2/25 [precompute]\n",
      "2019-01-18 23:04:01,559 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 3/25 [precompute]\n",
      "2019-01-18 23:04:03,133 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 4/25 [precompute]\n",
      "2019-01-18 23:04:04,733 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 5/25 [precompute]\n",
      "2019-01-18 23:04:05,622 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 6/25 [precompute]\n",
      "2019-01-18 23:04:05,776 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 7/25 [precompute]\n",
      "2019-01-18 23:04:07,134 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 8/25 [precompute]\n",
      "2019-01-18 23:04:08,720 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 9/25 [precompute]\n",
      "2019-01-18 23:04:10,268 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 10/25 [precompute]\n",
      "2019-01-18 23:04:10,434 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 11/25 [precompute]\n",
      "2019-01-18 23:04:11,587 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 12/25 [precompute]\n",
      "2019-01-18 23:04:13,169 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 13/25 [precompute]\n",
      "2019-01-18 23:04:14,764 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 14/25 [precompute]\n",
      "2019-01-18 23:04:14,921 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 15/25 [precompute]\n",
      "2019-01-18 23:04:16,409 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 16/25 [precompute]\n",
      "2019-01-18 23:04:17,785 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 17/25 [precompute]\n",
      "2019-01-18 23:04:19,266 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 18/25 [precompute]\n",
      "2019-01-18 23:04:20,868 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 19/25 [precompute]\n",
      "2019-01-18 23:04:21,223 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 20/25 [precompute]\n",
      "2019-01-18 23:04:22,752 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 21/25 [precompute]\n",
      "2019-01-18 23:04:22,908 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 22/25 [precompute]\n",
      "2019-01-18 23:04:24,411 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 23/25 [precompute]\n",
      "2019-01-18 23:04:26,012 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 24/25 [precompute]\n",
      "2019-01-18 23:04:27,235 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 25/25 [precompute]\n",
      "2019-01-18 23:04:28,788 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 1/25 [build h5 file]\n",
      "2019-01-18 23:04:45,995 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 2/25 [build h5 file]\n",
      "2019-01-18 23:04:59,888 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 3/25 [build h5 file]\n",
      "2019-01-18 23:05:17,167 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 4/25 [build h5 file]\n",
      "2019-01-18 23:05:34,625 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 5/25 [build h5 file]\n"
     ]
    }
   ],
   "source": [
    "data = Data(sequence_length = sequence_length, \n",
    "            return_CNN_features = False, \n",
    "            pretrained_model_name=\"vgg16\",\n",
    "            pooling = \"max\",\n",
    "            batch_size=32,\n",
    "            return_generator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-18T23:05:06.016Z"
    }
   },
   "outputs": [],
   "source": [
    "architecture = Architecture(architecture=\"video_LRCNN_trainable\", \n",
    "                            sequence_model = 'LSTM',\n",
    "                            sequence_model_layers = 1,\n",
    "                            sequence_length = sequence_length,\n",
    "                            num_classes = data.num_classes, \n",
    "                            frame_size = data.frame_size, \n",
    "                            pretrained_model_name='vgg16', \n",
    "                            pooling='max',\n",
    "                            layer_1_size=64, \n",
    "                            layer_2_size=32, \n",
    "                            layer_3_size=8, \n",
    "                            dropout=0.2,\n",
    "                            convolution_kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "fit_history = fit(model_id = 1, model = architecture.model, data = data, learning_rate = 0.001, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T23:14:32.403927Z",
     "start_time": "2019-01-07T23:14:32.400506Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## 3DCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:23:40.739639Z",
     "start_time": "2019-01-18T22:20:04.437Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 3d-CNN\n",
    "data = Data(sequence_length = 16, \n",
    "            return_CNN_features = False, \n",
    "            frame_size = (112,112))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:23:40.741029Z",
     "start_time": "2019-01-18T22:20:04.440Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv3D(\n",
    "    32, (3,3,3), activation='relu', input_shape=(sequence_length, 112, 112, 3)\n",
    "))\n",
    "model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "model.add(Conv3D(64, (3,3,3), activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "model.add(Conv3D(128, (3,3,3), activation='relu'))\n",
    "# model.add(Conv3D(128, (3,3,3), activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "model.add(Conv3D(256, (2,2,2), activation='relu'))\n",
    "# model.add(Conv3D(256, (2,2,2), activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1024))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(data.num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:23:40.742341Z",
     "start_time": "2019-01-18T22:20:04.442Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr = 0.001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(data.x_train, data.y_train, \n",
    "              validation_data=(data.x_valid, data.y_valid),\n",
    "              batch_size=32,\n",
    "              epochs=10,\n",
    "              shuffle=True,\n",
    "              verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:23:40.746130Z",
     "start_time": "2019-01-18T22:20:04.446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# # 1st layer group\n",
    "# model.add(Convolution3D(64, 3, 3, 3, activation='relu', \n",
    "#                         border_mode='same', name='conv1',\n",
    "#                         subsample=(1, 1, 1), \n",
    "#                         input_shape=(3, 16, 112, 112)))\n",
    "# model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), \n",
    "#                        border_mode='valid', name='pool1'))\n",
    "# # 2nd layer group\n",
    "# model.add(Convolution3D(128, 3, 3, 3, activation='relu', \n",
    "#                         border_mode='same', name='conv2',\n",
    "#                         subsample=(1, 1, 1)))\n",
    "# model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), \n",
    "#                        border_mode='valid', name='pool2'))\n",
    "# # 3rd layer group\n",
    "# model.add(Convolution3D(256, 3, 3, 3, activation='relu', \n",
    "#                         border_mode='same', name='conv3a',\n",
    "#                         subsample=(1, 1, 1)))\n",
    "# model.add(Convolution3D(256, 3, 3, 3, activation='relu', \n",
    "#                         border_mode='same', name='conv3b',\n",
    "#                         subsample=(1, 1, 1)))\n",
    "# model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), \n",
    "#                        border_mode='valid', name='pool3'))\n",
    "# # 4th layer group\n",
    "# model.add(Convolution3D(512, 3, 3, 3, activation='relu', \n",
    "#                         border_mode='same', name='conv4a',\n",
    "#                         subsample=(1, 1, 1)))\n",
    "# model.add(Convolution3D(512, 3, 3, 3, activation='relu', \n",
    "#                         border_mode='same', name='conv4b',\n",
    "#                         subsample=(1, 1, 1)))\n",
    "# model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), \n",
    "#                        border_mode='valid', name='pool4'))\n",
    "# # 5th layer group\n",
    "# model.add(Convolution3D(512, 3, 3, 3, activation='relu', \n",
    "#                         border_mode='same', name='conv5a',\n",
    "#                         subsample=(1, 1, 1)))\n",
    "# model.add(Convolution3D(512, 3, 3, 3, activation='relu', \n",
    "#                         border_mode='same', name='conv5b',\n",
    "#                         subsample=(1, 1, 1)))\n",
    "# model.add(ZeroPadding3D(padding=(0, 1, 1)))\n",
    "# model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), \n",
    "#                        border_mode='valid', name='pool5'))\n",
    "# model.add(Flatten())\n",
    "# # FC layers group\n",
    "# model.add(Dense(4096, activation='relu', name='fc6'))\n",
    "# model.add(Dropout(.5))\n",
    "# model.add(Dense(4096, activation='relu', name='fc7'))\n",
    "# model.add(Dropout(.5))\n",
    "# model.add(Dense(487, activation='softmax', name='fc8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:23:40.747372Z",
     "start_time": "2019-01-18T22:20:04.448Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:23:40.748444Z",
     "start_time": "2019-01-18T22:20:04.451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # reorder default tensorflow axis order for 3D-CNN architecture\n",
    "# data.x_train = np.moveaxis(data.x_train,4,1)\n",
    "# data.x_valid = np.moveaxis(data.x_valid,4,1)\n",
    "# data.x_test = np.moveaxis(data.x_test,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:23:40.749727Z",
     "start_time": "2019-01-18T22:20:04.453Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# num_classes = data.num_classes \n",
    "# frame_size = data.frame_size\n",
    "# num_features = pretrained_model_len_features[pretrained_model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:23:40.751056Z",
     "start_time": "2019-01-18T22:20:04.455Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "architecture = Architecture(architecture=\"3DCNN\", \n",
    "                     sequence_length = 16,\n",
    "                     num_classes =  data.num_classes,\n",
    "                     frame_size = data.frame_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate (move to model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:23:40.752150Z",
     "start_time": "2019-01-18T22:20:04.456Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate(architecture.model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:23:40.753301Z",
     "start_time": "2019-01-18T22:20:04.458Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set, returning percentage of classes exactly correct and dataframe with predicted probabilities and labels\n",
    "    \n",
    "    :model: trained model object\n",
    "    :data: data object\n",
    "    \"\"\"\n",
    "    #######################\n",
    "    ### predict on test set\n",
    "    #######################\n",
    "\n",
    "    # calculate predictions on test set\n",
    "    predictions = model.predict(data.x_test)\n",
    "\n",
    "    # build dataframe and calculate test error\n",
    "    pdf = pd.DataFrame(predictions, columns = list(data.label_map.values()))\n",
    "    pdf['prediction'] = pdf.idxmax(axis=1)\n",
    "    truth = pd.DataFrame(data.y_test, columns = list(data.label_map.values()))\n",
    "    truth['label'] = truth.idxmax(axis=1)\n",
    "    truth = truth[['label']]\n",
    "    pdf = pd.concat([pdf, truth], axis=1)\n",
    "    pdf['error'] = (pdf['prediction'] != pdf['label']).astype(int)\n",
    "    test_acc = 1 - pdf['error'].mean()\n",
    "    print(\"test_acc {}\".format(test_acc))\n",
    "\n",
    "    return pdf, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:23:40.754575Z",
     "start_time": "2019-01-18T22:20:04.460Z"
    }
   },
   "outputs": [],
   "source": [
    "path_models = pwd + 'models/'\n",
    "\n",
    "results = []\n",
    "\n",
    "for folder, subs, files in os.walk(path_models):\n",
    "    for filename in files:\n",
    "        if 'results.json' in filename:\n",
    "            with open(os.path.abspath(os.path.join(folder, filename))) as f:\n",
    "                data = json.load(f)\n",
    "            results.append(data)\n",
    "\n",
    "results = pd.DataFrame(results)        \n",
    "results.sort_values(\"fit_val_acc\", inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T22:23:40.755822Z",
     "start_time": "2019-01-18T22:20:04.461Z"
    }
   },
   "outputs": [],
   "source": [
    "results.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
