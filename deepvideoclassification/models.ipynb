{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T00:25:51.405086Z",
     "start_time": "2019-01-06T00:25:51.401182Z"
    }
   },
   "outputs": [],
   "source": [
    "# whether to log each feature and sequence status\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T00:25:55.284991Z",
     "start_time": "2019-01-06T00:25:51.780848Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from keras.layers import Dense, Flatten, Dropout, ZeroPadding3D\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling3D, Conv3D, MaxPooling2D, Convolution1D\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T00:25:56.922122Z",
     "start_time": "2019-01-06T00:25:56.918509Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup paths\n",
    "pwd = os.getcwd().replace(\"deepvideoclassification\",\"\")\n",
    "path_cache = pwd + 'cache/'\n",
    "path_data = pwd + 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T00:25:57.303543Z",
     "start_time": "2019-01-06T00:25:57.299009Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"{0}/{1}.log\".format(pwd, \"logs\")),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained_CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T00:26:01.046325Z",
     "start_time": "2019-01-06T00:26:01.042179Z"
    }
   },
   "outputs": [],
   "source": [
    "# # define pretrained model shapes\n",
    "# pretrained_model_len_features = {}\n",
    "# #\n",
    "# pretrained_model_len_features['vgg16'] = 512\n",
    "# pretrained_model_len_features['resnet50'] = 2048\n",
    "# pretrained_model_len_features['mobilenetv2_1.00_224'] = 1280\n",
    "# #\n",
    "# pretrained_model_len_features['xception'] = 9999999999\n",
    "# pretrained_model_len_features['inception_v3'] = 9999999999\n",
    "# pretrained_model_len_features['inception_resnet_v2'] = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pretrained model shapes\n",
    "pretrained_model_sizes = {}\n",
    "#\n",
    "pretrained_model_sizes['vgg16'] = (224,224)\n",
    "pretrained_model_sizes['resnet50'] = (224,224)\n",
    "pretrained_model_sizes['mobilenetv2_1.00_224'] = (224,224)\n",
    "#\n",
    "pretrained_model_sizes['xception'] = (299,299)\n",
    "pretrained_model_sizes['inception_v3'] = (299,299)\n",
    "pretrained_model_sizes['inception_resnet_v2'] = (299,299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_names = [\"inception_resnet_v2\", \"inception_v3\", \"mobilenetv2_1.00_224\", \"resnet50\", \"vgg16\", \"xception\"]\n",
    "poolings = ['max','avg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> insert precompute stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image/video classification model object with various architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T00:26:37.503044Z",
     "start_time": "2019-01-06T00:26:37.476870Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, sequence_length, target_size, architecture, pretrained_model_name, pooling, sequence_model = None,\n",
    "                num_epochs = 100, layer_1_size = 0, layer_2_size = 0, layer_3_size = 0, dropout = 0):\n",
    "        \"\"\"\n",
    "        Model object constructor\n",
    "        \n",
    "        :sequence_length: number of frames in sequence to be returned by Data object\n",
    "        :target_size: size that frames are resized to (different models / architectures accept different input sizes)\n",
    "\n",
    "        :architecture: architecture of model in [image_MLP_frozen, image_MLP_trainable, video_MLP_concat, video_LRCNN_frozen, video_LRCNN_trainable, 3DCNN]\n",
    "        :pretrained_model_name: name of pretrained model (or None if not using pretrained model e.g. for 3D-CNN)\n",
    "        :pooling: name of pooling variant (or None if not using pretrained model e.g. for 3D-CNN)\n",
    "        :sequence_model: sequence model variant in [LSTM, SimpleRNN, GRU, Convolution1D]\n",
    "        \"\"\"\n",
    "    \n",
    "        # required params\n",
    "        self.sequence_length = sequence_length\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # optional params\n",
    "        self.architecture = architecture\n",
    "        self.pretrained_model_name = pretrained_model_name\n",
    "        self.pooling = pooling\n",
    "        self.sequence_model = sequence_model\n",
    "        self.num_epochs = num_epochs\n",
    "        self.layer_1_size = layer_1_size\n",
    "        self.layer_2_size = layer_2_size\n",
    "        self.layer_3_size = layer_3_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # maybe read target size from pretrained model \n",
    "        if pretrained_model_name is not None:\n",
    "            self.num_features = pretrained_model_len_features[pretrained_model_name]\n",
    "        \n",
    "        self.model = None\n",
    "        \n",
    "        if architecture == \"image_MLP_frozen\":\n",
    "            \n",
    "            ####################\n",
    "            ### image_MLP_frozen\n",
    "            ####################\n",
    "            \n",
    "            # image classification (single frame)\n",
    "            # train MLP on top of weights extracted from pretrained CNN with no fine-tuning\n",
    "            \n",
    "            model = Sequential()\n",
    "            \n",
    "            if layer_1_size > 0\n",
    "            model.add(Flatten(input_shape=(self.sequence_length, self.num_features)))\n",
    "            model.add(Dense(layer_1_size, activation='relu'))\n",
    "\n",
    "            if layer_2_size > 0 and layer_1_size > 0:\n",
    "                model.add(Dense(layer_2_size, activation='relu'))\n",
    "                if dropout > 0\n",
    "                    model.add(Dropout(dropout))\n",
    "\n",
    "            if layer_2_size > 0 and layer_3_size > 0 and layer_1_size > 0:\n",
    "                model.add(Dense(layer_3_size, activation='relu'))\n",
    "                if dropout > 0:\n",
    "                    model.add(Dropout(dropout))\n",
    "                \n",
    "            # final layer\n",
    "            model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "            # define optimizer and compile model\n",
    "            opt = Adam()\n",
    "            model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        elif architecture == \"image_MLP_trainable\":\n",
    "            \n",
    "            #######################\n",
    "            ### image_MLP_trainable\n",
    "            #######################\n",
    "            \n",
    "            # image classification (single frame)\n",
    "            # fine-tune pretrained CNN and fit MLP on top\n",
    "            #\n",
    "            # later we will compare our best fine-tuned CNN as a feature extractor vs fixed CNN features\n",
    "            \n",
    "            # create the base pre-trained model\n",
    "#             base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "            base_model = load_pretrained_model(pretrained_model_name, pooling)\n",
    "            \n",
    "            ################ \n",
    "            ### TODO\n",
    "            # replace base_model with get_pretrained_model\n",
    "            \n",
    "\n",
    "            # add a global spatial average pooling layer\n",
    "            x = base_model.output\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "            # let's add a fully-connected layer\n",
    "            x = Dense(1024, activation='relu')(x)\n",
    "            # and a logistic layer -- let's say we have 200 classes\n",
    "            predictions = Dense(200, activation='softmax')(x)\n",
    "\n",
    "            # this is the model we will train\n",
    "            model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "            # first: train only the top layers (which were randomly initialized)\n",
    "            # i.e. freeze all convolutional InceptionV3 layers\n",
    "            for layer in base_model.layers:\n",
    "                layer.trainable = False\n",
    "\n",
    "            # compile the model (should be done *after* setting layers to non-trainable)\n",
    "            model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        elif architecture == \"video_MLP_concat\":\n",
    "\n",
    "            ####################\n",
    "            ### video_MLP_concat\n",
    "            ####################\n",
    "            \n",
    "            # video classification\n",
    "            # concatenate all frames in sequence and train MLP on top of concatenated frame input\n",
    "            \n",
    "        elif architecture == \"video_LRCNN_frozen\":\n",
    "\n",
    "            ######################\n",
    "            ### video_LRCNN_frozen\n",
    "            ######################\n",
    "            \n",
    "            # Implement:\n",
    "            # “Long-Term Recurrent Convolutional Networks for Visual Recognition and Description.”\n",
    "            # Donahue, Jeff, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, \n",
    "            # Sergio Guadarrama, Kate Saenko, and Trevor Darrell.  \n",
    "            # Proceedings of the IEEE Computer Society Conference on Computer Vision and \n",
    "            # Pattern Recognition, 2015, 2625–34.\n",
    "            #\n",
    "            # Essentially they extract features with fine-tuned CNN then fit recurrent models on top\n",
    "            # in the paper they only use LSTM but we will also try RNN, GRU and 1-D CNN\n",
    "            # \n",
    "            # note: no fine-tuning of CNN\n",
    "            \n",
    "            if self.sequence_model == \"LSTM\"\n",
    "                print(\"TODO\")\n",
    "            elif self.sequence_model == \"SimpleRNN\"\n",
    "                print(\"TODO\")\n",
    "            elif self.sequence_model == \"GRU\"\n",
    "                print(\"TODO\")\n",
    "            elif self.sequence_model == \"Convolution1D\"\n",
    "                print(\"TODO\")\n",
    "            else:\n",
    "                raise NameError('Invalid sequence_model - must be one of [LSTM, SimpleRNN, GRU, Convolution1D]')    \n",
    "\n",
    "        elif architecture == \"video_LRCNN_trainable\":\n",
    "            \n",
    "            #########################\n",
    "            ### video_LRCNN_trainable\n",
    "            #########################\n",
    "            \n",
    "            # Same as above:\n",
    "            # “Long-Term Recurrent Convolutional Networks for Visual Recognition and Description.”\n",
    "            # Donahue, Jeff, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, \n",
    "            # Sergio Guadarrama, Kate Saenko, and Trevor Darrell.  \n",
    "            # Proceedings of the IEEE Computer Society Conference on Computer Vision and \n",
    "            # Pattern Recognition, 2015, 2625–34.\n",
    "            #\n",
    "            # But with fine-tuning of the CNNs that are input into the recurrent models\n",
    "            # \n",
    "            # note: will take long because not precomputing the CNN part so re-computed \n",
    "            # on each training pass\n",
    "            \n",
    "        elif architecture == \"3DCNN\":\n",
    "            \n",
    "            #########\n",
    "            ### 3DCNN\n",
    "            #########\n",
    "            \n",
    "            # Implement:\n",
    "            \n",
    "            # “3D Convolutional Neural Networks for Human Action Recognition.” \n",
    "            # Ji, Shuiwang, Wei Xu, Ming Yang, and Kai Yu. \n",
    "            # IEEE Transactions on Pattern Analysis and Machine Intelligence \n",
    "            # 35, no. 1 (2013): 221–31. doi:10.1109/TPAMI.2012.59.\n",
    "            #\n",
    "            # They fit a 3-D convolutional model on top of stacked frame volumes\n",
    "            \n",
    "        else:\n",
    "            raise NameError('Invalid architecture - must be one of [image_MLP_frozen, image_MLP_trainable, video_MLP_concat, video_LRCNN_frozen, video_LRCNN_trainable, 3DCNN]')    \n",
    "        \n",
    "        # set class model to constructed model\n",
    "        self.model = model\n",
    "        \n",
    "\n",
    "    def update_learning_rate(multiplier = 0.1):\n",
    "        \"\"\"Update learning rate by multiplier\"\"\"\n",
    "        K.set_value(model.optimizer.lr, multiplier * model.optimizer.lr.get_value())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
