{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO\n",
    "# * check base model outputs without pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T00:25:51.405086Z",
     "start_time": "2019-01-06T00:25:51.401182Z"
    }
   },
   "outputs": [],
   "source": [
    "# whether to log each feature and sequence status\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T00:25:55.284991Z",
     "start_time": "2019-01-06T00:25:51.780848Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from keras.layers import Dense, Flatten, Dropout, ZeroPadding3D\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling3D, Conv3D, MaxPooling2D, Convolution1D, Convolution3D, MaxPooling3D, ZeroPadding3D\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T00:25:56.922122Z",
     "start_time": "2019-01-06T00:25:56.918509Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup paths\n",
    "pwd = os.getcwd().replace(\"deepvideoclassification\",\"\")\n",
    "path_cache = pwd + 'cache/'\n",
    "path_data = pwd + 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T00:25:57.303543Z",
     "start_time": "2019-01-06T00:25:57.299009Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"{0}/{1}.log\".format(pwd, \"logs\")),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained_CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T00:26:01.046325Z",
     "start_time": "2019-01-06T00:26:01.042179Z"
    }
   },
   "outputs": [],
   "source": [
    "# # define pretrained model shapes\n",
    "# pretrained_model_len_features = {}\n",
    "# #\n",
    "# pretrained_model_len_features['vgg16'] = 512\n",
    "# pretrained_model_len_features['resnet50'] = 2048\n",
    "# pretrained_model_len_features['mobilenetv2_1.00_224'] = 1280\n",
    "# #\n",
    "# pretrained_model_len_features['xception'] = 9999999999\n",
    "# pretrained_model_len_features['inception_v3'] = 9999999999\n",
    "# pretrained_model_len_features['inception_resnet_v2'] = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pretrained model shapes\n",
    "pretrained_model_sizes = {}\n",
    "#\n",
    "pretrained_model_sizes['vgg16'] = (224,224)\n",
    "pretrained_model_sizes['resnet50'] = (224,224)\n",
    "pretrained_model_sizes['mobilenetv2_1.00_224'] = (224,224)\n",
    "#\n",
    "pretrained_model_sizes['xception'] = (299,299)\n",
    "pretrained_model_sizes['inception_v3'] = (299,299)\n",
    "pretrained_model_sizes['inception_resnet_v2'] = (299,299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_names = [\"inception_resnet_v2\", \"inception_v3\", \"mobilenetv2_1.00_224\", \"resnet50\", \"vgg16\", \"xception\"]\n",
    "poolings = ['max','avg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> insert precompute stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image/video classification model object with various architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T12:39:32.916991Z",
     "start_time": "2019-01-06T12:39:32.884395Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-18c94b94816f>, line 64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-18c94b94816f>\"\u001b[0;36m, line \u001b[0;32m64\u001b[0m\n\u001b[0;31m    if layer_1_size > 0\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, sequence_length, num_classes, target_size, \n",
    "                architecture, pretrained_model_name, pooling = None,\n",
    "                layer_1_sequence_model = None, layer_2_sequence_model = None, layer_3_sequence_model = None,\n",
    "                layer_1_size = 0, layer_2_size = 0, layer_3_size = 0, \n",
    "                dropout = 0):\n",
    "        \"\"\"\n",
    "        Model object constructor\n",
    "        \n",
    "        :sequence_length: number of frames in sequence to be returned by Data object\n",
    "        :num_classes: number of classes to predict\n",
    "        :target_size: size that frames are resized to (different models / architectures accept different input sizes)\n",
    "\n",
    "        :architecture: architecture of model in [image_MLP_frozen, image_MLP_trainable, video_MLP_concat, video_LRCNN_frozen, video_LRCNN_trainable, 3DCNN]\n",
    "        :pretrained_model_name: name of pretrained model (or None if not using pretrained model e.g. for 3D-CNN)\n",
    "        :pooling: name of pooling variant (or None if not using pretrained model e.g. for 3D-CNN or if fitting more non-dense layers on top of pretrained model base)\n",
    "        \n",
    "        :layer_1_sequence_model: sequence model for layer 1 in [LSTM, SimpleRNN, GRU, Convolution1D]\n",
    "        :layer_2_sequence_model: sequence model for layer 2 in [LSTM, SimpleRNN, GRU, Convolution1D]\n",
    "        :layer_3_sequence_model: sequence model for layer 3 in [LSTM, SimpleRNN, GRU, Convolution1D]\n",
    "        \n",
    "        :layer_1_size: number of neurons in layer 1\n",
    "        :layer_2_size: number of neurons in layer 2\n",
    "        :layer_3_size: number of neurons in layer 3\n",
    "        \n",
    "        :dropout: amount of dropout to add (same amount throughout)\n",
    "        \"\"\"\n",
    "    \n",
    "        # required params\n",
    "        self.sequence_length = sequence_length\n",
    "        self.target_size = target_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # model architecture params\n",
    "        self.architecture = architecture\n",
    "        self.pretrained_model_name = pretrained_model_name\n",
    "        self.pooling = pooling\n",
    "        self.sequence_model = sequence_model\n",
    "        #\n",
    "        self.layer_1_size = layer_1_size\n",
    "        self.layer_2_size = layer_2_size\n",
    "        self.layer_3_size = layer_3_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # maybe read target size from pretrained model \n",
    "        if pretrained_model_name is not None:\n",
    "            self.num_features = pretrained_model_len_features[pretrained_model_name]\n",
    "        \n",
    "        self.model = None\n",
    "        \n",
    "        if architecture == \"image_MLP_frozen\":\n",
    "            \n",
    "            ####################\n",
    "            ### image_MLP_frozen\n",
    "            ####################\n",
    "            \n",
    "            # image classification (single frame)\n",
    "            # train MLP on top of weights extracted from pretrained CNN with no fine-tuning\n",
    "            \n",
    "            model = Sequential()\n",
    "            \n",
    "            # 1st layer group\n",
    "            if layer_1_size > 0:\n",
    "                model.add(Flatten(input_shape=(self.sequence_length, self.num_features)))\n",
    "                model.add(Dense(layer_1_size, activation='relu'))\n",
    "            else:\n",
    "                # flatten input\n",
    "                model.add(Flatten(input_shape=(self.sequence_length, self.num_features)))\n",
    "\n",
    "            if layer_2_size > 0 and layer_1_size > 0:\n",
    "                model.add(Dense(layer_2_size, activation='relu'))\n",
    "                if dropout > 0\n",
    "                    model.add(Dropout(dropout))\n",
    "\n",
    "            if layer_2_size > 0 and layer_3_size > 0 and layer_1_size > 0:\n",
    "                model.add(Dense(layer_3_size, activation='relu'))\n",
    "                if dropout > 0:\n",
    "                    model.add(Dropout(dropout))\n",
    "                \n",
    "            # final layer\n",
    "            model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        elif architecture == \"image_MLP_trainable\":\n",
    "            \n",
    "            #######################\n",
    "            ### image_MLP_trainable\n",
    "            #######################\n",
    "            \n",
    "            # image classification (single frame)\n",
    "            # fine-tune pretrained CNN and fit MLP on top\n",
    "            #\n",
    "            # later we will compare our best fine-tuned CNN as a feature extractor vs fixed CNN features\n",
    "            \n",
    "            # create the base pre-trained model\n",
    "#             base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "            base_model = load_pretrained_model(pretrained_model_name, pooling)\n",
    "            \n",
    "        \n",
    "        \n",
    "            ###### TODO\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            # add a global spatial average pooling layer\n",
    "            x = base_model.output\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "            # let's add a fully-connected layer\n",
    "            x = Dense(1024, activation='relu')(x)\n",
    "            # and a logistic layer -- let's say we have 200 classes\n",
    "            predictions = Dense(200, activation='softmax')(x)\n",
    "\n",
    "            # this is the model we will train\n",
    "            model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "            # first: train only the top layers (which were randomly initialized)\n",
    "            # i.e. freeze all convolutional InceptionV3 layers\n",
    "            for layer in base_model.layers:\n",
    "                layer.trainable = False\n",
    "\n",
    "            \n",
    "            \n",
    "        elif architecture == \"video_MLP_concat\":\n",
    "\n",
    "            ####################\n",
    "            ### video_MLP_concat\n",
    "            ####################\n",
    "            \n",
    "            # video classification\n",
    "            # concatenate all frames in sequence and train MLP on top of concatenated frame input\n",
    "            \n",
    "        elif architecture == \"video_LRCNN_frozen\":\n",
    "\n",
    "            ######################\n",
    "            ### video_LRCNN_frozen\n",
    "            ######################\n",
    "            \n",
    "            # Implement:\n",
    "            # “Long-Term Recurrent Convolutional Networks for Visual Recognition and Description.”\n",
    "            # Donahue, Jeff, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, \n",
    "            # Sergio Guadarrama, Kate Saenko, and Trevor Darrell.  \n",
    "            # Proceedings of the IEEE Computer Society Conference on Computer Vision and \n",
    "            # Pattern Recognition, 2015, 2625–34.\n",
    "            #\n",
    "            # Essentially they extract features with fine-tuned CNN then fit recurrent models on top\n",
    "            # in the paper they only use LSTM but we will also try RNN, GRU and 1-D CNN\n",
    "            # \n",
    "            # note: no fine-tuning of CNN\n",
    "            \n",
    "            if self.sequence_model == \"LSTM\"\n",
    "                print(\"TODO\")\n",
    "            elif self.sequence_model == \"SimpleRNN\"\n",
    "                print(\"TODO\")\n",
    "            elif self.sequence_model == \"GRU\"\n",
    "                print(\"TODO\")\n",
    "            elif self.sequence_model == \"Convolution1D\"\n",
    "                print(\"TODO\")\n",
    "            else:\n",
    "                raise NameError('Invalid sequence_model - must be one of [LSTM, SimpleRNN, GRU, Convolution1D]')    \n",
    "\n",
    "        elif architecture == \"video_LRCNN_trainable\":\n",
    "            \n",
    "            #########################\n",
    "            ### video_LRCNN_trainable\n",
    "            #########################\n",
    "            \n",
    "            # Same as above:\n",
    "            # “Long-Term Recurrent Convolutional Networks for Visual Recognition and Description.”\n",
    "            # Donahue, Jeff, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, \n",
    "            # Sergio Guadarrama, Kate Saenko, and Trevor Darrell.  \n",
    "            # Proceedings of the IEEE Computer Society Conference on Computer Vision and \n",
    "            # Pattern Recognition, 2015, 2625–34.\n",
    "            #\n",
    "            # But with fine-tuning of the CNNs that are input into the recurrent models\n",
    "            # \n",
    "            # note: will take long because not precomputing the CNN part so re-computed \n",
    "            # on each training pass\n",
    "            \n",
    "        elif architecture == \"3DCNN\":\n",
    "            \n",
    "            #########\n",
    "            ### 3DCNN\n",
    "            #########\n",
    "            \n",
    "            # Implement:\n",
    "            \n",
    "            # “3D Convolutional Neural Networks for Human Action Recognition.” \n",
    "            # Ji, Shuiwang, Wei Xu, Ming Yang, and Kai Yu. \n",
    "            # IEEE Transactions on Pattern Analysis and Machine Intelligence \n",
    "            # 35, no. 1 (2013): 221–31. doi:10.1109/TPAMI.2012.59.\n",
    "            #\n",
    "            # They fit a 3-D convolutional model on top of stacked frame volumes\n",
    "            \n",
    "            # Implementation from: \n",
    "            # https://gist.github.com/albertomontesg/d8b21a179c1e6cca0480ebdf292c34d2\n",
    "            \n",
    "\n",
    "            model = Sequential()\n",
    "            \n",
    "            # 1st layer group\n",
    "            model.add(Convolution3D(64, 3, 3, 3, activation='relu',  border_mode='same', name='conv1', subsample=(1, 1, 1), input_shape=(3, 16, 112, 112)))\n",
    "            model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), border_mode='valid', name='pool1'))\n",
    "\n",
    "            # 2nd layer group\n",
    "            model.add(Convolution3D(128, 3, 3, 3, activation='relu',border_mode='same', name='conv2', subsample=(1, 1, 1)))\n",
    "            model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2),  border_mode='valid', name='pool2'))\n",
    "\n",
    "            # 3rd layer group\n",
    "            model.add(Convolution3D(256, 3, 3, 3, activation='relu',border_mode='same', name='conv3a', subsample=(1, 1, 1)))\n",
    "            model.add(Convolution3D(256, 3, 3, 3, activation='relu',  border_mode='same', name='conv3b', subsample=(1, 1, 1)))\n",
    "            model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='valid', name='pool3'))\n",
    "\n",
    "            # 4th layer group\n",
    "            model.add(Convolution3D(512, 3, 3, 3, activation='relu',  border_mode='same', name='conv4a', subsample=(1, 1, 1)))\n",
    "            model.add(Convolution3D(512, 3, 3, 3, activation='relu',  border_mode='same', name='conv4b', subsample=(1, 1, 1)))\n",
    "            model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='valid', name='pool4'))\n",
    "\n",
    "            # 5th layer group\n",
    "            model.add(Convolution3D(512, 3, 3, 3, activation='relu',border_mode='same', name='conv5a', subsample=(1, 1, 1)))\n",
    "            model.add(Convolution3D(512, 3, 3, 3, activation='relu', border_mode='same', name='conv5b', subsample=(1, 1, 1)))\n",
    "            model.add(ZeroPadding3D(padding=(0, 1, 1)))\n",
    "            model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='valid', name='pool5'))\n",
    "            model.add(Flatten())\n",
    "            \n",
    "            # FC layers group\n",
    "            model.add(Dense(4096, activation='relu', name='fc6'))\n",
    "            model.add(Dropout(.5))\n",
    "            model.add(Dense(4096, activation='relu', name='fc7'))\n",
    "            model.add(Dropout(.5))\n",
    "            model.add(Dense(487, activation='softmax', name='fc8'))\n",
    "            \n",
    "        else:\n",
    "            raise NameError('Invalid architecture - must be one of [image_MLP_frozen, image_MLP_trainable, video_MLP_concat, video_LRCNN_frozen, video_LRCNN_trainable, 3DCNN]')    \n",
    "        \n",
    "        # set class model to constructed model\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move to experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define optimizer and compile model\n",
    "# # (compiling the model should be done *after* setting layers to non-trainable)\n",
    "# opt = Adam()\n",
    "# model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T00:26:37.503044Z",
     "start_time": "2019-01-06T00:26:37.476870Z"
    }
   },
   "outputs": [],
   "source": [
    "# def update_learning_rate(multiplier = 0.1):\n",
    "#     \"\"\"Update learning rate by multiplier\"\"\"\n",
    "#     K.set_value(model.optimizer.lr, multiplier * model.optimizer.lr.get_value())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
