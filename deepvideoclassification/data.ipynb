{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T14:57:35.983159Z",
     "start_time": "2019-01-14T14:57:35.979773Z"
    }
   },
   "outputs": [],
   "source": [
    "# whether to log each feature and sequence status\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T14:57:38.225836Z",
     "start_time": "2019-01-14T14:57:36.140277Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import h5py\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T14:57:39.535695Z",
     "start_time": "2019-01-14T14:57:38.228159Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T14:57:39.548287Z",
     "start_time": "2019-01-14T14:57:39.538398Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pretrained model functions\n",
    "from deepvideoclassification.models import precompute_CNN_features\n",
    "from deepvideoclassification.models import load_pretrained_model_preprocessor\n",
    "from deepvideoclassification.models import load_pretrained_model\n",
    "\n",
    "# import pretrained model properties\n",
    "from deepvideoclassification.models import pretrained_model_len_features\n",
    "from deepvideoclassification.models import pretrained_model_sizes\n",
    "from deepvideoclassification.models import pretrained_model_names, poolings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T14:57:39.554829Z",
     "start_time": "2019-01-14T14:57:39.551045Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup paths\n",
    "pwd = os.getcwd().replace(\"deepvideoclassification\",\"\")\n",
    "path_cache = pwd + 'cache/'\n",
    "path_data = pwd + 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T14:57:39.562972Z",
     "start_time": "2019-01-14T14:57:39.557227Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup logging\n",
    "# any explicit log messages or uncaught errors to stdout and file /logs.log\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"{0}/{1}.log\".format(pwd, \"logs\")),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "# init logger\n",
    "logger = logging.getLogger()\n",
    "# make logger aware of any uncaught exceptions\n",
    "def handle_exception(exc_type, exc_value, exc_traceback):\n",
    "    if issubclass(exc_type, KeyboardInterrupt):\n",
    "        sys.__excepthook__(exc_type, exc_value, exc_traceback)\n",
    "        return\n",
    "\n",
    "    logger.error(\"Uncaught exception\", exc_info=(exc_type, exc_value, exc_traceback))\n",
    "sys.excepthook = handle_exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T14:57:39.570661Z",
     "start_time": "2019-01-14T14:57:39.565428Z"
    }
   },
   "outputs": [],
   "source": [
    "# read vid folders\n",
    "def get_video_paths():\n",
    "    \"\"\"\n",
    "    Return list of video paths \n",
    "\n",
    "    Videos should be in /data/video_1/, /data/video_2/ style folders \n",
    "    with sequentially numbered frame images e.g. /data/video_1/frame00001.jpg\n",
    "\n",
    "    There should be at least 3 videos, 1 for each of train/test/valid splits\n",
    "    Split assignment is given in /data/labels.csv (also to be provided by user)\n",
    "\n",
    "    Functionality to use different parts of a video as train/valid/test \n",
    "    not currently implemented.\n",
    "    \"\"\"\n",
    "    path_videos = []\n",
    "    for filename in os.listdir(path_data):\n",
    "        if os.path.isdir(os.path.join(path_data, filename)):\n",
    "            path_videos.append(filename)\n",
    "\n",
    "    path_videos = [path_data + v + '/' for v in path_videos]\n",
    "\n",
    "    # make sure that there is video data in /data/ and give instructions if not done correctly\n",
    "    assert len(path_videos)>0, \"There need to be at least 3 video folders (at least 1 for each of train, valid, \\\n",
    "    and test splits) in /data/ - each video should be its own folder of frame images with ascending time-ordered \\\n",
    "    filenames e.g. /data/vid1/frame00001.jpg ... videos assignment to train/valid/test split should be given in \\\n",
    "    /data/labels.csv ... cross-validation or train/valid/test splits within a single long video not currently implemented\"\n",
    "\n",
    "    return path_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T14:57:39.581698Z",
     "start_time": "2019-01-14T14:57:39.573524Z"
    }
   },
   "outputs": [],
   "source": [
    "def resize_frames(target_size):\n",
    "    \"\"\"\n",
    "    Resize the frames of all videos and save them to /cache/ \n",
    "    to make model fitting faster .\n",
    "\n",
    "    We resize once upfront rather than each we use a pretrained model or architecture.\n",
    "\n",
    "    Our models require inputs resized to:\n",
    "    * 224 x 224 VGG16, ResNet50, DenseNet, MobileNet\n",
    "    * 299 x 299 XCeption, InceptionV3, InceptionResNetV2\n",
    "    * 112 x 112 3D CNN \n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(path_cache + 'frames/' + str(target_size[0]) + \"_\" + str(target_size[1]) + '/'):\n",
    "        \n",
    "        os.makedirs(path_cache + 'frames/' + str(target_size[0]) + \"_\" + str(target_size[1]) + '/')\n",
    "\n",
    "        # read vid paths\n",
    "        path_videos = get_video_paths()\n",
    "\n",
    "        # loop over all vids and resize frames, saving to new folder in /cache/frames/\n",
    "        for c,path_video in enumerate(path_videos):\n",
    "\n",
    "            logger.info(\"resizing vid {}/{} to {}x{}\".format(c+1,len(path_videos),target_size[0], target_size[1]))\n",
    "\n",
    "            # get vid name from path\n",
    "            video_name = path_video.split(\"/\")[-2]\n",
    "\n",
    "            # create directory for resized frames - just storing arrays now so commented out\n",
    "            # e.g. path_vid_resized = /cache/frames/224_224/s23-4847/\n",
    "            # path_vid_resized = path_cache + 'frames/'\n",
    "            # path_vid_resized += str(target_size[0]) + \"_\" + str(target_size[1]) + '/' \n",
    "            # path_vid_resized += video_name + '/'\n",
    "\n",
    "            # load frame paths for vid\n",
    "            path_frames = os.listdir(path_video)\n",
    "            path_frames = [path_video + f for f in path_frames if f != '.DS_Store']\n",
    "            path_frames.sort()\n",
    "\n",
    "            # load frames\n",
    "            frames = []\n",
    "            for path_frame in path_frames:\n",
    "\n",
    "                # open image and resize\n",
    "                filename = path_frame.split(\"/\").pop()\n",
    "                img_frame = Image.open(path_frame)\n",
    "                img_frame = img_frame.resize(target_size)\n",
    "                # img_frame.save(path_vid_resized + filename, \"JPEG\", quality = 100)\n",
    "\n",
    "                # convert to array and append to list\n",
    "                img_frame = np.array(img_frame)\n",
    "                frames.append(img_frame)\n",
    "\n",
    "            # save array of resized frames\n",
    "            np.save(path_cache + \"frames/\" + str(target_size[0]) + \"_\" + str(target_size[1]) + \"/\" + video_name, np.array(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T14:57:39.588989Z",
     "start_time": "2019-01-14T14:57:39.584328Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_labels():\n",
    "    # read labels - should be CSV with columns \"video\",\"frame\",\"label\",\"split\"\n",
    "    # e.g. \"s1-218\", \"s1-218-00001.jpeg\", \"noseal\", \"train\"\n",
    "    labels = None\n",
    "    try:\n",
    "        labels = pd.read_csv(path_data + 'labels.csv', usecols=['video','frame','label','split'])\n",
    "    except ValueError as e:\n",
    "        raise Exception(\"Labels file must contain columns ['video','frame','label','split'] - if you only have ['video','frame','label'], use Jupyter notebook in notebooks/add_splits_to_labels_file.ipynb to add train/valid/test splits to your labels file\")\n",
    "    except FileNotFoundError as e:\n",
    "        raise Exception(\"No labels found - please save labels file to /data/labels.csv\") from e\n",
    "\n",
    "    return labels.sort_values([\"video\",\"frame\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T14:57:39.597868Z",
     "start_time": "2019-01-14T14:57:39.591550Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_video_label_arrays():\n",
    "    \"\"\"\n",
    "    Create numpy array with labels for each vid and a label_map.json file\n",
    "    in /cache/labels/\n",
    "    \"\"\"\n",
    "\n",
    "    # create folder for labels\n",
    "    if not os.path.exists(path_cache + 'labels/'):\n",
    "        os.makedirs(path_cache + 'labels/')\n",
    "\n",
    "    # load labels\n",
    "    labels = get_labels()\n",
    "\n",
    "    # build label_map\n",
    "    label_dummies = pd.get_dummies(labels, columns = ['label'])\n",
    "\n",
    "    # get label columns list and build label map dict\n",
    "    label_columns = []\n",
    "    label_map = {}\n",
    "    label_map_idx = 0\n",
    "    for i, col in enumerate(label_dummies.columns):\n",
    "        if col[:6] == 'label_':\n",
    "            label_columns.append(col)\n",
    "            label_map[label_map_idx] = col\n",
    "            label_map_idx+=1\n",
    "\n",
    "    # save label map to json\n",
    "    with open(path_cache + 'labels/label_map.json', 'w') as fp:\n",
    "        json.dump(label_map, fp)\n",
    "\n",
    "    # get video paths\n",
    "    path_videos = get_video_paths()\n",
    "\n",
    "    # save numpy array of labels for each vid\n",
    "    for path_video in path_videos:\n",
    "\n",
    "        # get vid name from path\n",
    "        video_name = path_video.split(\"/\")[-2]\n",
    "\n",
    "        vid_labels = np.array(label_dummies[label_dummies['video'] == video_name][label_columns])\n",
    "\n",
    "        # save labels array for this vid\n",
    "        np.save(path_cache + \"/labels/\" + video_name, np.array(vid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T14:57:39.605739Z",
     "start_time": "2019-01-14T14:57:39.600706Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_label_map():\n",
    "    \"\"\"\n",
    "    Returns label map - read from disk\n",
    "    \"\"\"\n",
    "\n",
    "    # load label map from disk\n",
    "    label_map = None\n",
    "    try:\n",
    "        if os.path.exists(path_cache + 'labels/label_map.json'):\n",
    "            with open(path_cache + 'labels/label_map.json', 'r') as fp:\n",
    "                label_map = json.load(fp)\n",
    "        else:\n",
    "            # build labels and label map\n",
    "            create_video_label_arrays()\n",
    "            if os.path.exists(path_cache + 'labels/label_map.json'):\n",
    "                with open(path_cache + 'labels/label_map.json', 'r') as fp:\n",
    "                    label_map = json.load(fp)\n",
    "    except Exception as e:\n",
    "        logger.error ('label map not found - make sure /data/labels.csv exists and data cache has been built')\n",
    "\n",
    "    return label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T14:57:39.615618Z",
     "start_time": "2019-01-14T14:57:39.608349Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Generator (used in Data) that generates data for Keras fit_generator method because full dataset too big to load into memory\n",
    "    \n",
    "    > inherits from keras.utils.Sequence\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, h5_path, h5_row_count):\n",
    "        \"\"\"\n",
    "        Initialization DataGenerator class\n",
    "        \n",
    "        :batch_size: number of samples to return in batch \n",
    "        :h5_path: path to h5 dataset (where we stored the generated sequence data via save_frame_sequences_to_h5())\n",
    "        :h5_row_count: number of rows in h5 dataset\n",
    "        \n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.h5_path = h5_path\n",
    "        self.h5_row_count = h5_row_count\n",
    "        \n",
    "        # init (shuffle dataset)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.h5_row_count / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        \n",
    "        # Generate indexes of the batch\n",
    "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        x, y = self.__data_generation(batch_indexes)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.h5_row_count)\n",
    "        \n",
    "        # shuffle indexes -> shuffle samples returned in each batch\n",
    "        np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, batch_indexes):\n",
    "        \"\"\"\n",
    "        Generates data containing batch_size samples\n",
    "        \n",
    "        :batch_indexes: list (of size batch_size) with indexes into h5 file\n",
    "        \"\"\" \n",
    "        x, y = None, None\n",
    "\n",
    "        # slices into h5 file need to be sorted\n",
    "        batch_indexes.sort()\n",
    "\n",
    "        # read sample from h5 file\n",
    "        with h5py.File(self.h5_path, 'r') as h5:\n",
    "            ### read sample indexes from h5 file\n",
    "            # sample sequences\n",
    "            x = h5['sequences'][batch_indexes,:]\n",
    "            # sample labels\n",
    "            y = h5['labels'][batch_indexes,:]\n",
    "\n",
    "        return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T16:22:22.121344Z",
     "start_time": "2019-01-14T16:22:22.059899Z"
    }
   },
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    \n",
    "    def __init__(self, sequence_length, \n",
    "                    return_CNN_features = False, pretrained_model_name = None, pooling = None, \n",
    "                    frame_size = None, augmentation = False, oversampling = False,\n",
    "                    model_weights_path = None, custom_model_name = None,\n",
    "                    return_generator = False, batch_size = None,\n",
    "                    verbose = True):\n",
    "        \"\"\"\n",
    "        Data object constructor\n",
    "        \n",
    "        \n",
    "        :sequence_length: number of frames in sequence to be returned by Data object\n",
    "        :return_CNN_features: whether to return precomputed features or return frames (or sequences of features/frames if sequence_length>1)\n",
    "\n",
    "        :return_features: if True then return features (or sequences of feature) from pretrained model, if False then return frames (or sequences of frames)        \n",
    "        :pretrained_model_name: name of pretrained model (or None if not using pretrained model e.g. for 3D-CNN)\n",
    "        :pooling: name of pooling variant (or None if not using pretrained model e.g. for 3D-CNN)\n",
    "        :frame_size: size that frames are resized to (this is looked up for pretrained models)\n",
    "        :augmentation: whether to apply data augmentation (horizontal flips)\n",
    "        :oversampling: whether to apply oversampling to create class balance\n",
    "        \n",
    "        :model_weights_path: path to custom model weights if we want to load CNN model we've fine-tuned to produce features (e.g. for LRCNN)\n",
    "        :custom_model_name: custom output name to append to pretrained model name\n",
    "        \n",
    "        :return_generator: if True and sequence_length > 1 and return_CNN_features == False, then do not return dataset, instead construct h5 file with sequences for each split and return generator that samples from that (dataset of sequecne frames too big to load into memory)\n",
    "        :batch_size: size of batches that generator must return\n",
    "        \n",
    "        :verbose: whether to log details\n",
    "        \n",
    "        Notes: \n",
    "        * if pretrained_model_name != None and return_CNN_features=False then will first apply preprocessor to frames (or frame sequences)\n",
    "        * if return_generator = True and sequence_length > 1 and return_CNN_features == False, large h5 files will be created in cache before returning generator\n",
    "        \"\"\"\n",
    "    \n",
    "        # required params\n",
    "        self.sequence_length = sequence_length\n",
    "        self.frame_size = frame_size\n",
    "        \n",
    "        # optional params\n",
    "        self.pretrained_model_name = pretrained_model_name\n",
    "        self.pooling = pooling\n",
    "        self.return_CNN_features = return_CNN_features\n",
    "        self.augmentation = augmentation\n",
    "        self.oversampling = oversampling\n",
    "        \n",
    "        # init model data\n",
    "        self.x_train = []\n",
    "        self.y_train = []\n",
    "        #\n",
    "        self.x_valid = []\n",
    "        self.y_valid = []\n",
    "        # \n",
    "        self.x_test = []\n",
    "        self.y_test = []\n",
    "        \n",
    "        # fix case sensitivity\n",
    "        if type(self.pretrained_model_name) == str:\n",
    "            self.pretrained_model_name = self.pretrained_model_name.lower()\n",
    "        if type(self.pooling) == str:\n",
    "            self.pooling = self.pooling.lower()\n",
    "        \n",
    "        ################\n",
    "        ### Prepare data\n",
    "        ################\n",
    "        \n",
    "        # get video paths\n",
    "        self.path_videos = get_video_paths()\n",
    "        \n",
    "        # create label array for each video and load label map\n",
    "        create_video_label_arrays()\n",
    "        self.label_map = load_label_map()\n",
    "        \n",
    "        # get labels\n",
    "        self.labels = get_labels()\n",
    "        \n",
    "        # check that there is 1 frame file for each label file and raise error if they don't match\n",
    "        paths_frames = []\n",
    "        for folder, subs, files in os.walk(path_data):        \n",
    "            for filename in files:\n",
    "                if filename[-4:].lower() == '.jpg' or filename[-4:].lower() == 'jpeg' or filename[-4:].lower() == 'png':\n",
    "                    paths_frames.append(os.path.abspath(os.path.join(folder, filename)))\n",
    "        if len(paths_frames) != len(self.labels):\n",
    "            error_msg = 'IMPORTANT ERROR: Number of frames ({}) in /data/ video folders needs to match number of labels ({}) in labels.csv - use notebooks/helper_check_frames_against_labels.ipynb to investigate... Note, only labels.csv and the frames you want to use (in video subfolders) should be in /data/'.format(len(paths_frames), len(self.labels))\n",
    "            logger.info(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "        \n",
    "        # pull number of classes from labels shape\n",
    "        self.num_classes = self.labels['label'].nunique()\n",
    "        \n",
    "        # create dict mapping video to train/valid/test split assignment\n",
    "        video_splits = self.labels[['video','split']].drop_duplicates()\n",
    "        video_splits.set_index(\"video\", inplace=True)\n",
    "        video_splits = video_splits.to_dict()['split']\n",
    "        self.video_splits = video_splits\n",
    "        \n",
    "        # look up target size for pretrained model\n",
    "        if pretrained_model_name is not None:\n",
    "            self.frame_size = pretrained_model_sizes[pretrained_model_name]\n",
    "        \n",
    "        # precompute resized frames (won't recompute if already resized)\n",
    "        resize_frames(self.frame_size)\n",
    "\n",
    "        # pre compute CNN features (won't recompute if already computed)\n",
    "        if return_CNN_features and pretrained_model_name is not None:\n",
    "            # check if pass custom weights to precompute from\n",
    "            if model_weights_path is not None and custom_model_name is not None:\n",
    "                # precompute with custom weights input and name\n",
    "                precompute_CNN_features(self.pretrained_model_name, self.pooling, self.model_weights_path, self.custom_model_name)\n",
    "            else:\n",
    "                precompute_CNN_features(self.pretrained_model_name, self.pooling)\n",
    "            \n",
    "            \n",
    "            \n",
    "        # get preprocessor given pretrained if we will need to apply preprocessor \n",
    "        # (i.e. if return_CNN_features == False and pretrained_model_name != None)\n",
    "        if not return_CNN_features and pretrained_model_name is not None:\n",
    "            self.preprocess_input = load_pretrained_model_preprocessor(self.pretrained_model_name)\n",
    "        \n",
    "        \n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.return_generator = return_generator\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # do some checks\n",
    "        if self.return_generator:\n",
    "            assert self.batch_size != None, \"batch size required to construct generator\"\n",
    "        if self.return_generator:\n",
    "            assert self.return_CNN_features == False, \"generator only implemented for frame sequences - features usually large enough to load into memory [may take a few minutes]\"\n",
    "        \n",
    "        ###################################\n",
    "        ### load features / build sequences\n",
    "        ###################################\n",
    "        \n",
    "        \n",
    "        # load features/frames from all videos and concat into big array for each of train, valid and test\n",
    "        if self.sequence_length > 1:\n",
    "            \n",
    "            ### sequences\n",
    "            \n",
    "            if return_CNN_features:\n",
    "                \n",
    "                if verbose:\n",
    "                    logging.info(\"Loading features sequence data into memory [may take a few minutes]\")\n",
    "                \n",
    "                #####################\n",
    "                ### feature sequences\n",
    "                #####################\n",
    "                \n",
    "                path_features = path_cache + 'features/' + pretrained_model_name + \"/\" + pooling + '/'\n",
    "                if not return_CNN_features and pretrained_model_name is not None:\n",
    "                    path_features = path_cache + 'features/' + pretrained_model_name + \"__\" + custom_model_name + \"/\" + pooling + '/'\n",
    "                path_labels = path_cache + 'labels/'\n",
    "                \n",
    "                # read vid paths\n",
    "                path_videos = get_video_paths()\n",
    "\n",
    "                # loop over all vids and load precomputed features into memory as sequences\n",
    "                for c, path_video in enumerate(path_videos):\n",
    "\n",
    "                    if verbose:\n",
    "                        logging.info(\"Loading features sequence data into memory {}/{}\".format(c+1,len(path_videos)))\n",
    "                    \n",
    "                    # get vid name from path\n",
    "                    video_name = path_video.split(\"/\")[-2]\n",
    "                    \n",
    "                    ### create sequence: features\n",
    "                    # load precomputed features\n",
    "                    features = np.load(path_features + video_name + '.npy')\n",
    "                    # build sequences\n",
    "                    x = []\n",
    "                    for i in range(sequence_length, len(features) + 1):\n",
    "                        x.append(features[i-sequence_length:i])\n",
    "                    x = np.array(x)\n",
    "                    \n",
    "                    \n",
    "                    ### create sequence: labels\n",
    "                    # load precomputed labels\n",
    "                    labels = np.load(path_labels + video_name + '.npy')     \n",
    "\n",
    "                    # temp lists to store sequences\n",
    "                    y = []\n",
    "                    for i in range(sequence_length, len(self.labels) + 1):\n",
    "                        y.append(labels[i-1])\n",
    "                    y = (np.array(y))\n",
    "                    \n",
    "                    ### build output\n",
    "                    if self.video_splits[video_name] == \"train\":\n",
    "                        self.x_train.append(x)\n",
    "                        self.y_train.append(y)\n",
    "                    if self.video_splits[video_name] == \"valid\":\n",
    "                        self.x_valid.append(x)\n",
    "                        self.y_valid.append(y)\n",
    "                    if self.video_splits[video_name] == \"test\":\n",
    "                        self.x_test.append(x)\n",
    "                        self.y_test.append(y)\n",
    "                        \n",
    "            else:\n",
    "\n",
    "                ###################\n",
    "                ### frame sequences\n",
    "                ###################\n",
    "                \n",
    "                # load full frame sequecne dataset into memory and return\n",
    "                if not return_generator:\n",
    "                    \n",
    "                    ##############################################################################\n",
    "                    ### load full sequence dataset into memory (will likely run into memory error)\n",
    "                    ##############################################################################\n",
    "                    \n",
    "                    if verbose:\n",
    "                        logging.info(\"Loading frame sequence data into memory [may take a few minutes]\")\n",
    "\n",
    "                    # load resized numpy array\n",
    "                    path_vid_resized = path_cache + 'frames/'\n",
    "                    path_vid_resized += str(self.frame_size[0]) + \"_\" + str(self.frame_size[1]) + '/' \n",
    "\n",
    "                    path_labels = path_cache + 'labels/'\n",
    "\n",
    "                    # read vid paths\n",
    "                    path_videos = get_video_paths()\n",
    "\n",
    "                    # loop over all vids and load full frame sequences into memory \n",
    "                    # (recommend using generator if lots of data to avoid out of memory issues)\n",
    "                    for c, path_video in enumerate(path_videos):\n",
    "                        \n",
    "                        if verbose:\n",
    "                            logging.info(\"Loading frame sequence data into memory {}/{}\".format(c+1,len(path_videos)))\n",
    "\n",
    "                        # get vid name from path\n",
    "                        video_name = path_video.split(\"/\")[-2]\n",
    "\n",
    "                        ### create sequence: features\n",
    "                        # load precomputed frames\n",
    "                        frames = np.load(path_vid_resized  + video_name + '.npy')\n",
    "\n",
    "                        # first apply preprocessing if pretrained model given\n",
    "                        if pretrained_model_name != None:\n",
    "                            frames = self.preprocess_input(frames)\n",
    "\n",
    "                        # build sequences\n",
    "                        x = []\n",
    "                        for i in range(sequence_length, len(frames) + 1):\n",
    "                            x.append(frames[i-sequence_length:i])\n",
    "                        x = np.array(x)\n",
    "\n",
    "\n",
    "                        ### create sequence: labels\n",
    "                        # load precomputed labels\n",
    "                        labels = np.load(path_labels + video_name + '.npy')     \n",
    "\n",
    "                        # temp lists to store sequences\n",
    "                        y = []\n",
    "                        for i in range(sequence_length, len(self.labels) + 1):\n",
    "                            y.append(labels[i-1])\n",
    "                        y = (np.array(y))\n",
    "\n",
    "                        ### build output\n",
    "                        if self.video_splits[video_name] == \"train\":\n",
    "                            self.x_train.append(x)\n",
    "                            self.y_train.append(y)\n",
    "                        if self.video_splits[video_name] == \"valid\":\n",
    "                            self.x_valid.append(x)\n",
    "                            self.y_valid.append(y)\n",
    "                        if self.video_splits[video_name] == \"test\":\n",
    "                            self.x_test.append(x)\n",
    "                            self.y_test.append(y)\n",
    "                else:\n",
    "                    #############################\n",
    "                    ### Build sequences generator\n",
    "                    #############################\n",
    "            \n",
    "                    # compute and save h5 sequence files (save_frame_sequences_to_h5 returns \n",
    "                    # the sequence sizes which we need for our generator)\n",
    "                    self.total_rows_train, self.total_rows_valid, self.total_rows_test = self.save_frame_sequences_to_h5()\n",
    "                    \n",
    "                    # init generators\n",
    "                    self.generator_train = DataGenerator(self.batch_size, self.path_h5_train, self.total_rows_train)\n",
    "                    self.generator_valid = DataGenerator(self.batch_size, self.path_h5_valid, self.total_rows_valid)\n",
    "                    self.generator_test = DataGenerator(self.batch_size, self.path_h5_test, self.total_rows_test)\n",
    "                \n",
    "        else:\n",
    "\n",
    "            ### not sequence\n",
    "            \n",
    "            if return_CNN_features:\n",
    "                \n",
    "                if verbose:\n",
    "                    logging.info(\"Loading features data into memory [may take a few minutes]\")\n",
    "                \n",
    "                ###################\n",
    "                ### feature vectors\n",
    "                ###################\n",
    "                \n",
    "                path_features = path_cache + 'features/' + pretrained_model_name + \"/\" + pooling + '/'\n",
    "                if not return_CNN_features and pretrained_model_name is not None:\n",
    "                    path_features = path_cache + 'features/' + pretrained_model_name + \"__\" + custom_model_name + \"/\" + pooling + '/'\n",
    "                \n",
    "                path_labels = path_cache + 'labels/'\n",
    "                \n",
    "                # read vid paths\n",
    "                path_videos = get_video_paths()\n",
    "\n",
    "                # loop over all vids and load precomputed features\n",
    "                for c, path_video in enumerate(path_videos):\n",
    "\n",
    "                    if verbose:\n",
    "                        logging.info(\"Loading features data into memory: {}/{}\".format(c+1,len(path_videos)))\n",
    "                    \n",
    "                    # get vid name from path\n",
    "                    video_name = path_video.split(\"/\")[-2]\n",
    "                    \n",
    "                    ### load precomputed features\n",
    "                    x = np.load(path_features + video_name + '.npy')\n",
    "                    y = np.load(path_labels + video_name + '.npy')\n",
    "                    \n",
    "                    if x.shape[0] != y.shape[0]:\n",
    "                        print(\"XXX\", path_video, x.shape, y.shape)\n",
    "                    \n",
    "                    ### build output\n",
    "                    if self.video_splits[video_name] == \"train\":\n",
    "                        self.x_train.append(x)\n",
    "                        self.y_train.append(y)\n",
    "                    if self.video_splits[video_name] == \"valid\":\n",
    "                        self.x_valid.append(x)\n",
    "                        self.y_valid.append(y)\n",
    "                    if self.video_splits[video_name] == \"test\":\n",
    "                        self.x_test.append(x)\n",
    "                        self.y_test.append(y)\n",
    "            else:\n",
    "                \n",
    "                if verbose:\n",
    "                    logging.info(\"Loading frames into memory [may take a few minutes]\")\n",
    "                \n",
    "                #################\n",
    "                ### single frames\n",
    "                #################\n",
    "                \n",
    "                # load resized numpy array\n",
    "                path_vid_resized = path_cache + 'frames/'\n",
    "                path_vid_resized += str(self.frame_size[0]) + \"_\" + str(self.frame_size[1]) + '/' \n",
    "                \n",
    "                path_labels = path_cache + 'labels/'\n",
    "                \n",
    "                # read vid paths\n",
    "                path_videos = get_video_paths()\n",
    "\n",
    "                # loop over all vids and load frames into memory\n",
    "                for c, path_video in enumerate(path_videos):\n",
    "\n",
    "                    if verbose:\n",
    "                        logging.info(\"Loading frames into memory: {}/{}\".format(c+1,len(path_videos)))\n",
    "                    \n",
    "                    # get vid name from path\n",
    "                    video_name = path_video.split(\"/\")[-2]\n",
    "                    \n",
    "                    # load precomputed numpy arrays for frames and labels\n",
    "                    x = np.load(path_vid_resized  + video_name + '.npy')\n",
    "                    y = np.load(path_labels + video_name + '.npy')\n",
    "                    \n",
    "                    # apply preprocessing if pretrained model given\n",
    "                    if pretrained_model_name != None:\n",
    "                        x = self.preprocess_input(x)\n",
    "                \n",
    "                    ### build output\n",
    "                    if self.video_splits[video_name] == \"train\":\n",
    "                        self.x_train.append(x)\n",
    "                        self.y_train.append(y)\n",
    "                    if self.video_splits[video_name] == \"valid\":\n",
    "                        self.x_valid.append(x)\n",
    "                        self.y_valid.append(y)\n",
    "                    if self.video_splits[video_name] == \"test\":\n",
    "                        self.x_test.append(x)\n",
    "                        self.y_test.append(y)\n",
    "            \n",
    "            \n",
    "        #################################\n",
    "        ### get file paths for each split\n",
    "        #################################\n",
    "        #\n",
    "        # Note: only makes sense for sequence_length = 1\n",
    "        \n",
    "        # get file paths: train\n",
    "        dflab = self.labels[self.labels['split'] == 'train']\n",
    "        self.paths_train = list(path_data + dflab['video'] + \"/\" + dflab['frame'])\n",
    "\n",
    "        # get file paths: valid\n",
    "        dflab = self.labels[self.labels['split'] == 'valid']\n",
    "        self.paths_valid = list(path_data + dflab['video'] + \"/\" + dflab['frame'])\n",
    "\n",
    "        # get file paths: test\n",
    "        dflab = self.labels[self.labels['split'] == 'test']\n",
    "        self.paths_test = list(path_data + dflab['video'] + \"/\" + dflab['frame'])\n",
    "        \n",
    "        \n",
    "        \n",
    "        #################################################\n",
    "        ### reshape list outputs (if not using generator)\n",
    "        #################################################\n",
    "        \n",
    "        if not return_generator:\n",
    "            ## e.g. (9846, 224, 224, 3) for frames [return_CNN_features=True]\n",
    "            ## or  (9846, 512) for features [return_CNN_features=False]\n",
    "            self.x_train = np.concatenate(self.x_train, axis=0)\n",
    "            self.y_train = np.concatenate(self.y_train, axis=0)\n",
    "            self.x_valid = np.concatenate(self.x_valid, axis=0)\n",
    "            self.y_valid = np.concatenate(self.y_valid, axis=0)\n",
    "            self.x_test = np.concatenate(self.x_test, axis=0)\n",
    "            self.y_test = np.concatenate(self.y_test, axis=0)\n",
    "            \n",
    "            # shuffle train and validation set\n",
    "            self.shuffle()\n",
    "            \n",
    "\n",
    "    def __str__(self):\n",
    "        return \"x_train: {}, y_train: {} ... x_valid: {}, y_valid: {} ... x_test: {}, y_test: {}\".format(self.x_train.shape,self.y_train.shape,self.x_valid.shape,self.y_valid.shape,self.x_test.shape,self.y_test.shape)\n",
    "            \n",
    "    def shuffle(self):\n",
    "        \"\"\"\n",
    "        Randomize the order of samples in train and valid splits\n",
    "        \"\"\"\n",
    "        ###########\n",
    "        ### shuffle\n",
    "        ###########\n",
    "        print(self.x_train.shape, self.y_train.shape, len(self.paths_train))\n",
    "        if self.sequence_length == 1:\n",
    "            self.x_train, self.y_train, self.paths_train = shuffle(self.x_train, self.y_train, self.paths_train)\n",
    "            self.x_valid, self.y_valid, self.paths_valid = shuffle(self.x_valid, self.y_valid, self.paths_valid)\n",
    "        else:\n",
    "            self.x_train, self.y_train = shuffle(self.x_train, self.y_train)\n",
    "            self.x_valid, self.y_valid = shuffle(self.x_valid, self.y_valid)\n",
    "\n",
    "        \n",
    "\n",
    "    # Even at small sequence lengths, loading the full dataset as \n",
    "    # a sequence into memory is not feasible so we need to use generators\n",
    "    # that iterate over the dataset without loading it all into memory\n",
    "    # \n",
    "    # For now, we will assume that we will load the features datasets into memory\n",
    "    # because this is more feasible but for large datasets, we'd want to use generators for that too. \n",
    "    # An implementation for a feature generator  can be done by pattern matching the implementation for frames \n",
    "    # \n",
    "    # we first precompute a sequences h5 file (it's too big to fit in memory but we never have more than 1\n",
    "    # video's sequences in memory) ...then we will initialize a generator that samples sequences from the \n",
    "    # h5 file and returns batches that will be passed to our model's fit_generator method\n",
    "\n",
    "    def save_frame_sequences_to_h5(self):\n",
    "        \"\"\"\n",
    "        Save sequence of frames to h5 files (1 for each split) in cache \n",
    "        because dataset too big to load into memory\n",
    "        \n",
    "        Will create generator that reads random rows from these h5 files\n",
    "        \n",
    "        Inspired by: https://stackoverflow.com/questions/41849649/write-to-hdf5-and-shuffle-big-arrays-of-data\n",
    "        \"\"\"\n",
    "    \n",
    "        #######################\n",
    "        ### setup h5 file paths\n",
    "        #######################\n",
    "        \n",
    "        if not os.path.exists(path_cache + 'sequences/'):\n",
    "            os.makedirs(path_cache + 'sequences/')\n",
    "\n",
    "        path_h5_base = path_cache + 'sequences/'\n",
    "\n",
    "        # store h5 files in subfolder in cache/sequences/ either with pretrained model name or resize name\n",
    "        # since we need to run preprocessing for pretrained models but not for vanilla resizing (3DCNN)\n",
    "        if pretrained_model_name is not None:\n",
    "            path_h5_base += pretrained_model_name + '/'\n",
    "        else:\n",
    "            path_h5_base += str(self.frame_size[0]) + \"_\" + str(self.frame_size[1]) + '/' \n",
    "\n",
    "        if not os.path.exists(path_h5_base):\n",
    "            os.makedirs(path_h5_base)\n",
    "\n",
    "        self.path_h5_train = path_h5_base + 'h5_' + str(self.sequence_length) + '_train.h5'\n",
    "        self.path_h5_valid = path_h5_base + 'h5_' + str(self.sequence_length) + '_valid.h5'\n",
    "        self.path_h5_test = path_h5_base + 'h5_' + str(self.sequence_length) + '_test.h5'\n",
    "    \n",
    "        # build h5 file if doesn't exists()\n",
    "        if not os.path.exists(self.path_h5_train) or not os.path.exists(self.path_h5_valid) or not os.path.exists(self.path_h5_test) or not os.path.exists(path_h5_base + 'h5_meta.json'):\n",
    "            \n",
    "            # delete partially created cache\n",
    "            paths_to_clear = [self.path_h5_train, self.path_h5_valid, self.path_h5_test, path_h5_base + 'h5_meta.json']\n",
    "            for path_to_clear in paths_to_clear:                \n",
    "                if os.path.exists(path_to_clear):\n",
    "                    print(\"Removing partially created sequences cache file: {}\".format(path_to_clear))\n",
    "                    os.remove(path_to_clear)\n",
    "             \n",
    "            if verbose:\n",
    "                logging.info(\"Computing frame sequence h5 files: {} [may take a few minutes]\".format(path_h5_base))\n",
    "\n",
    "            ##################################################\n",
    "            ### get size of train/valid/test sequence datasets\n",
    "            ##################################################\n",
    "\n",
    "            # total number of rows of sequence data we have for each split\n",
    "            # this is not the same as the number of frames since we exclude\n",
    "            # the first (self.sequence_length-1) frames\n",
    "            total_rows_train = 0\n",
    "            total_rows_valid = 0\n",
    "            total_rows_test = 0\n",
    "\n",
    "            # load resized numpy array\n",
    "            path_vid_resized = path_cache + 'frames/'\n",
    "            path_vid_resized += str(self.frame_size[0]) + \"_\" + str(self.frame_size[1]) + '/' \n",
    "\n",
    "            path_labels = path_cache + 'labels/'\n",
    "\n",
    "            # read vid paths\n",
    "            path_videos = get_video_paths()\n",
    "\n",
    "            # loop over all vids and resize frames, saving to new folder in /cache/frames/\n",
    "            for c, path_video in enumerate(path_videos):\n",
    "                                \n",
    "                if verbose:\n",
    "                    logging.info(\"Computing frame sequence h5 files: {}/{} [precompute]\".format(c+1,len(path_videos)))\n",
    "\n",
    "                # get vid name from path\n",
    "                video_name = path_video.split(\"/\")[-2]\n",
    "\n",
    "                # load resized frames\n",
    "                frames = np.load(path_vid_resized  + video_name + '.npy')\n",
    "\n",
    "                # build sequences\n",
    "                x = []\n",
    "                for i in range(self.sequence_length, len(frames) + 1):\n",
    "                    x.append(frames[i-self.sequence_length:i])\n",
    "                x = np.array(x)\n",
    "\n",
    "                if self.video_splits[video_name] == \"train\":\n",
    "                    total_rows_train += len(x)\n",
    "                if self.video_splits[video_name] == \"valid\":\n",
    "                    total_rows_valid += len(x)\n",
    "                if self.video_splits[video_name] == \"test\":\n",
    "                    total_rows_test += len(x)\n",
    "\n",
    "            # calc shapes required for full sequence dataset\n",
    "            h5_shape_train_x = (total_rows_train, self.sequence_length, self.frame_size[0], self.frame_size[1], 3)\n",
    "            h5_shape_train_y = (total_rows_train, self.num_classes)\n",
    "\n",
    "            h5_shape_valid_x = (total_rows_valid, self.sequence_length, self.frame_size[0], self.frame_size[1], 3)\n",
    "            h5_shape_valid_y = (total_rows_valid, self.num_classes)\n",
    "\n",
    "            h5_shape_test_x = (total_rows_test, self.sequence_length, self.frame_size[0], self.frame_size[1], 3)\n",
    "            h5_shape_test_y = (total_rows_test, self.num_classes)\n",
    "\n",
    "\n",
    "            ################################\n",
    "            ### Initialize and open h5 files\n",
    "            ################################\n",
    "\n",
    "            # open h5 file to store big sequence dataset feature and label arrays\n",
    "            # path_h5file = MODEL -> SEQUENCE LENGTH\n",
    "            f_train = h5py.File(self.path_h5_train, 'a')\n",
    "            f_valid = h5py.File(self.path_h5_valid, 'a')\n",
    "            f_test = h5py.File(self.path_h5_test, 'a')\n",
    "\n",
    "            # initialize h5 datasets\n",
    "            h5_train_x = f_train.create_dataset('sequences', shape= h5_shape_train_x, dtype='uint8')\n",
    "            h5_train_y = f_train.create_dataset('labels', shape= h5_shape_train_y, dtype='uint8')\n",
    "\n",
    "            h5_valid_x = f_valid.create_dataset('sequences', shape= h5_shape_valid_x, dtype='uint8')\n",
    "            h5_valid_y = f_valid.create_dataset('labels', shape= h5_shape_valid_y, dtype='uint8')\n",
    "\n",
    "            h5_test_x = f_test.create_dataset('sequences', shape= h5_shape_test_x, dtype='uint8')\n",
    "            h5_test_y = f_test.create_dataset('labels', shape= h5_shape_test_y, dtype='uint8')\n",
    "\n",
    "            ##################################################\n",
    "            ### Build h5 files for this sequence / model combo\n",
    "            ##################################################\n",
    "\n",
    "            # load resized numpy array\n",
    "            path_vid_resized = path_cache + 'frames/'\n",
    "            path_vid_resized += str(self.frame_size[0]) + \"_\" + str(self.frame_size[1]) + '/' \n",
    "\n",
    "            path_labels = path_cache + 'labels/'\n",
    "\n",
    "            # read vid paths\n",
    "            path_videos = get_video_paths()\n",
    "\n",
    "            # keep track of where we are in the h5 file\n",
    "            h5_cursor_train = 0\n",
    "            h5_cursor_valid = 0\n",
    "            h5_cursor_test = 0\n",
    "\n",
    "            # loop over all vids and build sequences file\n",
    "            for c, path_video in enumerate(path_videos):\n",
    "                \n",
    "                if verbose:\n",
    "                    logging.info(\"Computing frame sequence h5 files: {}/{} [build h5 file]\".format(c+1,len(path_videos)))\n",
    "\n",
    "                # get vid name from path\n",
    "                video_name = path_video.split(\"/\")[-2]\n",
    "\n",
    "                ### create sequence: features\n",
    "                # load precomputed frames\n",
    "                frames = np.load(path_vid_resized  + video_name + '.npy')\n",
    "                \n",
    "                # first apply preprocessing if pretrained model given\n",
    "                if pretrained_model_name != None:\n",
    "                    frames = self.preprocess_input(frames)\n",
    "                    \n",
    "                # build sequences\n",
    "                x = []\n",
    "                for i in range(self.sequence_length, len(frames) + 1):\n",
    "                    x.append(frames[i-self.sequence_length:i])\n",
    "                x = np.array(x)\n",
    "\n",
    "                ### create sequence: labels\n",
    "                # load precomputed labels\n",
    "                labels = np.load(path_labels + video_name + '.npy')     \n",
    "\n",
    "                # temp lists to store sequences\n",
    "                y = []\n",
    "                for i in range(self.sequence_length, len(labels) + 1):\n",
    "                    y.append(labels[i-1])\n",
    "                y = (np.array(y))\n",
    "\n",
    "                ### write this vid's data to relevant h5 dataset\n",
    "                if self.video_splits[video_name] == \"train\":\n",
    "                    h5_train_x[h5_cursor_train:h5_cursor_train + x.shape[0], :, :, :, :] = x\n",
    "                    h5_train_y[h5_cursor_train:h5_cursor_train + y.shape[0], :] = y\n",
    "                    h5_cursor_train += len(x)\n",
    "                if self.video_splits[video_name] == \"valid\":\n",
    "                    h5_valid_x[h5_cursor_valid:h5_cursor_valid + x.shape[0], :, :, :, :] = x\n",
    "                    h5_valid_y[h5_cursor_valid:h5_cursor_valid + y.shape[0], :] = y\n",
    "                    h5_cursor_valid += len(x)\n",
    "                if self.video_splits[video_name] == \"test\":\n",
    "                    h5_test_x[h5_cursor_test:h5_cursor_test + x.shape[0], :, :, :, :] = x\n",
    "                    h5_test_y[h5_cursor_test:h5_cursor_test + y.shape[0], :] = y\n",
    "                    h5_cursor_test += len(x)\n",
    "            \n",
    "            # save total row counts to file\n",
    "            with open(path_h5_base + 'h5_meta.json', 'w') as fp:\n",
    "                json.dump({'total_rows_train':total_rows_train,\n",
    "                           'total_rows_valid': total_rows_valid,\n",
    "                           'total_rows_test':total_rows_test}\n",
    "                          , fp)\n",
    "                    \n",
    "            # close h5 files\n",
    "            f_train.close()\n",
    "            f_valid.close()\n",
    "            f_test.close()\n",
    "        \n",
    "            # return total samples for each split so we can pass them to our DataGenerator\n",
    "            return total_rows_train, total_rows_valid, total_rows_test\n",
    "        \n",
    "        else:\n",
    "            # h5 sequence file already exists - just load the sequence meta and return sequence lengths \n",
    "            # so we can pass them to our DataGenerator\n",
    "            total_rows_train, total_rows_valid, total_rows_test = None, None, None\n",
    "            \n",
    "            with open(path_h5_base + 'h5_meta.json', 'r') as fp:\n",
    "                h5_meta = json.load(fp)\n",
    "                total_rows_train = h5_meta['total_rows_train']\n",
    "                total_rows_valid = h5_meta['total_rows_valid']\n",
    "                total_rows_test = h5_meta['total_rows_test']\n",
    "                \n",
    "            # return total samples for each split so we can pass them to our DataGenerator\n",
    "            return total_rows_train, total_rows_valid, total_rows_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST GENERATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T15:46:23.349489Z",
     "start_time": "2019-01-14T15:46:23.345971Z"
    }
   },
   "outputs": [],
   "source": [
    "from deepvideoclassification.models import Architecture\n",
    "from deepvideoclassification.models import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T15:46:23.545803Z",
     "start_time": "2019-01-14T15:46:23.542142Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model_name = \"vgg16\"\n",
    "pooling=\"max\"\n",
    "sequence_length = 3\n",
    "\n",
    "layer_1_size = 128\n",
    "layer_2_size = 64\n",
    "layer_3_size = 32\n",
    "dropout=0.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train with no generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T16:53:43.917880Z",
     "start_time": "2019-01-11T16:52:42.619542Z"
    }
   },
   "outputs": [],
   "source": [
    "data = Data(sequence_length = sequence_length, \n",
    "            return_CNN_features = False, \n",
    "            pretrained_model_name=pretrained_model_name,\n",
    "            pooling = pooling)\n",
    "\n",
    "num_classes = data.num_classes \n",
    "frame_size = data.frame_size\n",
    "num_features = pretrained_model_len_features[pretrained_model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T16:57:00.634326Z",
     "start_time": "2019-01-11T16:56:59.407512Z"
    }
   },
   "outputs": [],
   "source": [
    "architecture = Architecture(architecture=\"video_LRCNN_trainable\", \n",
    "                            sequence_model = 'LSTM',\n",
    "                            sequence_model_layers = 1,\n",
    "                            sequence_length = sequence_length,\n",
    "                            num_classes = data.num_classes, \n",
    "                            frame_size = data.frame_size, \n",
    "                            pretrained_model_name='vgg16', \n",
    "                            pooling='max',\n",
    "                            layer_1_size=64, \n",
    "                            layer_2_size=32, \n",
    "                            layer_3_size=8, \n",
    "                            dropout=0.2,\n",
    "                            convolution_kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T17:13:25.237102Z",
     "start_time": "2019-01-11T16:57:02.331628Z"
    }
   },
   "outputs": [],
   "source": [
    "# train model with no data generator\n",
    "train(architecture.model, data, path_model = pwd+'models/', learning_rate = 0.001, epochs = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train with generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T16:29:03.705921Z",
     "start_time": "2019-01-14T16:22:47.440786Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-14 16:22:48,370 [MainThread  ] [INFO ]  Computing frame sequence h5 files: /mnt/seals/cache/sequences/vgg16/ [may take a few minutes]\n",
      "2019-01-14 16:22:48,372 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 1/25 [precompute]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing partially created sequences cache file: /mnt/seals/cache/sequences/vgg16//h5_3train.h5\n",
      "Removing partially created sequences cache file: /mnt/seals/cache/sequences/vgg16//h5_3valid.h5\n",
      "Removing partially created sequences cache file: /mnt/seals/cache/sequences/vgg16//h5_3test.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-14 16:22:49,791 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 2/25 [precompute]\n",
      "2019-01-14 16:22:50,994 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 3/25 [precompute]\n",
      "2019-01-14 16:22:52,469 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 4/25 [precompute]\n",
      "2019-01-14 16:22:54,013 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 5/25 [precompute]\n",
      "2019-01-14 16:22:55,030 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 6/25 [precompute]\n",
      "2019-01-14 16:22:55,313 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 7/25 [precompute]\n",
      "2019-01-14 16:22:56,637 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 8/25 [precompute]\n",
      "2019-01-14 16:22:58,193 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 9/25 [precompute]\n",
      "2019-01-14 16:22:59,727 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 10/25 [precompute]\n",
      "2019-01-14 16:23:00,041 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 11/25 [precompute]\n",
      "2019-01-14 16:23:01,142 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 12/25 [precompute]\n",
      "2019-01-14 16:23:02,610 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 13/25 [precompute]\n",
      "2019-01-14 16:23:04,115 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 14/25 [precompute]\n",
      "2019-01-14 16:23:04,414 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 15/25 [precompute]\n",
      "2019-01-14 16:23:05,802 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 16/25 [precompute]\n",
      "2019-01-14 16:23:07,124 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 17/25 [precompute]\n",
      "2019-01-14 16:23:08,564 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 18/25 [precompute]\n",
      "2019-01-14 16:23:10,113 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 19/25 [precompute]\n",
      "2019-01-14 16:23:10,512 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 20/25 [precompute]\n",
      "2019-01-14 16:23:12,031 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 21/25 [precompute]\n",
      "2019-01-14 16:23:12,346 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 22/25 [precompute]\n",
      "2019-01-14 16:23:13,826 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 23/25 [precompute]\n",
      "2019-01-14 16:23:15,400 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 24/25 [precompute]\n",
      "2019-01-14 16:23:16,563 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 25/25 [precompute]\n",
      "2019-01-14 16:23:18,051 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 1/25 [build h5 file]\n",
      "2019-01-14 16:23:35,088 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 2/25 [build h5 file]\n",
      "2019-01-14 16:23:48,729 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 3/25 [build h5 file]\n",
      "2019-01-14 16:24:05,578 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 4/25 [build h5 file]\n",
      "2019-01-14 16:24:22,719 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 5/25 [build h5 file]\n",
      "2019-01-14 16:24:34,692 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 6/25 [build h5 file]\n",
      "2019-01-14 16:24:37,392 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 7/25 [build h5 file]\n",
      "2019-01-14 16:24:52,053 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 8/25 [build h5 file]\n",
      "2019-01-14 16:25:08,811 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 9/25 [build h5 file]\n",
      "2019-01-14 16:25:27,901 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 10/25 [build h5 file]\n",
      "2019-01-14 16:25:30,795 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 11/25 [build h5 file]\n",
      "2019-01-14 16:25:43,316 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 12/25 [build h5 file]\n",
      "2019-01-14 16:25:59,727 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 13/25 [build h5 file]\n",
      "2019-01-14 16:26:17,631 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 14/25 [build h5 file]\n",
      "2019-01-14 16:26:20,597 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 15/25 [build h5 file]\n",
      "2019-01-14 16:26:37,131 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 16/25 [build h5 file]\n",
      "2019-01-14 16:26:51,992 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 17/25 [build h5 file]\n",
      "2019-01-14 16:27:08,911 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 18/25 [build h5 file]\n",
      "2019-01-14 16:27:26,625 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 19/25 [build h5 file]\n",
      "2019-01-14 16:27:30,416 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 20/25 [build h5 file]\n",
      "2019-01-14 16:27:46,900 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 21/25 [build h5 file]\n",
      "2019-01-14 16:27:49,888 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 22/25 [build h5 file]\n",
      "2019-01-14 16:28:06,834 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 23/25 [build h5 file]\n",
      "2019-01-14 16:28:28,122 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 24/25 [build h5 file]\n",
      "2019-01-14 16:28:40,804 [MainThread  ] [INFO ]  Computing frame sequence h5 files: 25/25 [build h5 file]\n"
     ]
    }
   ],
   "source": [
    "data = Data(sequence_length = sequence_length, \n",
    "            return_CNN_features = False, \n",
    "            pretrained_model_name=pretrained_model_name,\n",
    "            pooling = pooling,\n",
    "            batch_size=32,\n",
    "            return_generator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T16:32:39.190057Z",
     "start_time": "2019-01-14T16:32:39.186819Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T16:32:49.662123Z",
     "start_time": "2019-01-14T16:32:39.887813Z"
    }
   },
   "outputs": [],
   "source": [
    "architecture = Architecture(architecture=\"video_LRCNN_trainable\", \n",
    "                            sequence_model = 'LSTM',\n",
    "                            sequence_model_layers = 1,\n",
    "                            sequence_length = sequence_length,\n",
    "                            num_classes = data.num_classes, \n",
    "                            frame_size = data.frame_size, \n",
    "                            pretrained_model_name='vgg16', \n",
    "                            pooling='max',\n",
    "                            layer_1_size=64, \n",
    "                            layer_2_size=32, \n",
    "                            layer_3_size=8, \n",
    "                            dropout=0.2,\n",
    "                            convolution_kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T16:32:53.317654Z",
     "start_time": "2019-01-14T16:32:53.264593Z"
    }
   },
   "outputs": [],
   "source": [
    "# create optimizer with given learning rate \n",
    "opt = Adam()\n",
    "\n",
    "# compile model\n",
    "architecture.model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T16:40:05.143002Z",
     "start_time": "2019-01-14T16:33:02.316014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 485/1892 [======>.......................] - ETA: 20:20 - loss: 0.6752 - acc: 0.4692"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-ed273e90d345>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     workers=8)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model on dataset\n",
    "architecture.model.fit_generator(generator=data.generator_train,\n",
    "                    validation_data=data.generator_valid,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train features model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T17:26:34.153407Z",
     "start_time": "2019-01-14T16:41:50.460090Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-14 16:41:52,117 [MainThread  ] [INFO ]  Computing pretrained model features for video 1/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 16:44:06,970 [MainThread  ] [INFO ]  Computing pretrained model features for video 2/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 16:45:53,712 [MainThread  ] [INFO ]  Computing pretrained model features for video 3/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 16:48:09,095 [MainThread  ] [INFO ]  Computing pretrained model features for video 4/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 16:50:24,553 [MainThread  ] [INFO ]  Computing pretrained model features for video 5/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 16:51:52,066 [MainThread  ] [INFO ]  Computing pretrained model features for video 6/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 16:52:13,442 [MainThread  ] [INFO ]  Computing pretrained model features for video 7/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 16:54:15,821 [MainThread  ] [INFO ]  Computing pretrained model features for video 8/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 16:56:31,185 [MainThread  ] [INFO ]  Computing pretrained model features for video 9/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 16:58:47,766 [MainThread  ] [INFO ]  Computing pretrained model features for video 10/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 16:59:08,952 [MainThread  ] [INFO ]  Computing pretrained model features for video 11/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 17:00:51,820 [MainThread  ] [INFO ]  Computing pretrained model features for video 12/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 17:03:08,737 [MainThread  ] [INFO ]  Computing pretrained model features for video 13/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 17:05:25,358 [MainThread  ] [INFO ]  Computing pretrained model features for video 14/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 17:05:47,383 [MainThread  ] [INFO ]  Computing pretrained model features for video 15/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 17:08:27,510 [MainThread  ] [INFO ]  Computing pretrained model features for video 16/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 17:10:26,668 [MainThread  ] [INFO ]  Computing pretrained model features for video 17/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 17:12:35,093 [MainThread  ] [INFO ]  Computing pretrained model features for video 18/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 17:14:51,386 [MainThread  ] [INFO ]  Computing pretrained model features for video 19/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 17:15:20,841 [MainThread  ] [INFO ]  Computing pretrained model features for video 20/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 17:17:37,825 [MainThread  ] [INFO ]  Computing pretrained model features for video 21/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 17:17:59,537 [MainThread  ] [INFO ]  Computing pretrained model features for video 22/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 17:20:17,568 [MainThread  ] [INFO ]  Computing pretrained model features for video 23/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 17:22:34,070 [MainThread  ] [INFO ]  Computing pretrained model features for video 24/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 17:24:17,235 [MainThread  ] [INFO ]  Computing pretrained model features for video 25/25 using pretrained model: vgg16, pooling: max\n",
      "2019-01-14 17:26:33,781 [MainThread  ] [INFO ]  Loading features data into memory [may take a few minutes]\n",
      "2019-01-14 17:26:33,784 [MainThread  ] [INFO ]  Loading features data into memory: 1/25\n",
      "2019-01-14 17:26:33,788 [MainThread  ] [INFO ]  Loading features data into memory: 2/25\n",
      "2019-01-14 17:26:33,792 [MainThread  ] [INFO ]  Loading features data into memory: 3/25\n",
      "2019-01-14 17:26:33,796 [MainThread  ] [INFO ]  Loading features data into memory: 4/25\n",
      "2019-01-14 17:26:33,800 [MainThread  ] [INFO ]  Loading features data into memory: 5/25\n",
      "2019-01-14 17:26:33,803 [MainThread  ] [INFO ]  Loading features data into memory: 6/25\n",
      "2019-01-14 17:26:33,806 [MainThread  ] [INFO ]  Loading features data into memory: 7/25\n",
      "2019-01-14 17:26:33,810 [MainThread  ] [INFO ]  Loading features data into memory: 8/25\n",
      "2019-01-14 17:26:33,814 [MainThread  ] [INFO ]  Loading features data into memory: 9/25\n",
      "2019-01-14 17:26:33,819 [MainThread  ] [INFO ]  Loading features data into memory: 10/25\n",
      "2019-01-14 17:26:33,821 [MainThread  ] [INFO ]  Loading features data into memory: 11/25\n",
      "2019-01-14 17:26:33,827 [MainThread  ] [INFO ]  Loading features data into memory: 12/25\n",
      "2019-01-14 17:26:33,834 [MainThread  ] [INFO ]  Loading features data into memory: 13/25\n",
      "2019-01-14 17:26:33,842 [MainThread  ] [INFO ]  Loading features data into memory: 14/25\n",
      "2019-01-14 17:26:33,844 [MainThread  ] [INFO ]  Loading features data into memory: 15/25\n",
      "2019-01-14 17:26:33,852 [MainThread  ] [INFO ]  Loading features data into memory: 16/25\n",
      "2019-01-14 17:26:33,859 [MainThread  ] [INFO ]  Loading features data into memory: 17/25\n",
      "2019-01-14 17:26:33,866 [MainThread  ] [INFO ]  Loading features data into memory: 18/25\n",
      "2019-01-14 17:26:33,874 [MainThread  ] [INFO ]  Loading features data into memory: 19/25\n",
      "2019-01-14 17:26:33,877 [MainThread  ] [INFO ]  Loading features data into memory: 20/25\n",
      "2019-01-14 17:26:33,884 [MainThread  ] [INFO ]  Loading features data into memory: 21/25\n",
      "2019-01-14 17:26:33,887 [MainThread  ] [INFO ]  Loading features data into memory: 22/25\n",
      "2019-01-14 17:26:33,894 [MainThread  ] [INFO ]  Loading features data into memory: 23/25\n",
      "2019-01-14 17:26:33,902 [MainThread  ] [INFO ]  Loading features data into memory: 24/25\n",
      "2019-01-14 17:26:33,908 [MainThread  ] [INFO ]  Loading features data into memory: 25/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60597, 512) (60597, 8) 60597\n"
     ]
    }
   ],
   "source": [
    "data = Data(sequence_length = 1, \n",
    "            return_CNN_features = True, \n",
    "            pretrained_model_name=pretrained_model_name,\n",
    "            pooling = pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T19:05:01.859123Z",
     "start_time": "2019-01-14T19:05:01.747317Z"
    }
   },
   "outputs": [],
   "source": [
    "architecture = Architecture(architecture=\"image_MLP_frozen\", \n",
    "                            sequence_length=1,\n",
    "                            num_classes = data.num_classes, \n",
    "                            frame_size = data.frame_size, \n",
    "                            pretrained_model_name='vgg16', \n",
    "                            pooling='max',\n",
    "                            layer_1_size=64, \n",
    "                            layer_2_size=32, \n",
    "                            layer_3_size=8, \n",
    "                            dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T19:06:30.816910Z",
     "start_time": "2019-01-14T19:05:02.958836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60597 samples, validate on 6416 samples\n",
      "Epoch 1/30\n",
      "60597/60597 [==============================] - 13s 211us/step - loss: 0.2564 - acc: 0.9110 - val_loss: 0.2061 - val_acc: 0.9262\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.92624, saving model to /mnt/seals/models/model.h5\n",
      "Epoch 2/30\n",
      "60597/60597 [==============================] - 12s 205us/step - loss: 0.1719 - acc: 0.9335 - val_loss: 0.2001 - val_acc: 0.9342\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.92624 to 0.93417, saving model to /mnt/seals/models/model.h5\n",
      "Epoch 3/30\n",
      "60597/60597 [==============================] - 13s 212us/step - loss: 0.1516 - acc: 0.9408 - val_loss: 0.1723 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.93417 to 0.94260, saving model to /mnt/seals/models/model.h5\n",
      "Epoch 4/30\n",
      "60597/60597 [==============================] - 12s 203us/step - loss: 0.1416 - acc: 0.9448 - val_loss: 0.1944 - val_acc: 0.9323\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.94260\n",
      "Epoch 5/30\n",
      "60597/60597 [==============================] - 12s 204us/step - loss: 0.1356 - acc: 0.9475 - val_loss: 0.1901 - val_acc: 0.9331\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.94260\n",
      "Epoch 6/30\n",
      "60597/60597 [==============================] - 12s 206us/step - loss: 0.1305 - acc: 0.9494 - val_loss: 0.1706 - val_acc: 0.9437\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.94260 to 0.94366, saving model to /mnt/seals/models/model.h5\n",
      "Epoch 7/30\n",
      "16032/60597 [======>.......................] - ETA: 8s - loss: 0.1307 - acc: 0.9495"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-0e91ca0f8ea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfit_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchitecture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'models/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/seals/deepvideoclassification/models.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, path_model, learning_rate, epochs, batch_size, patience, verbose)\u001b[0m\n\u001b[1;32m    794\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m               verbose=verbose)\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "fit_history = train(architecture.model, data, path_model = pwd+'models/', learning_rate = 0.001, epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T19:06:38.924696Z",
     "start_time": "2019-01-14T19:06:38.911100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 72        \n",
      "=================================================================\n",
      "Total params: 35,248\n",
      "Trainable params: 35,248\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "architecture.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T19:07:29.489Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-14 19:07:29,691 [MainThread  ] [INFO ]  Building resized and feature cache\n",
      "2019-01-14 19:07:30,624 [MainThread  ] [INFO ]  resizing vid 1/25 to 299x299\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    logger.info(\"Building resized and feature cache\")\n",
    "    # build feature cache in advance by running python3 data.py\n",
    "    for pretrained_model_name in pretrained_model_names:\n",
    "        for pooling in poolings:\n",
    "            data = Data(sequence_length=1, \n",
    "                        return_CNN_features=True,\n",
    "                        pretrained_model_name = pretrained_model_name,\n",
    "                        pooling=pooling)\n",
    "            \n",
    "            \n",
    "    # build h5 cache\n",
    "    logger.info(\"Building h5 cache\")\n",
    "    for sequence_length in [2,3,5,10,20]:\n",
    "        for pretrained_model_name in pretrained_model_names:\n",
    "\n",
    "            data = Data(sequence_length=sequence_length, \n",
    "                        return_CNN_features=False, \n",
    "                        pretrained_model_name = pretrained_model_name, \n",
    "                        return_generator = True,\n",
    "                        verbose=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
