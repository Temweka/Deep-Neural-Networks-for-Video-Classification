{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T17:52:01.901453Z",
     "start_time": "2019-01-06T17:52:01.898839Z"
    }
   },
   "outputs": [],
   "source": [
    "### TODO:\n",
    "# * add train/valid/test generators to data\n",
    "# * need option to apply preprocessor when requesting frame data\n",
    "# * make sure don't recompute sequences if inputs don't change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T17:52:01.908159Z",
     "start_time": "2019-01-06T17:52:01.903919Z"
    }
   },
   "outputs": [],
   "source": [
    "# whether to log each feature and sequence status\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T17:52:02.804569Z",
     "start_time": "2019-01-06T17:52:01.911004Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T17:52:04.066254Z",
     "start_time": "2019-01-06T17:52:02.806885Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import pretrained model functions\n",
    "from deepvideoclassification.models import precompute_CNN_features    \n",
    "from deepvideoclassification.models import load_pretrained_model_preprocessor\n",
    "from deepvideoclassification.models import load_pretrained_model\n",
    "\n",
    "# import pretrained model properties\n",
    "from deepvideoclassification.models import pretrained_model_len_features\n",
    "from deepvideoclassification.models import pretrained_model_sizes\n",
    "from deepvideoclassification.models import pretrained_model_names, poolings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T17:52:04.071729Z",
     "start_time": "2019-01-06T17:52:04.068671Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup paths\n",
    "pwd = os.getcwd().replace(\"deepvideoclassification\",\"\")\n",
    "path_cache = pwd + 'cache/'\n",
    "path_data = pwd + 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T17:52:04.078401Z",
     "start_time": "2019-01-06T17:52:04.074108Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"{0}/{1}.log\".format(pwd, \"logs\")),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T17:52:04.086462Z",
     "start_time": "2019-01-06T17:52:04.081077Z"
    }
   },
   "outputs": [],
   "source": [
    "# read vid folders\n",
    "def get_video_paths():\n",
    "    \"\"\"\n",
    "    Return list of video paths \n",
    "\n",
    "    Videos should be in /data/video_1/, /data/video_2/ style folders \n",
    "    with sequentially numbered frame images e.g. /data/video_1/frame00001.jpg\n",
    "\n",
    "    There should be at least 3 videos, 1 for each of train/test/valid splits\n",
    "    Split assignment is given in /data/labels.csv (also to be provided by user)\n",
    "\n",
    "    Functionality to use different parts of a video as train/valid/test \n",
    "    not currently implemented.\n",
    "    \"\"\"\n",
    "    path_videos = []\n",
    "    for filename in os.listdir(path_data):\n",
    "        if os.path.isdir(os.path.join(path_data, filename)):\n",
    "            path_videos.append(filename)\n",
    "\n",
    "    path_videos = [path_data + v + '/' for v in path_videos]\n",
    "\n",
    "    # make sure that there is video data in /data/ and give instructions if not done correctly\n",
    "    assert len(path_videos)>0, \"There need to be at least 3 video folders (at least 1 for each of train, valid, \\\n",
    "    and test splits) in /data/ - each video should be its own folder of frame images with ascending time-ordered \\\n",
    "    filenames e.g. /data/vid1/frame00001.jpg ... videos assignment to train/valid/test split should be given in \\\n",
    "    /data/labels.csv ... cross-validation or train/valid/test splits within a single long video not currently implemented\"\n",
    "\n",
    "    return path_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T17:52:04.097195Z",
     "start_time": "2019-01-06T17:52:04.089558Z"
    }
   },
   "outputs": [],
   "source": [
    "def resize_frames(target_size):\n",
    "    \"\"\"\n",
    "    Resize the frames of all videos and save them to /cache/ \n",
    "    to make model fitting faster .\n",
    "\n",
    "    We resize once upfront rather than each we use a pretrained model or architecture.\n",
    "\n",
    "    Our models require inputs resized to:\n",
    "    * 224 x 224 VGG16, ResNet50, DenseNet, MobileNet\n",
    "    * 299 x 299 XCeption, InceptionV3, InceptionResNetV2\n",
    "    * 112 x 112 3D CNN \n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(path_cache + 'frames/' + str(target_size[0]) + \"_\" + str(target_size[1]) + '/'):\n",
    "\n",
    "        # read vid paths\n",
    "        path_videos = get_video_paths()\n",
    "\n",
    "        # loop over all vids and resize frames, saving to new folder in /cache/frames/\n",
    "        for c,path_video in enumerate(path_videos):\n",
    "\n",
    "            logger.info(\"resizing vid {}/{} to {}x{}\".format(c+1,len(path_videos),target_size[0], target_size[1]))\n",
    "\n",
    "            # get vid name from path\n",
    "            video_name = path_video.split(\"/\")[-2]\n",
    "\n",
    "            # create directory for resized frames - just storing arrays now so commented out\n",
    "            # e.g. path_vid_resized = /cache/frames/224_224/s23-4847/\n",
    "            # path_vid_resized = path_cache + 'frames/'\n",
    "            # path_vid_resized += str(target_size[0]) + \"_\" + str(target_size[1]) + '/' \n",
    "            # path_vid_resized += video_name + '/'\n",
    "\n",
    "            # load frame paths for vid\n",
    "            path_frames = os.listdir(path_video)\n",
    "            path_frames = [path_video + f for f in path_frames if f != '.DS_Store']\n",
    "            path_frames.sort()\n",
    "\n",
    "            # load frames\n",
    "            frames = []\n",
    "            for path_frame in path_frames:\n",
    "\n",
    "                # open image and resize\n",
    "                filename = path_frame.split(\"/\").pop()\n",
    "                img_frame = Image.open(path_frame)\n",
    "                img_frame = img_frame.resize(target_size)\n",
    "                # img_frame.save(path_vid_resized + filename, \"JPEG\", quality = 100)\n",
    "\n",
    "                # convert to array and append to list\n",
    "                img_frame = np.array(img_frame)\n",
    "                frames.append(img_frame)\n",
    "\n",
    "            # save array of resized frames\n",
    "            np.save(\"/mnt/seals/cache/frames/\" + str(target_size[0]) + \"_\" + str(target_size[1]) + \"/\" + video_name, np.array(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T17:52:04.103977Z",
     "start_time": "2019-01-06T17:52:04.099714Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_labels():\n",
    "    # read labels - should be CSV with columns \"video\",\"frame\",\"label\",\"split\"\n",
    "    # e.g. \"s1-218\", \"s1-218-00001.jpeg\", \"noseal\", \"train\"\n",
    "    labels = None\n",
    "    try:\n",
    "        labels = pd.read_csv(path_data + 'labels.csv', usecols=['video','frame','label','split'])\n",
    "    except ValueError as e:\n",
    "        raise Exception(\"Labels file must contain columns ['video','frame','label','split'] - if you only have ['video','frame','label'], use the helper function add_splits_to_labels_file to add train/valid/test splits to your labels file\")\n",
    "    except FileNotFoundError as e:\n",
    "        raise Exception(\"No labels found - please save labels file to /data/labels.csv\") from e\n",
    "\n",
    "    return labels.sort_values([\"video\",\"frame\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T17:52:04.113279Z",
     "start_time": "2019-01-06T17:52:04.106666Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_video_label_arrays():\n",
    "    \"\"\"\n",
    "    Create numpy array with labels for each vid and a label_map.json file\n",
    "    in /cache/labels/\n",
    "    \"\"\"\n",
    "\n",
    "    # create folder for labels\n",
    "    if not os.path.exists(path_cache + 'labels/'):\n",
    "        os.makedirs(path_cache + 'labels/')\n",
    "\n",
    "    # load labels\n",
    "    labels = get_labels()\n",
    "\n",
    "    # build label_map\n",
    "    label_dummies = pd.get_dummies(labels, columns = ['label'])\n",
    "\n",
    "    # get label columns list and build label map dict\n",
    "    label_columns = []\n",
    "    label_map = {}\n",
    "    label_map_idx = 0\n",
    "    for i, col in enumerate(label_dummies.columns):\n",
    "        if col[:6] == 'label_':\n",
    "            label_columns.append(col)\n",
    "            label_map[label_map_idx] = col\n",
    "            label_map_idx+=1\n",
    "\n",
    "    # save label map to json\n",
    "    with open(path_cache + 'labels/label_map.json', 'w') as fp:\n",
    "        json.dump(label_map, fp)\n",
    "\n",
    "    # get video paths\n",
    "    path_videos = get_video_paths()\n",
    "\n",
    "    # save numpy array of labels for each vid\n",
    "    for path_video in path_videos:\n",
    "\n",
    "        # get vid name from path\n",
    "        video_name = path_video.split(\"/\")[-2]\n",
    "\n",
    "        vid_labels = np.array(label_dummies[label_dummies['video'] == video_name][label_columns])\n",
    "\n",
    "        # save labels array for this vid\n",
    "        np.save(\"/mnt/seals/cache/labels/\" + video_name, np.array(vid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T17:52:04.121230Z",
     "start_time": "2019-01-06T17:52:04.116036Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_label_map():\n",
    "    \"\"\"\n",
    "    Returns label map - read from disk\n",
    "    \"\"\"\n",
    "\n",
    "    # load label map from disk\n",
    "    label_map = None\n",
    "    try:\n",
    "        if os.path.exists(path_cache + 'labels/label_map.json'):\n",
    "            with open(path_cache + 'labels/label_map.json', 'r') as fp:\n",
    "                label_map = json.load(fp)\n",
    "        else:\n",
    "            # build labels and label map\n",
    "            create_video_label_arrays()\n",
    "            if os.path.exists(path_cache + 'labels/label_map.json'):\n",
    "                with open(path_cache + 'labels/label_map.json', 'r') as fp:\n",
    "                    label_map = json.load(fp)\n",
    "    except Exception as e:\n",
    "        logger.error ('label map not found - make sure /data/labels.csv exists and data cache has been built')\n",
    "\n",
    "    return label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T23:33:29.207073Z",
     "start_time": "2019-01-06T23:33:29.173586Z"
    }
   },
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    \n",
    "    def __init__(self, sequence_length, \n",
    "                    return_CNN_features = False, pretrained_model_name = None, pooling = None, \n",
    "                    frame_size = None, augmentation = False, oversampling = False,\n",
    "                    model_weights_path = None, custom_model_name = None):\n",
    "        \"\"\"\n",
    "        Data object constructor\n",
    "        \n",
    "        \n",
    "        :sequence_length: number of frames in sequence to be returned by Data object\n",
    "        :return_CNN_features: whether to return precomputed features or return frames (or sequences of features/frames if sequence_length>1)\n",
    "\n",
    "        :return_features: if True then return features (or sequences of feature) from pretrained model, if False then return frames (or sequences of frames)        \n",
    "        :pretrained_model_name: name of pretrained model (or None if not using pretrained model e.g. for 3D-CNN)\n",
    "        :pooling: name of pooling variant (or None if not using pretrained model e.g. for 3D-CNN)\n",
    "        :frame_size: size that frames are resized to (this is looked up for pretrained models)\n",
    "        :augmentation: whether to apply data augmentation (horizontal flips)\n",
    "        :oversampling: whether to apply oversampling to create class balance\n",
    "        \n",
    "        :model_weights_path: path to custom model weights if we want to load CNN model we've fine-tuned to produce features (e.g. for LRCNN)\n",
    "        :custom_model_name: custom output name to append to pretrained model name\n",
    "        \n",
    "        \n",
    "        Notes: \n",
    "        * if pretrained_model_name != None and return_CNN_features=False then will first apply preprocessor to frames (or frame sequences)\n",
    "        \"\"\"\n",
    "    \n",
    "        # required params\n",
    "        self.sequence_length = sequence_length\n",
    "        self.frame_size = frame_size\n",
    "        \n",
    "        # optional params\n",
    "        self.pretrained_model_name = pretrained_model_name\n",
    "        self.pooling = pooling\n",
    "        self.return_CNN_features = return_CNN_features\n",
    "        self.augmentation = augmentation\n",
    "        self.oversampling = oversampling\n",
    "        \n",
    "        # init model data\n",
    "        self.x_train = []\n",
    "        self.y_train = []\n",
    "        #\n",
    "        self.x_valid = []\n",
    "        self.y_valid = []\n",
    "        # \n",
    "        self.x_test = []\n",
    "        self.y_test = []\n",
    "        \n",
    "        # fix case sensitivity\n",
    "        if type(self.pretrained_model_name) == str:\n",
    "            self.pretrained_model_name = self.pretrained_model_name.lower()\n",
    "        if type(self.pooling) == str:\n",
    "            self.pooling = self.pooling.lower()\n",
    "        \n",
    "        ################\n",
    "        ### Prepare data\n",
    "        ################\n",
    "        \n",
    "        # get video paths\n",
    "        self.path_videos = get_video_paths()\n",
    "        \n",
    "        # create label array for each video and load label map\n",
    "        create_video_label_arrays()\n",
    "        self.label_map = load_label_map()\n",
    "        \n",
    "        # get labels\n",
    "        self.labels = get_labels()\n",
    "        \n",
    "        # create dict mapping video to train/valid/test split assignment\n",
    "        video_splits = self.labels[['video','split']].drop_duplicates()\n",
    "        video_splits.set_index(\"video\", inplace=True)\n",
    "        video_splits = video_splits.to_dict()['split']\n",
    "        self.video_splits = video_splits\n",
    "        \n",
    "        # look up target size for pretrained model\n",
    "        if pretrained_model_name is not None:\n",
    "            self.frame_size = pretrained_model_sizes[pretrained_model_name]\n",
    "        \n",
    "        # precompute resized frames (won't recompute if already resized)\n",
    "        resize_frames(self.frame_size)\n",
    "\n",
    "        # pre compute CNN features (won't recompute if already computed)\n",
    "        if return_CNN_features and pretrained_model_name is not None:\n",
    "            if model_weights_path is not None and custom_model_name is not None:\n",
    "                # precompute with custom weights input and name\n",
    "                precompute_CNN_features(self.pretrained_model_name, self.pooling, self.model_weights_path, self.custom_model_name)\n",
    "            else:\n",
    "                precompute_CNN_features(self.pretrained_model_name, self.pooling)\n",
    "            \n",
    "            \n",
    "            \n",
    "        # get preprocessor given pretrained if we will need to apply preprocessor \n",
    "        # (i.e. if return_CNN_features == False and pretrained_model_name != None)\n",
    "        if not return_CNN_features and pretrained_model_name is not None:\n",
    "            self.preprocess_input = load_pretrained_model_preprocessor(self.pretrained_model_name)\n",
    "        \n",
    "        \n",
    "        ###################################\n",
    "        ### load features / build sequences\n",
    "        ###################################\n",
    "        \n",
    "        \n",
    "        # load features/frames from all videos and concat into big array for each of train, valid and test\n",
    "        if self.sequence_length > 1:\n",
    "            \n",
    "            ###################\n",
    "            ### frame sequences\n",
    "            ###################\n",
    "            \n",
    "            if return_CNN_features:\n",
    "                \n",
    "                #####################\n",
    "                ### feature sequences\n",
    "                #####################\n",
    "                \n",
    "                path_features = path_cache + 'features/' + pretrained_model_name + \"/\" + pooling + '/'\n",
    "                if not return_CNN_features and pretrained_model_name is not None:\n",
    "                    path_features = path_cache + 'features/' + pretrained_model_name + \"__\" + custom_model_name + \"/\" + pooling + '/'\n",
    "                path_labels = path_cache + 'labels/'\n",
    "                \n",
    "                # read vid paths\n",
    "                path_videos = get_video_paths()\n",
    "\n",
    "                # loop over all vids and resize frames, saving to new folder in /cache/frames/\n",
    "                for c, path_video in enumerate(path_videos):\n",
    "\n",
    "                    # get vid name from path\n",
    "                    video_name = path_video.split(\"/\")[-2]\n",
    "                    \n",
    "                    \n",
    "                    ### create sequence: features\n",
    "                    # load precomputed features\n",
    "                    features = np.load(path_features + video_name + '.npy')\n",
    "                    # build sequences\n",
    "                    x = []\n",
    "                    for i in range(sequence_length, len(features) + 1):\n",
    "                        x.append(features[i-sequence_length:i])\n",
    "                    x = np.array(x)\n",
    "                    \n",
    "                    \n",
    "                    ### create sequence: labels\n",
    "                    # load precomputed labels\n",
    "                    labels = np.load(path_labels + video_name + '.npy')     \n",
    "\n",
    "                    # temp lists to store sequences\n",
    "                    y = []\n",
    "                    for i in range(sequence_length, len(labels) + 1):\n",
    "                        y.append(labels[i-1])\n",
    "                    y = (np.array(y))\n",
    "\n",
    "                    \n",
    "                    ### build output\n",
    "                    if self.video_splits[video_name] == \"train\":\n",
    "                        self.x_train.append(x)\n",
    "                        self.y_train.append(y)\n",
    "                    if self.video_splits[video_name] == \"valid\":\n",
    "                        self.x_valid.append(x)\n",
    "                        self.y_valid.append(y)\n",
    "                    if self.video_splits[video_name] == \"test\":\n",
    "                        self.x_test.append(x)\n",
    "                        self.y_test.append(y)\n",
    "                        \n",
    "            else:\n",
    "\n",
    "                ###################\n",
    "                ### frame sequences\n",
    "                ###################\n",
    "                \n",
    "                # load resized numpy array\n",
    "                path_vid_resized = path_cache + 'frames/'\n",
    "                path_vid_resized += str(self.frame_size[0]) + \"_\" + str(self.frame_size[1]) + '/' \n",
    "                \n",
    "                path_labels = path_cache + 'labels/'\n",
    "                \n",
    "                # read vid paths\n",
    "                path_videos = get_video_paths()\n",
    "\n",
    "                # loop over all vids and resize frames, saving to new folder in /cache/frames/\n",
    "                for c, path_video in enumerate(path_videos):\n",
    "\n",
    "                    # get vid name from path\n",
    "                    video_name = path_video.split(\"/\")[-2]\n",
    "                    \n",
    "                    ### create sequence: features\n",
    "                    # load precomputed frames\n",
    "                    frames = np.load(path_vid_resized  + video_name + '.npy')\n",
    "                    \n",
    "                    # first apply preprocessing if pretrained model given\n",
    "                    if pretrained_model_name != None:\n",
    "                        frames = self.preprocess_input(frames)\n",
    "                    \n",
    "                    # build sequences\n",
    "                    x = []\n",
    "                    for i in range(sequence_length, len(frames) + 1):\n",
    "                        x.append(frames[i-sequence_length:i])\n",
    "                    x = np.array(x)\n",
    "                    \n",
    "                    \n",
    "                    ### create sequence: labels\n",
    "                    # load precomputed labels\n",
    "                    labels = np.load(path_labels + video_name + '.npy')     \n",
    "\n",
    "                    # temp lists to store sequences\n",
    "                    y = []\n",
    "                    for i in range(sequence_length, len(labels) + 1):\n",
    "                        y.append(labels[i-1])\n",
    "                    y = (np.array(y))\n",
    "\n",
    "                    ### build output\n",
    "                    if self.video_splits[video_name] == \"train\":\n",
    "                        self.x_train.append(x)\n",
    "                        self.y_train.append(y)\n",
    "                    if self.video_splits[video_name] == \"valid\":\n",
    "                        self.x_valid.append(x)\n",
    "                        self.y_valid.append(y)\n",
    "                    if self.video_splits[video_name] == \"test\":\n",
    "                        self.x_test.append(x)\n",
    "                        self.y_test.append(y)\n",
    "                \n",
    "        else:\n",
    "            ###############\n",
    "            ### no sequence\n",
    "            ###############\n",
    "            if return_CNN_features:\n",
    "                \n",
    "                ###################\n",
    "                ### feature vectors\n",
    "                ###################\n",
    "                \n",
    "                path_features = path_cache + 'features/' + pretrained_model_name + \"/\" + pooling + '/'\n",
    "                if not return_CNN_features and pretrained_model_name is not None:\n",
    "                    path_features = path_cache + 'features/' + pretrained_model_name + \"__\" + custom_model_name + \"/\" + pooling + '/'\n",
    "                \n",
    "                path_labels = path_cache + 'labels/'\n",
    "                \n",
    "                # read vid paths\n",
    "                path_videos = get_video_paths()\n",
    "\n",
    "                # loop over all vids and resize frames, saving to new folder in /cache/frames/\n",
    "                for c, path_video in enumerate(path_videos):\n",
    "\n",
    "                    # get vid name from path\n",
    "                    video_name = path_video.split(\"/\")[-2]\n",
    "                    \n",
    "                    ### load precomputed features\n",
    "                    x = np.load(path_features + video_name + '.npy')\n",
    "                    y = np.load(path_labels + video_name + '.npy')\n",
    "                    \n",
    "                    ### build output\n",
    "                    if self.video_splits[video_name] == \"train\":\n",
    "                        self.x_train.append(x)\n",
    "                        self.y_train.append(y)\n",
    "                    if self.video_splits[video_name] == \"valid\":\n",
    "                        self.x_valid.append(x)\n",
    "                        self.y_valid.append(y)\n",
    "                    if self.video_splits[video_name] == \"test\":\n",
    "                        self.x_test.append(x)\n",
    "                        self.y_test.append(y)\n",
    "            else:\n",
    "                \n",
    "                #################\n",
    "                ### single frames\n",
    "                #################\n",
    "                \n",
    "                # load resized numpy array\n",
    "                path_vid_resized = path_cache + 'frames/'\n",
    "                path_vid_resized += str(self.frame_size[0]) + \"_\" + str(self.frame_size[1]) + '/' \n",
    "                \n",
    "                path_labels = path_cache + 'labels/'\n",
    "                \n",
    "                # read vid paths\n",
    "                path_videos = get_video_paths()\n",
    "\n",
    "                # loop over all vids and resize frames, saving to new folder in /cache/frames/\n",
    "                for c, path_video in enumerate(path_videos):\n",
    "\n",
    "                    # get vid name from path\n",
    "                    video_name = path_video.split(\"/\")[-2]\n",
    "                    \n",
    "                    # load precomputed numpy arrays for frames and labels\n",
    "                    x = np.load(path_vid_resized  + video_name + '.npy')\n",
    "                    y = np.load(path_labels + video_name + '.npy')\n",
    "                    \n",
    "                    # apply preprocessing if pretrained model given\n",
    "                    if pretrained_model_name != None:\n",
    "                        x = self.preprocess_input(x)\n",
    "                \n",
    "                    ### build output\n",
    "                    if self.video_splits[video_name] == \"train\":\n",
    "                        self.x_train.append(x)\n",
    "                        self.y_train.append(y)\n",
    "                    if self.video_splits[video_name] == \"valid\":\n",
    "                        self.x_valid.append(x)\n",
    "                        self.y_valid.append(y)\n",
    "                    if self.video_splits[video_name] == \"test\":\n",
    "                        self.x_test.append(x)\n",
    "                        self.y_test.append(y)\n",
    "            \n",
    "            \n",
    "            \n",
    "        ########################\n",
    "        ### reshape list outputs\n",
    "        ########################\n",
    "        ## e.g. (9846, 224, 224, 3) for frames [return_CNN_features=True]\n",
    "        ## or  (9846, 512) for features [return_CNN_features=False]\n",
    "        self.x_train = np.concatenate(self.x_train, axis=0)\n",
    "        self.y_train = np.concatenate(self.y_train, axis=0)\n",
    "        self.x_valid = np.concatenate(self.x_valid, axis=0)\n",
    "        self.y_valid = np.concatenate(self.y_valid, axis=0)\n",
    "        self.x_test = np.concatenate(self.x_test, axis=0)\n",
    "        self.y_test = np.concatenate(self.y_test, axis=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #################################\n",
    "        ### get file paths for each split\n",
    "        #################################\n",
    "        #\n",
    "        # Note: only makes sense for sequence_length = 1\n",
    "        \n",
    "        # get file paths: train\n",
    "        dflab = self.labels[self.labels['split'] == 'train']\n",
    "        self.paths_train = list(path_data + dflab['video'] + \"/\" + dflab['frame'])\n",
    "\n",
    "        # get file paths: valid\n",
    "        dflab = self.labels[self.labels['split'] == 'valid']\n",
    "        self.paths_valid = list(path_data + dflab['video'] + \"/\" + dflab['frame'])\n",
    "\n",
    "        # get file paths: test\n",
    "        dflab = self.labels[self.labels['split'] == 'test']\n",
    "        self.paths_test = list(path_data + dflab['video'] + \"/\" + dflab['frame'])\n",
    "        \n",
    "        # pull number of classes from labels shape\n",
    "        self.num_classes = self.y_train.shape[1]\n",
    "            \n",
    "\n",
    "    def __str__(self):\n",
    "        return \"x_train: {}, y_train: {} ... x_valid: {}, y_valid: {} ... x_test: {}, y_test: {}\".format(self.x_train.shape,self.y_train.shape,self.x_valid.shape,self.y_valid.shape,self.x_test.shape,self.y_test.shape)\n",
    "            \n",
    "    def shuffle(self):\n",
    "        \"\"\"\n",
    "        randomize the order of samples in train and valid splits\n",
    "        \"\"\"\n",
    "        ###########\n",
    "        ### shuffle\n",
    "        ###########\n",
    "        # paths will no longer be correct (they're for debugging anyway)\n",
    "        self.x_train, self.y_train, self.paths_train = shuffle(self.x_train, self.y_train, self.paths_train)\n",
    "        self.x_valid, self.y_valid, self.paths_valid = shuffle(self.x_valid, self.y_valid, self.paths_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T19:11:04.807397Z",
     "start_time": "2019-01-06T19:11:04.537819Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 19:11:04,724 [MainThread  ] [INFO ]  Features already cached: /mnt/seals/cache/features/vgg16/max/\n"
     ]
    }
   ],
   "source": [
    "# data = Data(sequence_length = 1, \n",
    "#             return_CNN_features = True, \n",
    "#             pretrained_model_name='vgg16', \n",
    "#             pooling='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Uncomment below to build cache in advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T00:01:28.250805Z",
     "start_time": "2019-01-07T00:01:28.226484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>frame</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00001.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00002.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00003.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00004.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00005.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00006.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00007.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00008.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00009.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00010.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00011.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00012.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00013.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00014.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00015.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00016.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00017.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00018.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00019.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00020.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00021.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00022.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00023.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00024.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00025.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00026.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00027.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00028.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00029.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>s1-218</td>\n",
       "      <td>s1-218-00030.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12428</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00173.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12429</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00174.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12430</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00175.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12431</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00176.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12432</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00177.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12433</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00178.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12434</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00179.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12435</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00180.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12436</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00181.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12437</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00182.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12438</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00183.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12439</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00184.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12440</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00185.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12441</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00186.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12442</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00187.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12443</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00188.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12444</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00189.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12445</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00190.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12446</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00191.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12447</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00192.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12448</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00193.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12449</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00194.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12450</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00195.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12451</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00196.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12452</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00197.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12453</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00198.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12454</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00199.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12455</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00200.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12456</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00201.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12457</th>\n",
       "      <td>s9-5491</td>\n",
       "      <td>s9-5491-00202.jpeg</td>\n",
       "      <td>noseal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12458 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video               frame   label  split\n",
       "0       s1-218   s1-218-00001.jpeg  noseal  train\n",
       "1       s1-218   s1-218-00002.jpeg  noseal  train\n",
       "2       s1-218   s1-218-00003.jpeg  noseal  train\n",
       "3       s1-218   s1-218-00004.jpeg  noseal  train\n",
       "4       s1-218   s1-218-00005.jpeg  noseal  train\n",
       "5       s1-218   s1-218-00006.jpeg  noseal  train\n",
       "6       s1-218   s1-218-00007.jpeg  noseal  train\n",
       "7       s1-218   s1-218-00008.jpeg  noseal  train\n",
       "8       s1-218   s1-218-00009.jpeg  noseal  train\n",
       "9       s1-218   s1-218-00010.jpeg  noseal  train\n",
       "10      s1-218   s1-218-00011.jpeg  noseal  train\n",
       "11      s1-218   s1-218-00012.jpeg  noseal  train\n",
       "12      s1-218   s1-218-00013.jpeg  noseal  train\n",
       "13      s1-218   s1-218-00014.jpeg  noseal  train\n",
       "14      s1-218   s1-218-00015.jpeg  noseal  train\n",
       "15      s1-218   s1-218-00016.jpeg  noseal  train\n",
       "16      s1-218   s1-218-00017.jpeg  noseal  train\n",
       "17      s1-218   s1-218-00018.jpeg  noseal  train\n",
       "18      s1-218   s1-218-00019.jpeg  noseal  train\n",
       "19      s1-218   s1-218-00020.jpeg  noseal  train\n",
       "20      s1-218   s1-218-00021.jpeg  noseal  train\n",
       "21      s1-218   s1-218-00022.jpeg  noseal  train\n",
       "22      s1-218   s1-218-00023.jpeg  noseal  train\n",
       "23      s1-218   s1-218-00024.jpeg  noseal  train\n",
       "24      s1-218   s1-218-00025.jpeg  noseal  train\n",
       "25      s1-218   s1-218-00026.jpeg  noseal  train\n",
       "26      s1-218   s1-218-00027.jpeg  noseal  train\n",
       "27      s1-218   s1-218-00028.jpeg  noseal  train\n",
       "28      s1-218   s1-218-00029.jpeg  noseal  train\n",
       "29      s1-218   s1-218-00030.jpeg  noseal  train\n",
       "...        ...                 ...     ...    ...\n",
       "12428  s9-5491  s9-5491-00173.jpeg  noseal  train\n",
       "12429  s9-5491  s9-5491-00174.jpeg  noseal  train\n",
       "12430  s9-5491  s9-5491-00175.jpeg  noseal  train\n",
       "12431  s9-5491  s9-5491-00176.jpeg  noseal  train\n",
       "12432  s9-5491  s9-5491-00177.jpeg  noseal  train\n",
       "12433  s9-5491  s9-5491-00178.jpeg  noseal  train\n",
       "12434  s9-5491  s9-5491-00179.jpeg  noseal  train\n",
       "12435  s9-5491  s9-5491-00180.jpeg  noseal  train\n",
       "12436  s9-5491  s9-5491-00181.jpeg  noseal  train\n",
       "12437  s9-5491  s9-5491-00182.jpeg  noseal  train\n",
       "12438  s9-5491  s9-5491-00183.jpeg  noseal  train\n",
       "12439  s9-5491  s9-5491-00184.jpeg  noseal  train\n",
       "12440  s9-5491  s9-5491-00185.jpeg  noseal  train\n",
       "12441  s9-5491  s9-5491-00186.jpeg  noseal  train\n",
       "12442  s9-5491  s9-5491-00187.jpeg  noseal  train\n",
       "12443  s9-5491  s9-5491-00188.jpeg  noseal  train\n",
       "12444  s9-5491  s9-5491-00189.jpeg  noseal  train\n",
       "12445  s9-5491  s9-5491-00190.jpeg  noseal  train\n",
       "12446  s9-5491  s9-5491-00191.jpeg  noseal  train\n",
       "12447  s9-5491  s9-5491-00192.jpeg  noseal  train\n",
       "12448  s9-5491  s9-5491-00193.jpeg  noseal  train\n",
       "12449  s9-5491  s9-5491-00194.jpeg  noseal  train\n",
       "12450  s9-5491  s9-5491-00195.jpeg  noseal  train\n",
       "12451  s9-5491  s9-5491-00196.jpeg  noseal  train\n",
       "12452  s9-5491  s9-5491-00197.jpeg  noseal  train\n",
       "12453  s9-5491  s9-5491-00198.jpeg  noseal  train\n",
       "12454  s9-5491  s9-5491-00199.jpeg  noseal  train\n",
       "12455  s9-5491  s9-5491-00200.jpeg  noseal  train\n",
       "12456  s9-5491  s9-5491-00201.jpeg  noseal  train\n",
       "12457  s9-5491  s9-5491-00202.jpeg  noseal  train\n",
       "\n",
       "[12458 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T23:58:46.476375Z",
     "start_time": "2019-01-06T23:58:46.409464Z"
    }
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No labels found - please save labels file to /data/labels.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-125470f159f0>\u001b[0m in \u001b[0;36mget_labels\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'labels.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'frame'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'split'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/mnt/seals/data/labels.csv' does not exist",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-20a0996dc462>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mreturn_CNN_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mpretrained_model_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     pooling=pooling)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-7fce1ae554c7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sequence_length, return_CNN_features, pretrained_model_name, pooling, frame_size, augmentation, oversampling, model_weights_path, custom_model_name)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# create label array for each video and load label map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mcreate_video_label_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_label_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-5376b541cf3c>\u001b[0m in \u001b[0;36mcreate_video_label_arrays\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# load labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# build label_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-125470f159f0>\u001b[0m in \u001b[0;36mget_labels\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Labels file must contain columns ['video','frame','label','split'] - if you only have ['video','frame','label'], use the helper function add_splits_to_labels_file to add train/valid/test splits to your labels file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No labels found - please save labels file to /data/labels.csv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"video\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: No labels found - please save labels file to /data/labels.csv"
     ]
    }
   ],
   "source": [
    "# build cache\n",
    "for pretrained_model_name in pretrained_model_names:\n",
    "    for pooling in poolings:\n",
    "        data = Data(sequence_length=1, \n",
    "                    return_CNN_features=True,\n",
    "                    pretrained_model_name = pretrained_model_name,\n",
    "                    pooling=pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
